# Fine-tuning Echo

This directory contains the configuration for fine-tuning the Echo model (Qwen 2.5 7B) on the synthetic dataset generated by `agx`.

## Prerequisites
- NVIDIA GPU with >= 24GB VRAM (e.g., A10G, A100, 3090, 4090).
- [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) installed.

## Steps

1.  **Generate Data** (if not already done):
    ```bash
    cd ..
    export AGX_TEACHER_MODEL=qwen2.5:72b  # Use a strong teacher!
    export AGX_BACKEND=ollama
    ./target/release/generate_data
    ```
    This produces `dataset.jsonl` in the project root.

2.  **Install Axolotl**:
    ```bash
    pip install packaging
    pip install -e '.[flash-attn,deepspeed]'
    ```
    (Follow official Axolotl instructions for your environment).

3.  **Run Training**:
    ```bash
    accelerate launch -m axolotl.cli.train axolotl.yaml
    ```

4.  **Merge Adapter**:
    After training, you can merge the LoRA adapter back into the base model if needed, or load it dynamically in `agx` (requires code update to load adapters).
    ```bash
    python3 -m axolotl.cli.merge_lora axolotl.yaml --lora_model_dir=./qlora-out
    ```

## Configuration
The `axolotl.yaml` is configured for:
-   **Base Model**: Qwen 2.5 7B Instruct
-   **Method**: QLoRA (4-bit quantization)
-   **Sequence Length**: 4096
-   **Epochs**: 3
