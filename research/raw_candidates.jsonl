{"intent": "Find all log files larger than 100\u202fMB in /var/log that are older than 30\u202fdays, compress them with gzip, and move the archives to /var/archive/logs.", "plan": [{"command": "mkdir -p /var/archive/logs"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -mtime +30 -print0 | xargs -0 -I{} sh -c 'gzip -c \"{}\" > \"{}.gz\" && mv \"{}.gz\" /var/archive/logs/'"}], "domain": "File System Management"}
{"intent": "Organize the user's Downloads directory by file type, creating sub\u2011folders for images, documents, archives, and others, then moving each file into its appropriate folder.", "plan": [{"command": "mkdir -p ~/Downloads/Images ~/Downloads/Documents ~/Downloads/Archives ~/Downloads/Others"}, {"command": "mv ~/Downloads/*.{jpg,jpeg,png,gif,svg} ~/Downloads/Images/ 2>/dev/null || true"}, {"command": "mv ~/Downloads/*.{pdf,doc,docx,xls,xlsx,ppt,pptx,txt} ~/Downloads/Documents/ 2>/dev/null || true"}, {"command": "mv ~/Downloads/*.{zip,tar,gz,bz2,7z,rar} ~/Downloads/Archives/ 2>/dev/null || true"}, {"command": "mv ~/Downloads/*.* ~/Downloads/Others/ 2>/dev/null || true"}], "domain": "File System Management"}
{"intent": "Recursively set ownership of the /srv/www directory to user 'deploy' and group 'www-data', apply 2755 permissions to all directories and 0644 to all regular files.", "plan": [{"command": "chown -R deploy:www-data /srv/www"}, {"command": "find /srv/www -type d -exec chmod 2755 {} \\;"}, {"command": "find /srv/www -type f -exec chmod 0644 {} \\;"}], "domain": "File System Management"}
{"intent": "Create a disk\u2011usage report for /home, list the 10 largest top\u2011level directories sorted by size, and save the output to ~/disk_report.txt.", "plan": [{"command": "du -h --max-depth=1 /home 2>/dev/null | sort -hr | head -n 10 > ~/disk_report.txt"}], "domain": "File System Management"}
{"intent": "Detect duplicate files in /data/backups by comparing MD5 checksums, list groups of duplicates, and optionally remove all but the first copy of each duplicate set.", "plan": [{"command": "find /data/backups -type f -exec md5sum {} + | sort | awk 'BEGIN{prev=\"\"} $1==prev{print $2} {prev=$1}' > /tmp/duplicate_paths.txt"}, {"command": "echo \"Duplicate files found:\" && cat /tmp/duplicate_paths.txt"}, {"command": "while read -r dup; do rm -i \"$dup\"; done < <(sort /tmp/duplicate_paths.txt | uniq -u)"}], "domain": "File System Management"}
{"intent": "Find all log files larger than 100\u202fMiB in /var/log, archive them into a single compressed tarball, and safely remove the originals after verification.", "plan": [{"command": "find /var/log -type f -size +100M -print > /tmp/large_logs.txt"}, {"command": "tar -czf /tmp/large_logs_$(date +%Y%m%d%H%M%S).tar.gz -T /tmp/large_logs.txt"}, {"command": "if tar -tzf /tmp/large_logs_$(date +%Y%m%d%H%M%S).tar.gz >/dev/null 2>&1; then rm -f $(cat /tmp/large_logs.txt); fi"}, {"command": "rm -f /tmp/large_logs.txt"}], "domain": "File System Management"}
{"intent": "Organize the user's Downloads folder by moving files into sub\u2011folders based on their extensions, extracting archives, and setting appropriate permissions.", "plan": [{"command": "cd ~/Downloads"}, {"command": "for f in *.*; do ext=\"${f##*.}\"; mkdir -p \"${ext}\"; mv -n \"$f\" \"${ext}/\"; done"}, {"command": "for archive in *.zip *.tar.gz *.tgz *.tar.bz2 *.tbz2; do [ -e \"$archive\" ] && mkdir -p extracted && tar -xf \"$archive\" -C extracted && rm -f \"$archive\"; done"}, {"command": "chmod -R u+rwX,go+rX ~/Downloads"}], "domain": "File System Management"}
{"intent": "Recursively change ownership of the /opt/project directory to the user 'deploy', set group ownership to 'deploy', and apply the set\u2011gid bit to all sub\u2011directories.", "plan": [{"command": "chown -R deploy:deploy /opt/project"}, {"command": "find /opt/project -type d -exec chmod g+s {} +"}], "domain": "File System Management"}
{"intent": "Create a disk\u2011usage report for /home, list the ten largest top\u2011level directories, save the report to a file, and email a brief summary to the admin.", "plan": [{"command": "du -h --max-depth=1 /home | sort -hr > /tmp/home_disk_report.txt"}, {"command": "head -n 10 /tmp/home_disk_report.txt > /tmp/home_top10.txt"}, {"command": "mail -s \"Home Disk Usage Summary\" admin@example.com < /tmp/home_top10.txt"}], "domain": "File System Management"}
{"intent": "Find duplicate files in /data by comparing SHA\u2011256 checksums, list groups of duplicates, and create hard\u2011links for each duplicate into a deduplication directory.", "plan": [{"command": "mkdir -p /tmp/dedup_links"}, {"command": "find /data -type f -exec sha256sum {} + | sort | awk 'BEGIN{prev=\"\"; dup=\"\"} {if($1==prev){dup=dup\" \"$2} else {if(dup!=\"\"){print dup}; dup=$2} prev=$1} END{if(dup!=\"\"){print dup}}' > /tmp/duplicates.txt"}, {"command": "while read -r line; do set -- $line; first=\"${1}\"; shift; for f in \"$@\"; do ln -f \"$first\" \"/tmp/dedup_links/$(basename \"$f\")\"; done; done < /tmp/duplicates.txt"}], "domain": "File System Management"}
{"intent": "Find all log files larger than 100\u202fMiB in /var/log, compress them with gzip, and move the archives to /var/log/archive while preserving timestamps.", "plan": [{"command": "mkdir -p /var/log/archive"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -print0 | while IFS= read -r -d '' file; do gzip -c \"$file\" > \"/var/log/archive/$(basename \"$file\").gz\" && touch -r \"$file\" \"/var/log/archive/$(basename \"$file\").gz\"; done"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -delete"}], "domain": "File System Management"}
{"intent": "Delete backup files in /opt/backups that are older than 30 days, then compress any remaining .sql files, and produce a summary report.", "plan": [{"command": "find /opt/backups -type f -mtime +30 -delete"}, {"command": "find /opt/backups -type f -name \"*.sql\" -exec gzip -9 {} \\;"}, {"command": "du -sh /opt/backups > /opt/backups/backup_summary_$(date +%F).txt"}], "domain": "File System Management"}
{"intent": "Create a new project directory structure under /srv/projects/myapp with subdirectories for src, logs, and data, set group ownership to 'devops', and apply appropriate permissions.", "plan": [{"command": "mkdir -p /srv/projects/myapp/{src,logs,data}"}, {"command": "chown -R $(whoami):devops /srv/projects/myapp"}, {"command": "chmod 2775 /srv/projects/myapp"}, {"command": "chmod 2770 /srv/projects/myapp/{src,logs,data}"}], "domain": "File System Management"}
{"intent": "Generate a disk\u2011usage report of the top 10 largest home directories, sort them descending, and save the report to /tmp/home_usage_report.txt.", "plan": [{"command": "du -sh /home/* 2>/dev/null | sort -hr | head -n 10 > /tmp/home_usage_report.txt"}], "domain": "File System Management"}
{"intent": "Synchronize the contents of /data/staging to /data/production using rsync, preserving permissions, timestamps, and symlinks, and write a detailed log to /var/log/rsync_sync.log.", "plan": [{"command": "rsync -aHAX --delete /data/staging/ /data/production/ --log-file=/var/log/rsync_sync.log"}], "domain": "File System Management"}
{"intent": "Find all log files larger than 100\u202fMB in /var/log, archive them into a dated tar.gz backup, and delete the originals after successful verification.", "plan": [{"command": "mkdir -p /backup/logs_$(date +%F)"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -print0 | tar --null -cvzf /backup/logs_$(date +%F)/large_logs.tar.gz --files-from=-"}, {"command": "if tar -tzf /backup/logs_$(date +%F)/large_logs.tar.gz > /dev/null; then find /var/log -type f -name \"*.log\" -size +100M -delete; fi"}], "domain": "File System Management"}
{"intent": "Organize the user's Downloads folder by moving files into sub\u2011directories based on their file extensions, creating the sub\u2011directories when needed.", "plan": [{"command": "cd \"$HOME/Downloads\""}, {"command": "for file in *.*; do [ -f \"$file\" ] || continue; ext=\"${file##*.}\"; mkdir -p \"$ext\"; mv -n \"$file\" \"$ext/\"; done"}], "domain": "File System Management"}
{"intent": "Standardize permissions for the web root directory: directories should be 755, files 644, all owned by user and group www-data.", "plan": [{"command": "chown -R www-data:www-data /var/www/html"}, {"command": "find /var/www/html -type d -exec chmod 755 {} +"}, {"command": "find /var/www/html -type f -exec chmod 644 {} +"}], "domain": "File System Management"}
{"intent": "Create a disk\u2011usage report for all top\u2011level directories in the user's home, sort it by size descending, save to a file, and email it to the user.", "plan": [{"command": "du -sh \"$HOME\"/* | sort -hr > \"$HOME/disk_report.txt\""}, {"command": "mail -s \"Disk Usage Report\" \"$USER\" < \"$HOME/disk_report.txt\""}], "domain": "File System Management"}
{"intent": "In all configuration files under /etc/myapp, replace the placeholder string \"PLACEHOLDER\" with \"production\", backing up each original file with a .bak extension.", "plan": [{"command": "find /etc/myapp -type f -name \"*.conf\" -exec sed -i.bak 's/PLACEHOLDER/production/g' {} +"}], "domain": "File System Management"}
{"intent": "Compress recent large log files in /var/log and archive them safely", "plan": [{"command": "find /var/log -type f -name \"*.log\" -size +100M -mtime -7 -print0 | xargs -0 -I{} gzip -c \"{}\" > \"{}\".gz"}, {"command": "mkdir -p /var/archive/logs"}, {"command": "find /var/log -type f -name \"*.log.gz\" -mtime -7 -exec mv {} /var/archive/logs/ \\;"}, {"command": "echo \"$(date): Archived large recent logs\" >> /var/log/archive.log"}], "domain": "File System Management"}
{"intent": "Organize the ~/Downloads folder by moving files into subfolders based on their file extensions", "plan": [{"command": "cd \"$HOME/Downloads\""}, {"command": "for f in *.*; do ext=\"${f##*.}\"; mkdir -p \"$ext\"; dest=\"$ext/$f\"; if [[ -e \"$dest\" ]]; then i=1; while [[ -e \"${ext}/${f%.*}_$i.${ext}\" ]]; do ((i++)); done; dest=\"${ext}/${f%.*}_$i.${ext}\"; fi; mv \"$f\" \"$dest\"; done"}], "domain": "File System Management"}
{"intent": "Create a detailed disk usage report for /home, sorted by size, and email a summary", "plan": [{"command": "du -sh /home/* | sort -hr > \"$HOME/disk_report.txt\""}, {"command": "top5=$(head -5 \"$HOME/disk_report.txt\" | awk '{print $2 \": \" $1}')\nprintf \"Top 5 largest home directories:\\n%s\\n\" \"$top5\" | mail -s \"Home Disk Usage Summary\" user@example.com"}], "domain": "File System Management"}
{"intent": "Recursively set proper permissions and ownership for a project directory", "plan": [{"command": "chown -R deploy:devops \"/opt/project\""}, {"command": "find \"/opt/project\" -type d -exec chmod 2775 {} \\;"}, {"command": "find \"/opt/project\" -type f -exec chmod 664 {} \\;"}], "domain": "File System Management"}
{"intent": "Archive backup files older than 30 days in /srv/backups and clean up the originals", "plan": [{"command": "cd /srv/backups"}, {"command": "old_files=$(find . -type f -name \"*.bak\" -mtime +30 -print0)"}, {"command": "if [ -n \"$old_files\" ]; then tar -czf \"backup_$(date +%Y%m%d).tar.gz\" --null -T <(printf \"%s\" \"$old_files\"); fi"}, {"command": "find . -type f -name \"*.bak\" -mtime +30 -delete"}], "domain": "File System Management"}
{"intent": "Find all large log files in /var/log (>=100M), compress them, and archive with a timestamped directory.", "plan": [{"command": "mkdir -p /var/log/archive/$(date +%Y%m%d_%H%M%S)"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -print0 | while IFS= read -r -d '' file; do gzip -c \"$file\" > \"/var/log/archive/$(date +%Y%m%d_%H%M%S)/$(basename \"$file\").gz\"; done"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -delete"}], "domain": "File System Management"}
{"intent": "Move backup files older than 30 days from ~/backups to ~/old_backups and generate a size summary report.", "plan": [{"command": "mkdir -p ~/old_backups"}, {"command": "find ~/backups -type f -mtime +30 -print0 | while IFS= read -r -d '' old; do mv \"$old\" ~/old_backups/; done"}, {"command": "du -sh ~/old_backups/* > ~/old_backups/size_summary.txt"}], "domain": "File System Management"}
{"intent": "Set group read/execute permissions on all directories and read permission on files under /srv/www, and enable the setgid bit on directories.", "plan": [{"command": "find /srv/www -type d -exec chmod 2775 {} +"}, {"command": "find /srv/www -type f -exec chmod 664 {} +"}], "domain": "File System Management"}
{"intent": "Create a disk usage report for each user\u2019s home directory, sorted by size descending, and save it to /tmp/disk_report.txt.", "plan": [{"command": "du -sh /home/* 2>/dev/null | sort -hr > /tmp/disk_report.txt"}], "domain": "File System Management"}
{"intent": "Synchronize the project source directory to a backup location using rsync, preserving permissions and logging the operation.", "plan": [{"command": "mkdir -p /backup/project_sync"}, {"command": "rsync -a --delete /srv/project/ /backup/project_sync/ --log-file=/var/log/rsync_project_$(date +%Y%m%d).log"}], "domain": "File System Management"}
{"intent": "Find all log files larger than 100\u202fMiB in /var/log that are older than 30 days, compress them with gzip, and move the archives to /var/log/archive.", "plan": [{"command": "mkdir -p /var/log/archive"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -mtime +30 -print0 | while IFS= read -r -d '' file; do gzip -c \"$file\" > \"/var/log/archive/$(basename \"$file\").gz\"; done"}, {"command": "find /var/log/archive -type f -name \"*.log.gz\" -exec rm -f {} \\;"}], "domain": "File System Management"}
{"intent": "Organize the user's Downloads folder by moving files into subfolders based on their extensions and set appropriate permissions.", "plan": [{"command": "mkdir -p ~/Downloads/{PDFs,Images,Archives,Videos,Docs,Others}"}, {"command": "shopt -s nullglob; for f in ~/Downloads/*.*; do ext=\"${f##*.}\"; case \"${ext,,}\" in pdf) mv \"$f\" ~/Downloads/PDFs/ ;; jpg|jpeg|png|gif|bmp) mv \"$f\" ~/Downloads/Images/ ;; zip|tar|gz|bz2|xz|rar) mv \"$f\" ~/Downloads/Archives/ ;; mp4|mkv|avi|mov) mv \"$f\" ~/Downloads/Videos/ ;; doc|docx|odt|txt) mv \"$f\" ~/Downloads/Docs/ ;; *) mv \"$f\" ~/Downloads/Others/ ;; esac; done"}, {"command": "chmod -R 750 ~/Downloads/{PDFs,Images,Archives,Videos,Docs,Others}"}, {"command": "chmod -R 640 ~/Downloads/*.{pdf,doc,docx,odt,txt}"}, {"command": "chmod -R 640 ~/Downloads/*.{jpg,jpeg,png,gif,bmp,zip,tar,gz,bz2,xz,rar,mp4,mkv,avi,mov}"}], "domain": "File System Management"}
{"intent": "Recursively change ownership of /opt/project to user and group 'deploy', set directory permissions to 750 and file permissions to 640.", "plan": [{"command": "chown -R deploy:deploy /opt/project"}, {"command": "find /opt/project -type d -exec chmod 750 {} \\;"}, {"command": "find /opt/project -type f -exec chmod 640 {} \\;"}], "domain": "File System Management"}
{"intent": "Create a disk usage report for /home, list the top 10 largest directories, save it to /tmp/disk_report.txt, and email the report to admin@example.com.", "plan": [{"command": "du -h --max-depth=1 /home | sort -hr | head -n 10 > /tmp/disk_report.txt"}, {"command": "mail -s \"Top 10 largest directories in /home\" admin@example.com < /tmp/disk_report.txt"}], "domain": "File System Management"}
{"intent": "Identify duplicate files under /data/backups by checksum, write a list of duplicate groups to /tmp/duplicate_files.txt, and replace each duplicate with a hard link to the first occurrence to save space.", "plan": [{"command": "mkdir -p /tmp/duplicate_detection && cd /data/backups && find . -type f -exec md5sum \"{}\" + > /tmp/duplicate_detection/md5sums.txt"}, {"command": "awk '{print $1}' /tmp/duplicate_detection/md5sums.txt | sort | uniq -d > /tmp/duplicate_detection/duplicate_hashes.txt"}, {"command": "while read hash; do grep \"^$hash\" /tmp/duplicate_detection/md5sums.txt | cut -d' ' -f3- > /tmp/duplicate_detection/files.txt; first=$(head -n1 /tmp/duplicate_detection/files.txt); tail -n +2 /tmp/duplicate_detection/files.txt | while read dup; do rm -f \"$dup\" && ln \"$first\" \"$dup\"; done; done < /tmp/duplicate_detection/duplicate_hashes.txt"}, {"command": "rm -rf /tmp/duplicate_detection"}], "domain": "File System Management"}
{"intent": "Identify all files larger than 100\u202fMiB in /var/www, archive them into a timestamped tarball, and generate a summary report of the archived files.", "plan": [{"command": "mkdir -p /tmp/archive_reports"}, {"command": "TIMESTAMP=$(date +%Y%m%d_%H%M%S)"}, {"command": "find /var/www -type f -size +100M -print0 | tar --null -cvzf /tmp/archive_reports/large_files_${TIMESTAMP}.tar.gz --files-from=-"}, {"command": "tar -tzvf /tmp/archive_reports/large_files_${TIMESTAMP}.tar.gz > /tmp/archive_reports/large_files_${TIMESTAMP}_list.txt"}, {"command": "du -sh /tmp/archive_reports/large_files_${TIMESTAMP}.tar.gz >> /tmp/archive_reports/large_files_${TIMESTAMP}_list.txt"}], "domain": "File System Management"}
{"intent": "Clean up old backup snapshots in /backups: keep only the most recent three daily, weekly, and monthly backups, and move the rest to an archival directory.", "plan": [{"command": "mkdir -p /backups/archive"}, {"command": "cd /backups && ls -1d backup_* | sort -r > /tmp/backup_list.txt"}, {"command": "awk 'NR>3' /tmp/backup_list.txt | while read old; do mv \"$old\" /backups/archive/; done"}, {"command": "cd /backups && ls -1d weekly_* | sort -r > /tmp/weekly_list.txt"}, {"command": "awk 'NR>3' /tmp/weekly_list.txt | while read old; do mv \"$old\" /backups/archive/; done"}, {"command": "cd /backups && ls -1d monthly_* | sort -r > /tmp/monthly_list.txt"}, {"command": "awk 'NR>3' /tmp/monthly_list.txt | while read old; do mv \"$old\" /backups/archive/; done"}], "domain": "File System Management"}
{"intent": "Set up a secure shared directory at /srv/shared where group 'devops' has read/write access, others have read\u2011only, and enforce permission inheritance on new files.", "plan": [{"command": "mkdir -p /srv/shared"}, {"command": "chown root:devops /srv/shared"}, {"command": "chmod 2775 /srv/shared"}, {"command": "setfacl -d -m g::rwx /srv/shared"}, {"command": "setfacl -d -m o::r /srv/shared"}, {"command": "setfacl -d -m u::rw /srv/shared"}], "domain": "File System Management"}
{"intent": "Generate a disk usage heatmap for the user's home directory, highlighting directories consuming more than 1\u202fGiB, and export the result as a CSV file.", "plan": [{"command": "du -h --max-depth=3 $HOME | awk '$1~/[0-9\\.]+[G]/ {print}' > /tmp/du_raw.txt"}, {"command": "awk '{gsub(/\\t/,\",\"); print}' /tmp/du_raw.txt > /tmp/du_report.csv"}, {"command": "echo \"Size,Directory\" | cat - /tmp/du_report.csv > /tmp/du_report_final.csv && mv /tmp/du_report_final.csv /tmp/du_report.csv"}], "domain": "File System Management"}
{"intent": "Migrate all .conf configuration files from /etc to a version\u2011controlled repository under /opt/config_repo, preserving directory structure and committing each file with a timestamped message.", "plan": [{"command": "mkdir -p /opt/config_repo && cd /opt/config_repo && git init"}, {"command": "cd /etc && find . -type f -name \"*.conf\" -print0 | cpio -pdmv0 /opt/config_repo"}, {"command": "cd /opt/config_repo && git add . && git commit -m \"Initial import of .conf files on $(date +%Y-%m-%d)\""}], "domain": "File System Management"}
{"intent": "Find all log files larger than 100\u202fMiB in /var/log, archive them into a timestamped tar.gz file in /backup/logs, and remove the originals after successful archiving.", "plan": [{"command": "mkdir -p /backup/logs"}, {"command": "TIMESTAMP=$(date +%Y%m%d_%H%M%S)"}, {"command": "find /var/log -type f -size +100M -print0 | tar --null -czvf /backup/logs/logs_${TIMESTAMP}.tar.gz --files-from=-"}, {"command": "if [ $? -eq 0 ]; then find /var/log -type f -size +100M -delete; fi"}], "domain": "File System Management"}
{"intent": "Organize the user's Downloads folder by moving files into sub\u2011folders based on their MIME type (Images, Videos, Documents, Archives, Others).", "plan": [{"command": "DOWNLOADS=\"$HOME/Downloads\""}, {"command": "mkdir -p \"$DOWNLOADS/Images\" \"$DOWNLOADS/Videos\" \"$DOWNLOADS/Documents\" \"$DOWNLOADS/Archives\" \"$DOWNLOADS/Others\""}, {"command": "shopt -s nullglob"}, {"command": "for file in \"$DOWNLOADS\"/*; do\n    mime=$(file --mime-type -b \"$file\")\n    case \"$mime\" in\n        image/*)   mv -n \"$file\" \"$DOWNLOADS/Images/\";;\n        video/*)   mv -n \"$file\" \"$DOWNLOADS/Videos/\";;\n        application/pdf|application/msword|application/vnd.*) mv -n \"$file\" \"$DOWNLOADS/Documents/\";;\n        application/zip|application/x-tar|application/gzip|application/x-bzip2) mv -n \"$file\" \"$DOWNLOADS/Archives/\";;\n        *)         mv -n \"$file\" \"$DOWNLOADS/Others/\";;\n    esac\ndone"}], "domain": "File System Management"}
{"intent": "Recursively set proper permissions for a web project: directories to 755, files to 644, and ownership to user and group 'www-data'.", "plan": [{"command": "PROJECT_DIR=\"/var/www/html/myproject\""}, {"command": "chown -R www-data:www-data \"$PROJECT_DIR\""}, {"command": "find \"$PROJECT_DIR\" -type d -exec chmod 755 {} +"}, {"command": "find \"$PROJECT_DIR\" -type f -exec chmod 644 {} +"}], "domain": "File System Management"}
{"intent": "Create a daily disk\u2011usage report for all users' home directories, sorted by size descending, and store it in /var/reports with a date\u2011stamped filename.", "plan": [{"command": "REPORT_DIR=\"/var/reports\""}, {"command": "mkdir -p \"$REPORT_DIR\""}, {"command": "DATE=$(date +%Y%m%d)"}, {"command": "du -sh /home/* 2>/dev/null | sort -hr > \"$REPORT_DIR/disk_usage_${DATE}.txt\""}], "domain": "File System Management"}
{"intent": "Compress all files in /tmp that are older than 7 days into a single archive and then delete the original files after successful compression.", "plan": [{"command": "TMP_ARCHIVE=\"/tmp/tmp_cleanup_$(date +%Y%m%d_%H%M%S).tar.gz\""}, {"command": "find /tmp -type f -mtime +7 -print0 | tar --null -czvf \"$TMP_ARCHIVE\" --files-from=-"}, {"command": "if [ $? -eq 0 ]; then\n    find /tmp -type f -mtime +7 -delete\nfi"}], "domain": "File System Management"}
{"intent": "Find all log files larger than 100\u202fMB under /var/log, compress them with gzip, and move the archives to /var/archive/logs while preserving timestamps.", "plan": [{"command": "mkdir -p /var/archive/logs"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -print0 | xargs -0 -I{} sh -c 'gzip -c \"{}\" > \"/var/archive/logs/$(basename \"{}\" ).gz\" && touch -r \"{}\" \"/var/archive/logs/$(basename \"{}\" ).gz\"'"}], "domain": "File System Management"}
{"intent": "Organize the user's Downloads folder by moving files into sub\u2011directories based on their MIME type (e.g., images, documents, archives).", "plan": [{"command": "mkdir -p \"$HOME/Downloads/Images\" \"$HOME/Downloads/Documents\" \"$HOME/Downloads/Archives\" \"$HOME/Downloads/Others\""}, {"command": "for f in \"$HOME/Downloads\"/*; do [ -f \"$f\" ] || continue; mime=$(file --mime-type -b \"$f\"); case \"$mime\" in image/*) dest=\"$HOME/Downloads/Images\" ;; application/zip|application/x-tar|application/gzip) dest=\"$HOME/Downloads/Archives\" ;; application/pdf|application/msword|application/vnd.openxmlformats-officedocument.*) dest=\"$HOME/Downloads/Documents\" ;; *) dest=\"$HOME/Downloads/Others\" ;; esac; mv -n \"$f\" \"$dest/\"; done"}], "domain": "File System Management"}
{"intent": "Recursively change ownership of the project directory /opt/project to user 'deploy' and group 'devops', set directories to rwxrwsr-x (2775) and files to rw-rw-r-- (0664).", "plan": [{"command": "chown -R deploy:devops /opt/project"}, {"command": "find /opt/project -type d -exec chmod 2775 {} +"}, {"command": "find /opt/project -type f -exec chmod 0664 {} +"}], "domain": "File System Management"}
{"intent": "Create a disk\u2011usage report for each user\u2019s home directory, sorted from largest to smallest, save it to /tmp/disk_report.txt, and send a brief email summary.", "plan": [{"command": "du -sh /home/* 2>/dev/null | sort -hr > /tmp/disk_report.txt"}, {"command": "head -n 5 /tmp/disk_report.txt | mail -s \"Top 5 home directory usage\" user@example.com"}], "domain": "File System Management"}
{"intent": "Synchronize the remote backup directory /backup from user 'backup'@'backup.example.com' to the local /mnt/backup, preserving permissions, compressing data during transfer, excluding any .git directories, and log the operation.", "plan": [{"command": "mkdir -p /mnt/backup"}, {"command": "rsync -avz --exclude='.git/' --delete backup@backup.example.com:/backup/ /mnt/backup/ > /var/log/backup_sync_$(date +%F).log 2>&1"}, {"command": "echo \"Backup sync completed at $(date)\" >> /var/log/backup_sync_$(date +%F).log"}], "domain": "File System Management"}
{"intent": "Find all log files larger than 100\u202fMB in /var/log, compress them with gzip, and archive them in /var/archive/logs with timestamped filenames.", "plan": [{"command": "mkdir -p /var/archive/logs"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -print0 | while IFS= read -r -d '' file; do gzip -c \"$file\" > \"/var/archive/logs/$(basename \"$file\").$(date +%Y%m%d%H%M%S).gz\"; done"}], "domain": "File System Management"}
{"intent": "Organize the user's Downloads folder by moving files into subfolders according to their MIME type (e.g., Images, Documents, Archives, Others).", "plan": [{"command": "mkdir -p ~/Downloads/{Images,Documents,Archives,Others}"}, {"command": "shopt -s nullglob; for f in ~/Downloads/*; do if [ -f \"$f\" ]; then mime=$(file --mime-type -b \"$f\"); case \"$mime\" in image/*) mv \"$f\" ~/Downloads/Images/;; application/pdf|application/msword|application/vnd.openxmlformats-officedocument.wordprocessingml.document) mv \"$f\" ~/Downloads/Documents/;; application/zip|application/x-tar|application/gzip) mv \"$f\" ~/Downloads/Archives/;; *) mv \"$f\" ~/Downloads/Others/;; esac; fi; done"}], "domain": "File System Management"}
{"intent": "Recursively set directory permissions to 755 and file permissions to 644 for the web application directory, and change ownership to user and group www-data.", "plan": [{"command": "chown -R www-data:www-data /opt/webapp"}, {"command": "find /opt/webapp -type d -exec chmod 755 {} +"}, {"command": "find /opt/webapp -type f -exec chmod 644 {} +"}], "domain": "File System Management"}
{"intent": "Create a disk usage report for each user\u2019s home directory, sorted by size, save it to /tmp/disk_report.txt, and email the report to the system administrator.", "plan": [{"command": "du -sh /home/* 2>/dev/null | sort -hr > /tmp/disk_report.txt"}, {"command": "mailx -s \"Daily Home Directory Disk Usage Report\" admin@example.com < /tmp/disk_report.txt"}], "domain": "File System Management"}
{"intent": "Detect duplicate files in ~/Pictures by checksum, move older duplicates to a Trash folder, and log the actions to a duplicate_cleanup.log file.", "plan": [{"command": "mkdir -p ~/Pictures/Trash"}, {"command": "find ~/Pictures -type f -exec md5sum {} + | sort | awk 'BEGIN{FS=\" \";} {if (seen[$1]) print $2; else seen[$1]=$2}' > /tmp/duplicates_to_remove.txt"}, {"command": "while IFS= read -r dup; do newest=$(stat -c \"%Y %n\" \"$dup\" | sort -nr | head -1 | cut -d' ' -f2-); if [ \"$dup\" != \"$newest\" ]; then mv \"$dup\" ~/Pictures/Trash/; echo \"$(date +\"%Y-%m-%d %H:%M:%S\") Moved duplicate: $dup -> Trash\" >> ~/Pictures/duplicate_cleanup.log; fi; done < <(cat /tmp/duplicates_to_remove.txt)"}], "domain": "File System Management"}
{"intent": "Identify and archive all log files larger than 100\u202fMB in /var/log that are older than 30\u202fdays.", "plan": [{"command": "mkdir -p /tmp/log_archives"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -mtime +30 -print0 | tar --null -czvf /tmp/log_archives/old_large_logs_$(date +%Y%m%d).tar.gz --files-from=-"}], "domain": "File System Management"}
{"intent": "Reorganize the user's Downloads folder by moving files into subfolders based on their file extensions.", "plan": [{"command": "mkdir -p \"$HOME/Downloads\" && cd \"$HOME/Downloads\""}, {"command": "for f in *.*; do ext=\"${f##*.}\"; mkdir -p \"$ext\"; mv -n \"$f\" \"$ext/\"; done"}], "domain": "File System Management"}
{"intent": "Set strict permissions on the SSH configuration directory so that only root can read/write, and ensure all files are non\u2011executable.", "plan": [{"command": "chown -R root:root /etc/ssh"}, {"command": "find /etc/ssh -type d -exec chmod 750 {} +"}, {"command": "find /etc/ssh -type f -exec chmod 640 {} +"}], "domain": "File System Management"}
{"intent": "Generate a disk\u2011usage report for each user\u2019s home directory, sorted by size, and save it to /tmp/disk_report.txt.", "plan": [{"command": "du -sh /home/* 2>/dev/null | sort -h > /tmp/disk_report.txt"}, {"command": "echo \"Report generated on $(date)\" >> /tmp/disk_report.txt"}], "domain": "File System Management"}
{"intent": "Create a weekly snapshot backup of /srv/data using rsync with hard links, retaining the last four weeks in /backup/snapshots.", "plan": [{"command": "snapshot_dir=\"/backup/snapshots/$(date +%Y-%m-%d)\" && mkdir -p \"$snapshot_dir\""}, {"command": "rsync -a --delete --link-dest=\"/backup/snapshots/$(date -d '7 days ago' +%Y-%m-%d)\" /srv/data/ \"$snapshot_dir/\""}, {"command": "cd /backup/snapshots && ls -1d ????-??-?? | sort | head -n -4 | xargs -r rm -rf"}], "domain": "File System Management"}
{"intent": "Locate all archived log files older than 30 days in /var/log, compress them into a single tar.gz archive, and move the archive to /backup/logs.", "plan": [{"command": "find /var/log -type f -name \"*.log.*\" -mtime +30 > /tmp/old_logs.txt"}, {"command": "tar -czf /tmp/old_logs_$(date +%Y%m%d).tar.gz -T /tmp/old_logs.txt"}, {"command": "mv /tmp/old_logs_$(date +%Y%m%d).tar.gz /backup/logs/"}], "domain": "File System Management"}
{"intent": "Organize the user's ~/Downloads directory by moving files into subfolders based on their MIME type categories (e.g., images, documents, archives).", "plan": [{"command": "mkdir -p ~/Downloads/{Images,Documents,Archives,Audio,Video,Others}"}, {"command": "for f in ~/Downloads/*; do if [ -f \"$f\" ]; then mime=$(file --mime-type -b \"$f\"); case \"$mime\" in image/*) mv \"$f\" ~/Downloads/Images/ ;; application/pdf|application/msword|application/vnd.*) mv \"$f\" ~/Downloads/Documents/ ;; application/zip|application/x-tar|application/gzip) mv \"$f\" ~/Downloads/Archives/ ;; audio/*) mv \"$f\" ~/Downloads/Audio/ ;; video/*) mv \"$f\" ~/Downloads/Video/ ;; *) mv \"$f\" ~/Downloads/Others/ ;; esac; fi; done"}], "domain": "File System Management"}
{"intent": "Recursively set the group ownership of /srv/www to \"webdev\" and grant the group read/write/execute permissions while removing world write permission.", "plan": [{"command": "chgrp -R webdev /srv/www"}, {"command": "chmod -R g+rwX,o-w /srv/www"}], "domain": "File System Management"}
{"intent": "Identify the top 10 directories consuming the most disk space under /home, and generate a human\u2011readable report saved to /tmp/disk_usage_report.txt.", "plan": [{"command": "du -h --max-depth=2 /home | sort -hr | head -n 10 > /tmp/disk_usage_report.txt"}, {"command": "echo \"Disk usage report generated on $(date)\" >> /tmp/disk_usage_report.txt"}], "domain": "File System Management"}
{"intent": "Create a snapshot of the current state of the /etc configuration directory using rsync, storing the snapshot in /var/backups/etc_snapshot_YYYYMMDD_HHMMSS.", "plan": [{"command": "timestamp=$(date +%Y%m%d_%H%M%S)"}, {"command": "snapshot_dir=\"/var/backups/etc_snapshot_${timestamp}\""}, {"command": "mkdir -p \"$snapshot_dir\""}, {"command": "rsync -a --delete /etc/ \"$snapshot_dir/\""}], "domain": "File System Management"}
{"intent": "Find all log files larger than 100\u202fMB in /var/log that are older than 30\u202fdays, compress them with gzip, and move them to /var/archive/logs while preserving the original directory hierarchy.", "plan": [{"command": "mkdir -p /var/archive/logs"}, {"command": "cd /var/log && find . -type f -name \"*.log\" -size +100M -mtime +30 -print0 | while IFS= read -r -d '' file; do gzip -c \"$file\" > \"/var/archive/logs/${file}.gz\" && echo \"Compressed $file\"; done"}, {"command": "cd /var/log && find . -type f -name \"*.log\" -size +100M -mtime +30 -delete"}], "domain": "File System Management"}
{"intent": "Clean up old backup archives in /backups: locate .tar.gz files older than 90\u202fdays, verify their SHA256 checksums, move the verified ones to /archive/backups, and write a detailed log of the operation.", "plan": [{"command": "mkdir -p /archive/backups"}, {"command": "touch /var/log/backup_cleanup.log"}, {"command": "find /backups -type f -name \"*.tar.gz\" -mtime +90 -print0 | while IFS= read -r -d '' file; do\n  if sha256sum \"$file\" > \"${file}.sha256\"; then\n    mv \"$file\" /archive/backups/ && echo \"$(date '+%F %T') Moved $file\" >> /var/log/backup_cleanup.log\n  else\n    echo \"$(date '+%F %T') CHECKSUM FAILED for $file\" >> /var/log/backup_cleanup.log\n  fi\n done"}], "domain": "File System Management"}
{"intent": "Create a daily snapshot of /etc using rsync with hard\u2011links to save space, store snapshots in /snapshots with a date\u2011based name, keep only the last 7 snapshots, and ensure the snapshot directory is readable by the admin group.", "plan": [{"command": "SNAP_DIR=\"/snapshots/etc_$(date +%F)\"\nmkdir -p \"$SNAP_DIR\""}, {"command": "rsync -a --delete --link-dest=/snapshots/latest /etc/ \"$SNAP_DIR/\""}, {"command": "ln -sfn \"$SNAP_DIR\" /snapshots/latest"}, {"command": "cd /snapshots && ls -1d etc_* | sort | head -n -7 | xargs -r rm -rf"}, {"command": "chmod -R g+rx /snapshots && chgrp -R admin /snapshots"}], "domain": "File System Management"}
{"intent": "Reorganize the user's Pictures folder by moving image files into YEAR/MONTH sub\u2011directories based on their EXIF capture date, then generate a summary report of the number of files moved per month.", "plan": [{"command": "cd ~/Pictures"}, {"command": "find . -type f \\( -iname \"*.jpg\" -o -iname \"*.jpeg\" -o -iname \"*.png\" \\) -print0 | while IFS= read -r -d '' img; do\n  DATE=$(exiftool -d \"%Y/%m\" -DateTimeOriginal -s -s -s \"$img\" 2>/dev/null)\n  if [ -z \"$DATE\" ]; then\n    DATE=\"unknown/unknown\"\n  fi\n  DEST=\"$(pwd)/$DATE\"\n  mkdir -p \"$DEST\"\n  mv \"$img\" \"$DEST/\"\n  echo \"$DEST\" >> .move_log\n done"}, {"command": "awk -F'/' '{print $(NF-1)\"/\"$NF}' .move_log | sort | uniq -c > picture_reorg_summary.txt"}], "domain": "File System Management"}
{"intent": "Audit disk usage on the server by generating a CSV report of the 10 largest directories under /srv, include their sizes in human\u2011readable form, and email the report to admin@example.com.", "plan": [{"command": "du -h --max-depth=1 /srv | sort -hr | head -n 11 > /tmp/srv_sizes.txt"}, {"command": "awk 'NR>1 {print $2\",\"$1}' /tmp/srv_sizes.txt > /tmp/srv_report.csv"}, {"command": "echo \"Top 10 largest directories under /srv\" | mail -s \"Srv Disk Usage Report\" -A /tmp/srv_report.csv admin@example.com"}], "domain": "File System Management"}
{"intent": "Find all log files larger than 100 MB in /var/log, compress them with gzip, move the archives to /archive/logs, and set read\u2011only permissions for the archives.", "plan": [{"command": "mkdir -p /archive/logs"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -print0 | while IFS= read -r -d '' file; do gzip -c \"$file\" > \"/archive/logs/$(basename \"$file\").gz\"; done"}, {"command": "chmod 440 /archive/logs/*.gz"}], "domain": "File System Management"}
{"intent": "Archive backup files older than 30 days from /srv/backups into a tar.gz bundle per day, store the bundles in /srv/backup-archives, and delete the original files after successful archiving.", "plan": [{"command": "mkdir -p /srv/backup-archives"}, {"command": "cd /srv/backups && find . -type f -mtime +30 -print0 | xargs -0 -I {} sh -c 'day=$(date -r \"{}\" +%Y-%m-%d); mkdir -p \"/srv/backup-archives/$day\"; mv \"{}\" \"/srv/backup-archives/$day/\"'"}, {"command": "cd /srv/backup-archives && for d in */; do tar -czf \"${d%/}.tar.gz\" \"$d\" && rm -rf \"$d\"; done"}], "domain": "File System Management"}
{"intent": "Create a new project directory structure at /opt/myproject with subdirectories src, bin, lib, docs; assign group \"devteam\", set group\u2011write permissions, and apply default ACLs so new files inherit group ownership.", "plan": [{"command": "mkdir -p /opt/myproject/{src,bin,lib,docs}"}, {"command": "chown -R $(whoami):devteam /opt/myproject"}, {"command": "chmod -R 2775 /opt/myproject"}, {"command": "setfacl -d -m g:devteam:rwx /opt/myproject"}], "domain": "File System Management"}
{"intent": "Generate a disk usage report of all user home directories, sorted by size descending, and save the report to /tmp/disk_report.txt.", "plan": [{"command": "du -sh /home/* 2>/dev/null | sort -hr > /tmp/disk_report.txt"}, {"command": "echo \"Disk usage report generated on $(date)\" >> /tmp/disk_report.txt"}], "domain": "File System Management"}
{"intent": "Synchronize the contents of /data/source to /data/backup using rsync, preserving permissions, timestamps, and ownership, and write a detailed log to /var/log/rsync_backup.log.", "plan": [{"command": "mkdir -p /data/backup"}, {"command": "rsync -a --delete /data/source/ /data/backup/ --log-file=/var/log/rsync_backup.log"}], "domain": "File System Management"}
{"intent": "Find all large log files in /var/log older than 30 days, compress them, and move the archives to /var/log/archive.", "plan": [{"command": "mkdir -p /var/log/archive"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -mtime +30 -exec gzip -k {} \\;"}, {"command": "find /var/log -type f -name \"*.log.gz\" -mtime +30 -exec mv {} /var/log/archive/ \\;"}], "domain": "File System Management"}
{"intent": "Organize the user's Downloads folder by moving files into subfolders based on file extension, handling name collisions safely.", "plan": [{"command": "mkdir -p ~/Downloads/{Images,Documents,Archives,Executables,Others}"}, {"command": "shopt -s nullglob; for f in ~/Downloads/*.*; do ext=\"${f##*.}\"; case \"$ext\" in jpg|jpeg|png|gif) dest=Images;; pdf|doc|docx|txt) dest=Documents;; zip|tar|gz|bz2|xz) dest=Archives;; sh|exe|bin) dest=Executables;; *) dest=Others;; esac; base=\"$(basename \"$f\")\"; target=~/Downloads/$dest/$base; if [[ -e $target ]]; then i=1; while [[ -e ~/Downloads/$dest/${base%.*}_$i.${base##*.} ]]; do ((i++)); done; target=~/Downloads/$dest/${base%.*}_$i.${base##*.}; fi; mv \"$f\" \"$target\"; done"}], "domain": "File System Management"}
{"intent": "Recursively change ownership of the project directory to user 'deploy' and group 'www-data', set directory permissions to 2755 and file permissions to 0644.", "plan": [{"command": "chown -R deploy:www-data /srv/project"}, {"command": "find /srv/project -type d -exec chmod 2755 {} \\;"}, {"command": "find /srv/project -type f -exec chmod 0644 {} \\;"}], "domain": "File System Management"}
{"intent": "Generate a disk usage report for all mounted filesystems, list the top 10 largest directories for each mount point, and save the report with a timestamp.", "plan": [{"command": "report_file=\"/var/reports/disk_usage_$(date +%F).txt\""}, {"command": "mkdir -p /var/reports"}, {"command": "df -h > \"$report_file\""}, {"command": "echo \"\\nTop 10 directories by size per mount point:\" >> \"$report_file\""}, {"command": "while read -r mount; do echo \"\\nMount: $mount\" >> \"$report_file\"; du -ah \"${mount}\" 2>/dev/null | sort -rh | head -n 10 >> \"$report_file\"; done < <(df --output=target | tail -n +2)"}], "domain": "File System Management"}
{"intent": "Create a timestamped backup of /etc configuration files, excluding any files containing secret keys, and store it compressed in /backup.", "plan": [{"command": "mkdir -p /backup"}, {"command": "timestamp=$(date +%F_%H%M)"}, {"command": "tar --exclude='*secret*' -czf \"/backup/etc_${timestamp}.tar.gz\" /etc"}], "domain": "File System Management"}
{"intent": "Find all log files larger than 100\u202fMiB in /var/log that are older than 30 days, compress them with gzip, and move the archives to /var/log/archives.", "plan": [{"command": "mkdir -p /var/log/archives"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -mtime +30 -print0 | while IFS= read -r -d '' file; do gzip -c \"$file\" > \"/var/log/archives/$(basename \"$file\").gz\" && echo \"Compressed $file\"; done"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -mtime +30 -delete"}], "domain": "File System Management"}
{"intent": "Organize the user's Downloads folder by file type, placing files into subfolders like Images, Documents, Archives, and Others while handling name collisions safely.", "plan": [{"command": "DOWNLOADS=\"$HOME/Downloads\""}, {"command": "declare -A type_map=( [\"jpg\"]=Images [\"jpeg\"]=Images [\"png\"]=Images [\"gif\"]=Images [\"pdf\"]=Documents [\"doc\"]=Documents [\"docx\"]=Documents [\"txt\"]=Documents [\"zip\"]=Archives [\"tar\"]=Archives [\"gz\"]=Archives [\"bz2\"]=Archives )"}, {"command": "for file in \"$DOWNLOADS\"/*; do [ -f \"$file\" ] || continue; ext=\"${file##*.}\"; dest=\"${type_map[${ext,,}]:-Others}\"; mkdir -p \"$DOWNLOADS/$dest\"; base=\"$(basename \"$file\")\"; target=\"$DOWNLOADS/$dest/$base\"; i=1; while [ -e \"$target\" ]; do target=\"$DOWNLOADS/$dest/${base%.*}_$i.${ext}\"; i=$((i+1)); done; mv \"$file\" \"$target\"; done"}], "domain": "File System Management"}
{"intent": "Recursively change ownership of /opt/project to user and group 'deploy', set directories to 2750 (rwxr-s---) and files to 0640 (rw-r-----).", "plan": [{"command": "chown -R deploy:deploy /opt/project"}, {"command": "find /opt/project -type d -exec chmod 2750 {} +"}, {"command": "find /opt/project -type f -exec chmod 0640 {} +"}], "domain": "File System Management"}
{"intent": "Generate a disk usage report for all users' home directories, sort by size descending, extract the top 5 consumers, and email the report to admin@example.com.", "plan": [{"command": "du -sh /home/* 2>/dev/null | sort -hr > /tmp/home_usage_report.txt"}, {"command": "head -n 5 /tmp/home_usage_report.txt > /tmp/top5_home_usage.txt"}, {"command": "mail -s \"Top 5 Home Directory Disk Users\" admin@example.com < /tmp/top5_home_usage.txt"}], "domain": "File System Management"}
{"intent": "Find duplicate files in /srv/data based on SHA256 checksum, list duplicate groups, and replace each duplicate with a hard link to a single retained copy.", "plan": [{"command": "cd /srv/data && find . -type f -exec sha256sum {} + | sort | awk 'BEGIN{prev=\"\"} {if($1==prev){print $2} else {if(NR>1){print \"---\"} prev=$1}}' > /tmp/dup_checksums.txt"}, {"command": "awk '/---/{group=++c} !/---/{print $0 > \"/tmp/dup_group_\"group\".txt\"}' /tmp/dup_checksums.txt"}, {"command": "for f in /tmp/dup_group_*.txt; do master=$(head -n1 \"$f\"); while read dup; do [ \"$dup\" = \"$master\" ] && continue; rm -f \"$dup\" && ln \"$master\" \"$dup\"; done < <(tail -n +2 \"$f\"); done"}], "domain": "File System Management"}
{"intent": "Find all log files larger than 100\u202fMiB in /var/log that are older than 30\u202fdays, compress them with gzip, and move the archives to /var/log/archive.", "plan": [{"command": "mkdir -p /var/log/archive"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -mtime +30 -print0 | while IFS= read -r -d '' file; do gzip -c \"$file\" > \"/var/log/archive/$(basename \"$file\").gz\" && echo \"Compressed $file\"; done"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -mtime +30 -delete"}], "domain": "File System Management"}
{"intent": "Organize the user's Downloads folder by file type, creating sub\u2011folders for images, documents, archives, and others, then move each file into the appropriate folder.", "plan": [{"command": "mkdir -p \"$HOME/Downloads/Images\" \"$HOME/Downloads/Documents\" \"$HOME/Downloads/Archives\" \"$HOME/Downloads/Others\""}, {"command": "shopt -s nullglob"}, {"command": "for f in \"$HOME/Downloads\"/*; do [ -f \"$f\" ] || continue; case \"${f,,}\" in *.jpg|*.jpeg|*.png|*.gif|*.bmp) mv \"$f\" \"$HOME/Downloads/Images/\" ;; *.pdf|*.doc|*.docx|*.txt|*.odt) mv \"$f\" \"$HOME/Downloads/Documents/\" ;; *.zip|*.tar|*.gz|*.bz2|*.xz|*.rar) mv \"$f\" \"$HOME/Downloads/Archives/\" ;; *) mv \"$f\" \"$HOME/Downloads/Others/\" ;; esac; done"}], "domain": "File System Management"}
{"intent": "Recursively change ownership of the project directory /srv/project to user and group 'deploy', set directories to 755 permissions and files to 644 permissions.", "plan": [{"command": "chown -R deploy:deploy /srv/project"}, {"command": "find /srv/project -type d -exec chmod 755 {} +"}, {"command": "find /srv/project -type f -exec chmod 644 {} +"}], "domain": "File System Management"}
{"intent": "Generate a disk\u2011usage report showing the top 10 largest directories under /home, save it to ~/disk_report.txt, and email the report to the user.", "plan": [{"command": "du -h --max-depth=1 /home | sort -hr | head -n 10 > \"$HOME/disk_report.txt\""}, {"command": "mail -s \"Top 10 Home Directory Sizes\" \"$USER\" < \"$HOME/disk_report.txt\""}], "domain": "File System Management"}
{"intent": "Create a daily backup of /etc using tar, store the compressed archive in /backup/etc with a timestamped filename, and keep only the most recent 7 backups.", "plan": [{"command": "mkdir -p /backup/etc"}, {"command": "backup_file=\"/backup/etc/etc-backup-$(date +%Y%m%d-%H%M%S).tar.gz\""}, {"command": "tar -czf \"$backup_file\" /etc"}, {"command": "ls -1t /backup/etc/etc-backup-*.tar.gz | tail -n +8 | xargs -d '\\n' rm -f"}], "domain": "File System Management"}
{"intent": "Find all log files larger than 100 MB in /var/log that are older than 30 days, compress them with gzip, and move the compressed archives to /var/archive/logs.", "plan": [{"command": "mkdir -p /var/archive/logs"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -mtime +30 -print0 | while IFS= read -r -d '' file; do gzip -c \"$file\" > \"${file}.gz\" && mv \"${file}.gz\" /var/archive/logs/; done"}], "domain": "File System Management"}
{"intent": "Organize the user's Downloads folder by moving files into subdirectories based on their MIME type (images, documents, archives, others).", "plan": [{"command": "mkdir -p ~/Downloads/{Images,Documents,Archives,Others}"}, {"command": "for f in ~/Downloads/*; do if [ -f \"$f\" ]; then mime=$(file --mime-type -b \"$f\"); case \"$mime\" in image/*) mv \"$f\" ~/Downloads/Images/ ;; application/pdf|application/msword|application/vnd.openxmlformats-officedocument.wordprocessingml.document|text/plain) mv \"$f\" ~/Downloads/Documents/ ;; application/zip|application/x-tar|application/gzip|application/x-bzip2) mv \"$f\" ~/Downloads/Archives/ ;; *) mv \"$f\" ~/Downloads/Others/ ;; esac; fi; done"}], "domain": "File System Management"}
{"intent": "Recursively set directory permissions to 750 and file permissions to 640 in /srv/www, and change the group ownership to www-data.", "plan": [{"command": "chgrp -R www-data /srv/www"}, {"command": "find /srv/www -type d -exec chmod 750 {} +"}, {"command": "find /srv/www -type f -exec chmod 640 {} +"}], "domain": "File System Management"}
{"intent": "Generate a disk usage report for /home, listing the top 10 largest directories, and save the report to ~/disk_report.txt.", "plan": [{"command": "du -h --max-depth=1 /home | sort -hr | head -n 10 > ~/disk_report.txt"}, {"command": "echo \"Disk usage report generated on $(date)\" >> ~/disk_report.txt"}], "domain": "File System Management"}
{"intent": "Create a snapshot of the /etc configuration by copying all files while preserving attributes to a timestamped backup directory and archiving it as a tar.gz file.", "plan": [{"command": "backup_dir=\"/backup/etc_$(date +%F_%H-%M-%S)\" && mkdir -p \"$backup_dir\""}, {"command": "cp -a /etc/* \"$backup_dir/\""}, {"command": "tar -czf \"${backup_dir}.tar.gz\" -C \"/backup\" \"$(basename \"$backup_dir\")\""}, {"command": "rm -rf \"$backup_dir\""}], "domain": "File System Management"}
{"intent": "Find all log files larger than 100\u202fMiB in /var/log, compress them with gzip, and move the archives to /var/archive/logs.", "plan": [{"command": "mkdir -p /var/archive/logs"}, {"command": "find /var/log -type f -name \"*.log\" -size +100M -exec gzip -k {} \\;"}, {"command": "find /var/log -type f -name \"*.log.gz\" -size +100M -exec mv {} /var/archive/logs/ \\;"}], "domain": "File System Management"}
{"intent": "Organize the user's Downloads folder by file type, placing images, documents, archives, and others into separate sub\u2011directories.", "plan": [{"command": "mkdir -p ~/Downloads/{Images,Documents,Archives,Others}"}, {"command": "find ~/Downloads -maxdepth 1 -type f \\( -iname \"*.jpg\" -o -iname \"*.png\" -o -iname \"*.gif\" \\) -exec mv {} ~/Downloads/Images/ \\;"}, {"command": "find ~/Downloads -maxdepth 1 -type f \\( -iname \"*.pdf\" -o -iname \"*.docx\" -o -iname \"*.txt\" \\) -exec mv {} ~/Downloads/Documents/ \\;"}, {"command": "find ~/Downloads -maxdepth 1 -type f \\( -iname \"*.zip\" -o -iname \"*.tar.gz\" -o -iname \"*.rar\" \\) -exec mv {} ~/Downloads/Archives/ \\;"}, {"command": "find ~/Downloads -maxdepth 1 -type f -not -name \"*.*\" -exec mv {} ~/Downloads/Others/ \\;"}], "domain": "File System Management"}
{"intent": "Recursively change ownership of the /opt/project directory to user 'deploy' and group 'deploy', preserving existing permissions.", "plan": [{"command": "chown -R deploy:deploy /opt/project"}], "domain": "File System Management"}
{"intent": "Generate a detailed disk\u2011usage report for /home, summarize directories exceeding 5\u202fGiB, and email the report to admin@example.com.", "plan": [{"command": "du -h --max-depth=2 /home > /tmp/home_disk_usage.txt"}, {"command": "awk '$1 ~ /[0-9]+[G]$/ && $1+0 > 5 {print}' /tmp/home_disk_usage.txt > /tmp/home_large_dirs.txt"}, {"command": "mail -s \"Home directory disk usage report\" admin@example.com < /tmp/home_large_dirs.txt"}], "domain": "File System Management"}
{"intent": "Create a read\u2011only snapshot of /etc configuration files using rsync, store it in /var/backups/etc_snapshot_$(date +%F), and set appropriate permissions.", "plan": [{"command": "snapshot_dir=\"/var/backups/etc_snapshot_$(date +%F)\"; mkdir -p \"$snapshot_dir\""}, {"command": "rsync -a --link-dest=/etc \"$snapshot_dir\" /etc"}, {"command": "chmod -R a-w \"$snapshot_dir\""}, {"command": "chmod -R u+r \"$snapshot_dir\""}], "domain": "File System Management"}
{"intent": "Undo the last commit but keep the changes staged for further editing.", "plan": [{"command": "git reset --soft HEAD~1"}], "domain": "Git Version Control"}
{"intent": "Identify the commit that introduced a bug in the file src/main.rs using bisect.", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good $(git rev-list --max-parents=0 HEAD)"}, {"command": "git bisect run bash -c 'gcc -o /dev/null src/main.rs && echo ok || echo fail'"}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize the local repository with the upstream remote and rebase local commits on top of the updated upstream/main branch.", "plan": [{"command": "git fetch upstream"}, {"command": "git rebase upstream/main"}], "domain": "Git Version Control"}
{"intent": "Create a new feature branch from develop, cherry\u2011pick two specific commits from another branch, and push the new branch to origin.", "plan": [{"command": "git checkout develop"}, {"command": "git pull origin develop"}, {"command": "git checkout -b feature/new-awesome-feature"}, {"command": "git cherry-pick abc1234 def5678"}, {"command": "git push -u origin feature/new-awesome-feature"}], "domain": "Git Version Control"}
{"intent": "Create a signed tag for the current commit with version v1.2.3 and push the tag to the remote repository.", "plan": [{"command": "git tag -s v1.2.3 -m \"Release version 1.2.3\""}, {"command": "git push origin v1.2.3"}], "domain": "Git Version Control"}
{"intent": "Undo the last commit but keep the changes staged for further editing", "plan": [{"command": "git reset --soft HEAD~1"}], "domain": "Git Version Control"}
{"intent": "Identify the commit that introduced a bug in src/main.rs using bisect", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good $(git rev-list --max-parents=0 HEAD)"}, {"command": "git bisect run bash -c 'gcc -o /dev/null src/main.rs && echo ok || exit 1'"}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize the local repository with the upstream remote and rebase local commits on top of the latest upstream changes", "plan": [{"command": "git fetch upstream"}, {"command": "git checkout main"}, {"command": "git rebase upstream/main"}], "domain": "Git Version Control"}
{"intent": "Create a new feature branch from develop, cherry\u2011pick two specific commits, and push the branch to origin", "plan": [{"command": "git checkout develop"}, {"command": "git pull origin develop"}, {"command": "git checkout -b feature/awesome-improvement"}, {"command": "git cherry-pick abc1234 def5678"}, {"command": "git push -u origin feature/awesome-improvement"}], "domain": "Git Version Control"}
{"intent": "Squash the last three commits on a feature branch into a single commit with a custom message before merging into main", "plan": [{"command": "git checkout feature/awesome-improvement"}, {"command": "git pull --rebase origin feature/awesome-improvement"}, {"command": "git reset --soft HEAD~3"}, {"command": "git commit -m \"Add awesome improvement with consolidated changes\""}, {"command": "git checkout main"}, {"command": "git merge --no-ff feature/awesome-improvement"}, {"command": "git push origin main"}], "domain": "Git Version Control"}
{"intent": "Undo the last commit but keep the changes staged for a new commit", "plan": [{"command": "git reset --soft HEAD~1"}], "domain": "Git Version Control"}
{"intent": "Identify the commit that introduced a bug in src/main.rs using bisect", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good <known-good-commit-hash>"}, {"command": "git bisect run ./run_tests.sh"}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize the local main branch with the upstream remote and rebase local changes on top of it", "plan": [{"command": "git fetch upstream"}, {"command": "git checkout main"}, {"command": "git rebase upstream/main"}, {"command": "git push --force-with-lease"}], "domain": "Git Version Control"}
{"intent": "Create a new feature branch from develop, set the upstream tracking branch, and push it to origin", "plan": [{"command": "git checkout develop"}, {"command": "git pull origin develop"}, {"command": "git checkout -b feature/awesome-feature"}, {"command": "git push -u origin feature/awesome-feature"}], "domain": "Git Version Control"}
{"intent": "Cherry\u2011pick a specific commit from another branch onto main, preferring our changes in case of conflicts", "plan": [{"command": "git checkout main"}, {"command": "git fetch origin"}, {"command": "git cherry-pick -X ours <commit-sha>"}], "domain": "Git Version Control"}
{"intent": "Create a new feature branch from develop, cherry-pick specific commits from master, and push the branch to origin.", "plan": [{"command": "git fetch origin"}, {"command": "git checkout -b feature/awesome-feature origin/develop"}, {"command": "git cherry-pick abcdef1 ghijkl2"}, {"command": "git push -u origin feature/awesome-feature"}], "domain": "Git Version Control"}
{"intent": "Squash the last five commits on the current branch into a single commit with a custom message.", "plan": [{"command": "git reset --soft HEAD~5"}, {"command": "git commit -m \"Combined feature implementation\""}, {"command": "git log -1 --oneline"}], "domain": "Git Version Control"}
{"intent": "Remove a large binary file (assets/video.mp4) from the entire repository history and force\u2011push the cleaned history.", "plan": [{"command": "git filter-branch --force --index-filter 'git rm --cached --ignore-unmatch assets/video.mp4' --prune-empty --tag-name-filter cat -- --all"}, {"command": "git push --force-with-lease origin --all"}, {"command": "git push --force-with-lease origin --tags"}], "domain": "Git Version Control"}
{"intent": "Temporarily stash uncommitted changes, pull the latest updates from origin/main, apply the stash, and continue working.", "plan": [{"command": "git stash push -m \"temporary stash\""}, {"command": "git checkout main"}, {"command": "git pull origin main"}, {"command": "git stash pop"}], "domain": "Git Version Control"}
{"intent": "Create a patch between tags v1.2.0 and v1.3.0 and apply that patch to the release branch.", "plan": [{"command": "git format-patch v1.2.0..v1.3.0 -o /tmp/patches"}, {"command": "git checkout release"}, {"command": "git am /tmp/patches/*.patch"}], "domain": "Git Version Control"}
{"intent": "Undo the last commit but keep the changes staged for editing.", "plan": [{"command": "git reset --soft HEAD~1"}, {"command": "git status"}], "domain": "Git Version Control"}
{"intent": "Find the commit that introduced a regression in the function computeMetrics() located in src/analysis.py using git bisect.", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good <known-good-commit-hash>"}, {"command": "git bisect run bash -c 'python -m pytest src/analysis.py && exit 0 || exit 1'"}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize the local 'feature/login' branch with the remote, rebase it onto the latest 'main' branch, and automatically resolve any merge conflicts by keeping our version of the files.", "plan": [{"command": "git fetch origin"}, {"command": "git checkout feature/login"}, {"command": "git rebase origin/main"}, {"command": "git checkout --ours . && git add ."}, {"command": "git rebase --continue || true"}, {"command": "git push --force-with-lease origin feature/login"}], "domain": "Git Version Control"}
{"intent": "Create a temporary branch from the current HEAD, cherry\u2011pick three specific commits, and generate a patch file that shows the differences between this temporary branch and the original branch.", "plan": [{"command": "git branch temp-branch"}, {"command": "git checkout temp-branch"}, {"command": "git cherry-pick <commit-hash-1> <commit-hash-2> <commit-hash-3>"}, {"command": "git diff $(git rev-parse --abbrev-ref @{-1})...temp-branch > temp_changes.patch"}, {"command": "git checkout $(git rev-parse --abbrev-ref @{-1})"}, {"command": "git branch -D temp-branch"}], "domain": "Git Version Control"}
{"intent": "Stash current uncommitted work, switch to the 'release/v2.0' tag, create a new hotfix branch, apply the stash, commit the changes with an appropriate message, and push the hotfix branch to the remote.", "plan": [{"command": "git stash push -m 'temp changes'"}, {"command": "git checkout tags/release/v2.0 -b hotfix/v2.0.1"}, {"command": "git stash apply"}, {"command": "git add ."}, {"command": "git commit -m 'Hotfix: resolve issue causing crash on startup'"}, {"command": "git push -u origin hotfix/v2.0.1"}], "domain": "Git Version Control"}
{"intent": "Revert a specific merge commit without affecting later commits.", "plan": [{"command": "git log --graph --oneline --decorate"}, {"command": "git revert -m 1 <merge-commit-sha>"}, {"command": "git push origin HEAD"}], "domain": "Git Version Control"}
{"intent": "Split a large accidental commit into several smaller, logical commits.", "plan": [{"command": "git reset --soft HEAD~1"}, {"command": "git add -p"}, {"command": "git commit -m \"First part of split commit\""}, {"command": "git add -p"}, {"command": "git commit -m \"Second part of split commit\""}, {"command": "git add -p"}, {"command": "git commit -m \"Third part of split commit\""}, {"command": "git push --force-with-lease"}], "domain": "Git Version Control"}
{"intent": "Identify the commit that introduced a failing test using bisect.", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good <known-good-commit-sha>"}, {"command": "git bisect run ./run_tests.sh"}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize a fork's main branch with upstream and rebase a local feature branch on top of it.", "plan": [{"command": "git remote add upstream https://github.com/original/repo.git"}, {"command": "git fetch upstream"}, {"command": "git checkout main"}, {"command": "git merge upstream/main"}, {"command": "git push origin main"}, {"command": "git checkout feature-branch"}, {"command": "git rebase main"}, {"command": "git push --force-with-lease"}], "domain": "Git Version Control"}
{"intent": "Purge a large accidentally\u2011committed file from the entire repository history.", "plan": [{"command": "git filter-repo --path path/to/largefile --invert-paths"}, {"command": "git for-each-ref --format=\"%(refname)\" refs/original/ | xargs -n 1 git update-ref -d"}, {"command": "git reflog expire --expire=now --all"}, {"command": "git gc --prune=now --aggressive"}, {"command": "git push --force --all"}, {"command": "git push --force --tags"}], "domain": "Git Version Control"}
{"intent": "Undo the last commit but keep the changes staged for a new commit", "plan": [{"command": "git reset --soft HEAD~1"}], "domain": "Git Version Control"}
{"intent": "Identify the commit that introduced a regression in the file src/main.rs using binary search", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good $(git rev-list --max-parents=0 HEAD)"}, {"command": "git bisect run ./run_tests.sh"}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize the local branch 'feature' with the upstream 'main' branch while preserving a linear history", "plan": [{"command": "git fetch upstream"}, {"command": "git checkout feature"}, {"command": "git rebase upstream/main"}], "domain": "Git Version Control"}
{"intent": "Create a temporary backup of the current working state, then apply a series of cherry\u2011picked commits from another branch without affecting the original branch history", "plan": [{"command": "git stash push -m \"pre\u2011cherry\u2011pick backup\""}, {"command": "git checkout feature"}, {"command": "git cherry-pick abc1234 def5678"}, {"command": "git stash pop"}], "domain": "Git Version Control"}
{"intent": "Generate a detailed visual log of the last 30 commits showing branch merges and tags, and export it to a markdown file", "plan": [{"command": "git log --graph --abbrev-commit --decorate --date=relative -n 30 > git_history.md"}, {"command": "sed -i '1i # Recent Git History' git_history.md"}], "domain": "Git Version Control"}
{"intent": "Undo the last commit but keep the changes staged for a new commit.", "plan": [{"command": "git reset --soft HEAD~1"}, {"command": "git status -s"}], "domain": "Git Version Control"}
{"intent": "Identify the commit that introduced a regression in src/main.rs using bisect.", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good $(git rev-list --max-parents=0 HEAD)"}, {"command": "git bisect run bash -c \"git checkout --quiet src/main.rs && ./run_tests.sh && exit \\$?\""}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize the current branch with the upstream repository and rebase local commits on top of the updated upstream branch.", "plan": [{"command": "git fetch upstream"}, {"command": "git rebase upstream/$(git rev-parse --abbrev-ref HEAD)"}], "domain": "Git Version Control"}
{"intent": "Create a new feature branch from develop, cherry\u2011pick three specific commits, and push the branch to origin.", "plan": [{"command": "git checkout develop"}, {"command": "git pull origin develop"}, {"command": "git checkout -b feature/awesome-improvement"}, {"command": "git cherry-pick abc1234 def5678 9ab0cde"}, {"command": "git push -u origin feature/awesome-improvement"}], "domain": "Git Version Control"}
{"intent": "Squash the last four commits into a single commit, edit its message, and force\u2011push the result to the remote branch.", "plan": [{"command": "git reset --soft HEAD~4"}, {"command": "git commit -m \"<new combined commit message>\""}, {"command": "git push --force-with-lease origin $(git rev-parse --abbrev-ref HEAD)"}], "domain": "Git Version Control"}
{"intent": "Undo the last commit but keep the changes staged for amendment", "plan": [{"command": "git reset --soft HEAD~1"}, {"command": "git status -s"}], "domain": "Git Version Control"}
{"intent": "Identify the commit that introduced a regression in the function `processData` within src/utils.js", "plan": [{"command": "git log -p -S'processData' -- src/utils.js"}, {"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good $(git rev-list --max-parents=0 HEAD)"}, {"command": "git bisect run ./test_regression.sh"}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize the local feature branch with the upstream `develop` branch while preserving a linear history", "plan": [{"command": "git fetch upstream"}, {"command": "git checkout feature/my-feature"}, {"command": "git rebase upstream/develop"}, {"command": "git push --force-with-lease origin feature/my-feature"}], "domain": "Git Version Control"}
{"intent": "Create a temporary branch to test a hotfix, cherry\u2011pick the relevant commits, and submit a pull request", "plan": [{"command": "git checkout -b hotfix/tmp-test origin/main"}, {"command": "git cherry-pick abc1234 def5678"}, {"command": "git push origin hotfix/tmp-test"}, {"command": "gh pr create --base main --head hotfix/tmp-test --title \"Test hotfix\" --body \"Temporary branch for hotfix verification\""}], "domain": "Git Version Control"}
{"intent": "Generate a detailed changelog of all changes merged into `release/v2.0` since version 1.8, grouped by author", "plan": [{"command": "git checkout release/v2.0"}, {"command": "git log --pretty=format:'%h %ad %an %s' --date=short v1.8..HEAD | sort -k3,3 | uniq -c | sort -nr"}, {"command": "git log --pretty=format:'%h %ad %an %s' --date=short v1.8..HEAD > changelog_v1.8_to_v2.0.txt"}], "domain": "Git Version Control"}
{"intent": "Undo the last commit but keep the changes staged for further editing.", "plan": [{"command": "git reset --soft HEAD~1"}], "domain": "Git Version Control"}
{"intent": "Identify the exact commit that introduced a bug in src/main.rs using binary search.", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good <known-good-commit>"}, {"command": "git bisect run ./run-tests.sh src/main.rs"}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize the local repository with the upstream remote and rebase local changes on top of the latest upstream/main.", "plan": [{"command": "git fetch upstream"}, {"command": "git checkout main"}, {"command": "git rebase upstream/main"}], "domain": "Git Version Control"}
{"intent": "Create a new feature branch from develop, cherry\u2011pick two specific commits from the hotfix branch, and push the new branch to origin.", "plan": [{"command": "git checkout develop"}, {"command": "git checkout -b feature/add-logging"}, {"command": "git cherry-pick abc1234 def5678"}, {"command": "git push -u origin feature/add-logging"}], "domain": "Git Version Control"}
{"intent": "Rewrite the author name and email for the last three commits without altering the commit contents.", "plan": [{"command": "git rebase -i HEAD~3"}, {"command": "# In the interactive editor, change each line from 'pick' to 'edit', then for each commit run:"}, {"command": "git commit --amend --author='New Author <new.author@example.com>' --no-edit"}, {"command": "git rebase --continue"}], "domain": "Git Version Control"}
{"intent": "Undo the last commit but keep the changes staged for further editing.", "plan": [{"command": "git reset --soft HEAD~1"}], "domain": "Git Version Control"}
{"intent": "Identify the commit that introduced a bug in src/main.rs using bisect.", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good <known-good-commit-hash>"}, {"command": "git bisect run ./run-tests.sh src/main.rs"}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize the local branch with the upstream repository and reapply local commits on top of it.", "plan": [{"command": "git fetch upstream"}, {"command": "git rebase upstream/main"}], "domain": "Git Version Control"}
{"intent": "Create a new feature branch from develop, cherry\u2011pick two specific commits, and push the branch to origin with tracking.", "plan": [{"command": "git checkout -b feature/awesome-feature develop"}, {"command": "git cherry-pick <commit-hash-1> <commit-hash-2>"}, {"command": "git push -u origin feature/awesome-feature"}], "domain": "Git Version Control"}
{"intent": "Remove remote\u2011tracking branches that have already been merged and prune stale references.", "plan": [{"command": "git fetch --prune"}, {"command": "git branch -r --merged | grep -v '\\->' | while read remote; do git push origin --delete \"${remote#origin/}\"; done"}, {"command": "git remote prune origin"}], "domain": "Git Version Control"}
{"intent": "Undo the last commit but keep the changes staged for amendment.", "plan": [{"command": "git reset --soft HEAD~1"}], "domain": "Git Version Control"}
{"intent": "Identify the exact commit that introduced a regression in src/main.rs using bisect.", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good $(git rev-parse HEAD~20)"}, {"command": "git bisect run bash -c 'if grep -q \"bug signature\" src/main.rs; then exit 1; else exit 0; fi'"}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize the local main branch with the upstream repository and reapply local commits on top of it.", "plan": [{"command": "git fetch upstream"}, {"command": "git checkout main"}, {"command": "git rebase upstream/main"}], "domain": "Git Version Control"}
{"intent": "Create a new feature branch from develop, cherry-pick two specific commits, and push the branch to origin.", "plan": [{"command": "git checkout develop"}, {"command": "git pull origin develop"}, {"command": "git checkout -b feature/awesome-improvement"}, {"command": "git cherry-pick abcdef1"}, {"command": "git cherry-pick 1234567"}, {"command": "git push -u origin feature/awesome-improvement"}], "domain": "Git Version Control"}
{"intent": "Merge a long-lived release branch into main, preferring the release version in case of conflicts, and tag the new release.", "plan": [{"command": "git checkout main"}, {"command": "git pull origin main"}, {"command": "git merge --strategy-option theirs release/v2.3"}, {"command": "git tag -a v2.3.0 -m \"Release v2.3.0 after merging release branch\""}, {"command": "git push origin main --tags"}], "domain": "Git Version Control"}
{"intent": "Undo the last commit but keep the changes staged for further editing.", "plan": [{"command": "git reset --soft HEAD~1"}], "domain": "Git Version Control"}
{"intent": "Identify the commit that introduced a regression in the file src/main.rs using bisect.", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good <known-good-commit-hash>"}, {"command": "git bisect run ./run_tests.sh"}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize the local repository with the upstream remote and reapply local commits on top of the updated main branch.", "plan": [{"command": "git fetch upstream"}, {"command": "git checkout main"}, {"command": "git rebase upstream/main"}], "domain": "Git Version Control"}
{"intent": "Create a new feature branch from the develop branch, cherry\u2011pick two specific commits, and push the branch to the origin remote with tracking enabled.", "plan": [{"command": "git checkout develop"}, {"command": "git checkout -b feature/my-new-feature"}, {"command": "git cherry-pick abcdef1 ghijkl2"}, {"command": "git push -u origin feature/my-new-feature"}], "domain": "Git Version Control"}
{"intent": "Combine the last four commits on the current branch into a single commit with a new message, then force\u2011push the rewritten history to the remote branch safely.", "plan": [{"command": "git reset --soft HEAD~4"}, {"command": "git commit -m \"Combined feature work and cleaned up history\""}, {"command": "git push --force-with-lease"}], "domain": "Git Version Control"}
{"intent": "Undo the last commit but keep the changes staged for a new commit.", "plan": [{"command": "git reset --soft HEAD~1"}, {"command": "git status"}], "domain": "Git Version Control"}
{"intent": "Identify the commit that introduced a bug in src/main.rs using bisect.", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good $(git rev-list --max-parents=0 HEAD)"}, {"command": "git bisect run bash -c 'git checkout --quiet src/main.rs && ./run-tests.sh && exit $?' "}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize the local repository with the upstream remote and rebase local commits on top of the latest upstream/main.", "plan": [{"command": "git fetch upstream"}, {"command": "git checkout main"}, {"command": "git rebase upstream/main"}, {"command": "git push origin main --force-with-lease"}], "domain": "Git Version Control"}
{"intent": "Create a new feature branch from develop, cherry\u2011pick three specific commits, and push the branch to origin.", "plan": [{"command": "git checkout develop"}, {"command": "git pull origin develop"}, {"command": "git checkout -b feature/awesome-feature"}, {"command": "git cherry-pick abc1234 def5678 9ab0cde"}, {"command": "git push -u origin feature/awesome-feature"}], "domain": "Git Version Control"}
{"intent": "Squash the last four commits on a feature branch into a single commit with a new message, then force\u2011push the result.", "plan": [{"command": "git checkout feature/awesome-feature"}, {"command": "git pull --rebase origin feature/awesome-feature"}, {"command": "git reset --soft HEAD~4"}, {"command": "git commit -m \"Implement awesome feature with consolidated changes\""}, {"command": "git push origin feature/awesome-feature --force-with-lease"}], "domain": "Git Version Control"}
{"intent": "Undo the last commit but keep the changes staged for further editing.", "plan": [{"command": "git reset --soft HEAD~1"}], "domain": "Git Version Control"}
{"intent": "Identify the exact commit that introduced a regression in the file src/main.rs using bisect.", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good $(git rev-parse HEAD~50)"}, {"command": "git bisect run ./run_tests.sh"}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize the local main branch with the upstream repository and reapply local commits on top of it.", "plan": [{"command": "git fetch upstream"}, {"command": "git checkout main"}, {"command": "git rebase upstream/main"}], "domain": "Git Version Control"}
{"intent": "Apply a specific feature commit from a remote feature branch onto the current working branch without merging the whole branch.", "plan": [{"command": "git fetch origin feature-branch"}, {"command": "git cherry-pick <commit-hash>"}], "domain": "Git Version Control"}
{"intent": "Clean up local and remote branches that have already been merged into the main branch.", "plan": [{"command": "git fetch --prune origin"}, {"command": "git branch --merged main | grep -v '\\*' | xargs -n 1 git branch -d"}, {"command": "git branch -r --merged origin/main | grep -v 'origin/main' | sed 's#origin/##' | xargs -n 1 -I {} git push origin --delete {}"}], "domain": "Git Version Control"}
{"intent": "Undo the last commit but keep the changes staged for amendment", "plan": [{"command": "git reset --soft HEAD~1"}], "domain": "Git Version Control"}
{"intent": "Identify the commit that introduced a regression in the file src/main.rs using bisect", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good $(git rev-list --max-parents=0 HEAD)"}, {"command": "git bisect run ./run_tests.sh src/main.rs"}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize the local feature branch with the upstream master, rebasing local commits on top of the latest master", "plan": [{"command": "git fetch upstream"}, {"command": "git checkout feature-branch"}, {"command": "git rebase upstream/master"}], "domain": "Git Version Control"}
{"intent": "Create a new release branch from the current develop branch, cherry\u2011pick only the hot\u2011fix commits, and push it to origin", "plan": [{"command": "git checkout develop"}, {"command": "git pull origin develop"}, {"command": "git checkout -b release-$(date +%Y-%m-%d)"}, {"command": "git cherry-pick abc1234 def5678"}, {"command": "git push -u origin HEAD"}], "domain": "Git Version Control"}
{"intent": "Rewrite history to remove a large accidentally committed binary file from the last 5 commits and force\u2011push the cleaned branch", "plan": [{"command": "git filter-repo --path path/to/large.bin --invert-paths --force"}, {"command": "git reset --hard HEAD~5"}, {"command": "git rebase -i HEAD~5"}, {"command": "git push --force-with-lease origin $(git rev-parse --abbrev-ref HEAD)"}], "domain": "Git Version Control"}
{"intent": "Undo the last commit but keep the changes staged for editing or recommitting.", "plan": [{"command": "git reset --soft HEAD~1"}], "domain": "Git Version Control"}
{"intent": "Identify the exact commit that introduced a bug in the file src/main.rs using bisect.", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good <known-good-commit-hash>"}, {"command": "git bisect run bash -c 'git checkout src/main.rs && ./run-tests.sh && exit $?' "}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize the local repository with the upstream remote and rebase local commits on top of the updated upstream/main branch.", "plan": [{"command": "git fetch upstream"}, {"command": "git rebase upstream/main"}], "domain": "Git Version Control"}
{"intent": "Create a new feature branch from the develop branch, push it to origin, and set the upstream tracking branch.", "plan": [{"command": "git checkout develop"}, {"command": "git pull origin develop"}, {"command": "git checkout -b feature/awesome-feature"}, {"command": "git push -u origin feature/awesome-feature"}], "domain": "Git Version Control"}
{"intent": "Combine the last three local commits into a single commit with a custom message.", "plan": [{"command": "git reset --soft HEAD~3"}, {"command": "git commit -m \"Combined feature: refactor core logic and add tests\""}], "domain": "Git Version Control"}
{"intent": "Revert a faulty merge commit while preserving the original branch history.", "plan": [{"command": "git log --graph --oneline --decorate"}, {"command": "git revert -m 1 <merge-commit-hash>"}, {"command": "git push origin HEAD"}], "domain": "Git Version Control"}
{"intent": "Create a new feature branch from the latest stable tag, apply a series of patches, and push it for review.", "plan": [{"command": "git fetch --tags origin"}, {"command": "latest_tag=$(git describe --tags `git rev-list --tags --max-count=1`)"}, {"command": "git checkout -b feature/awesome-feature $latest_tag"}, {"command": "git am /path/to/patches/*.patch"}, {"command": "git push -u origin feature/awesome-feature"}], "domain": "Git Version Control"}
{"intent": "Identify the commit that introduced a regression in the 'authentication' module using bisect, then tag it for future reference.", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good $(git rev-parse HEAD~30)"}, {"command": "git bisect run ./run-tests.sh authentication"}, {"command": "git tag -a regression-auth-$(date +%Y%m%d) -m \"Introduced regression in authentication\" $(git rev-parse HEAD)"}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize a forked repository with upstream, rebase local feature branches onto the updated upstream main, and force\u2011push the rebased branches.", "plan": [{"command": "git remote add upstream https://github.com/original/project.git || true"}, {"command": "git fetch upstream"}, {"command": "git checkout main"}, {"command": "git rebase upstream/main"}, {"command": "git push origin main --force-with-lease"}, {"command": "for branch in $(git branch --format='%(refname:short)' | grep '^feature/'); do git checkout $branch && git rebase main && git push origin $branch --force-with-lease; done"}], "domain": "Git Version Control"}
{"intent": "Generate a detailed changelog of all commits between two release tags, format it in Markdown, and save to a file for release notes.", "plan": [{"command": "git fetch --tags origin"}, {"command": "git log --pretty=format:\"- %h %s (%an, %ad)\" --date=short v1.2.0..v1.3.0 > CHANGELOG_v1.2.0_to_v1.3.0.md"}, {"command": "echo \"# Changelog v1.2.0 \u2192 v1.3.0\" | cat - CHANGELOG_v1.2.0_to_v1.3.0.md > temp && mv temp CHANGELOG_v1.2.0_to_v1.3.0.md"}], "domain": "Git Version Control"}
{"intent": "Undo the last commit but keep the changes staged for amendment", "plan": [{"command": "git reset --soft HEAD~1"}], "domain": "Git Version Control"}
{"intent": "Identify the commit that introduced a regression in the file src/main.rs using binary search", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good $(git rev-list --max-parents=0 HEAD)"}, {"command": "git bisect run bash -c 'make test && ./run_tests.sh src/main.rs'"}, {"command": "git bisect reset"}], "domain": "Git Version Control"}
{"intent": "Synchronize the local branch with the upstream repository and rebase local commits on top of the updated upstream", "plan": [{"command": "git fetch upstream"}, {"command": "git checkout my-feature-branch"}, {"command": "git rebase upstream/main"}], "domain": "Git Version Control"}
{"intent": "Create a new feature branch from the current main branch, switch to it, and push it to the origin remote for collaboration", "plan": [{"command": "git checkout main"}, {"command": "git pull origin main"}, {"command": "git checkout -b feature/awesome-new-feature"}, {"command": "git push -u origin feature/awesome-new-feature"}], "domain": "Git Version Control"}
{"intent": "Merge a long-lived development branch into main using a squash commit to keep history linear", "plan": [{"command": "git checkout main"}, {"command": "git pull origin main"}, {"command": "git merge --squash develop"}, {"command": "git commit -m \"Squash merge develop into main: consolidate changes\""}, {"command": "git push origin main"}], "domain": "Git Version Control"}
{"intent": "Rewrite the last three commits to amend their messages and squash them into a single commit while preserving the working tree changes.", "plan": [{"command": "git rebase -i HEAD~3"}, {"command": "git commit --amend -m \"Combined commit message\""}], "domain": "Git Version Control"}
{"intent": "Identify the commit that introduced a regression in the file lib/utils.py using binary search through the history.", "plan": [{"command": "git bisect start"}, {"command": "git bisect bad"}, {"command": "git bisect good $(git rev-list --max-parents=0 HEAD)"}, {"command": "git bisect run ./run_tests.sh lib/utils.py"}], "domain": "Git Version Control"}
{"intent": "Create a new feature branch from the latest upstream develop, cherry-pick two specific commits from another branch, and push the new branch to origin.", "plan": [{"command": "git fetch upstream"}, {"command": "git checkout -b feature/new-awesome-feature upstream/develop"}, {"command": "git cherry-pick abc1234 def5678"}, {"command": "git push -u origin feature/new-awesome-feature"}], "domain": "Git Version Control"}
{"intent": "Synchronize a forked repository with the upstream master, rebase local feature work on top of it, and resolve any conflicts automatically using the ours strategy for conflicting files.", "plan": [{"command": "git remote add upstream https://github.com/original/repo.git || true"}, {"command": "git fetch upstream"}, {"command": "git checkout feature/my-work"}, {"command": "git rebase -X ours upstream/master"}], "domain": "Git Version Control"}
{"intent": "Generate a concise changelog of all merged pull requests between tag v1.2.0 and the current HEAD, including author and PR number.", "plan": [{"command": "git log --pretty=format:'%h %s' --grep='Merge pull request' v1.2.0..HEAD > changelog.txt"}, {"command": "sed -E 's/Merge pull request #([0-9]+) from .*/- PR #\\1/' changelog.txt"}], "domain": "Git Version Control"}
{"intent": "List the top 5 processes consuming the most memory.", "plan": [{"command": "ps aux --sort=-%mem | head -n 6"}], "domain": "System Information & Monitoring"}
{"intent": "Check available disk space on the root partition and warn if less than 10% free.", "plan": [{"command": "df -h /"}, {"command": "df -h / | awk 'NR==2 {gsub(/%/, \"\"); if ($5+0 > 90) print \"WARNING: Root partition usage is above 90%\"}'"}], "domain": "System Information & Monitoring"}
{"intent": "Find the process ID of the service listening on TCP port 8080.", "plan": [{"command": "ss -ltnp 'sport = :8080' | awk '{print $6}' | cut -d',' -f2 | cut -d'=' -f2"}], "domain": "System Information & Monitoring"}
{"intent": "Show the 15\u2011minute load average and the total number of running processes.", "plan": [{"command": "uptime | awk -F'load average:' '{print \"Load averages (1,5,15 min):\" $2}'"}, {"command": "ps -e --no-headers | wc -l | awk '{print \"Total running processes:\" $1}'"}], "domain": "System Information & Monitoring"}
{"intent": "Generate a concise hardware summary: CPU model, total RAM, and total disk capacity.", "plan": [{"command": "lscpu | grep -i 'model name' | sed 's/.*:\\s*//' | awk '{print \"CPU Model:\" $0}'"}, {"command": "free -h | awk '/Mem:/ {print \"Total RAM:\" $2}'"}, {"command": "lsblk -b -d -o SIZE | awk 'NR>1 {sum+=$1} END {printf \"Total Disk Capacity: %.2f GB\\n\", sum/1024/1024/1024}'"}], "domain": "System Information & Monitoring"}
{"intent": "Identify the top 5 processes consuming the most CPU along with their command lines.", "plan": [{"command": "ps -eo pid,comm,%cpu,%mem,cmd --sort=-%cpu | head -n 6"}, {"command": "echo \"---\" && ps -p $(ps -eo pid --sort=-%cpu | head -n 6 | tail -n +2) -o pid,cmd"}], "domain": "System Information & Monitoring"}
{"intent": "Monitor real\u2011time network bandwidth usage per interface for 30 seconds.", "plan": [{"command": "ifstat -t 1 30"}, {"command": "echo \"Monitoring completed.\""}], "domain": "System Information & Monitoring"}
{"intent": "Collect detailed hardware information (CPU, memory, disks) for documentation.", "plan": [{"command": "lscpu"}, {"command": "free -h"}, {"command": "lsblk -o NAME,SIZE,TYPE,MOUNTPOINT"}, {"command": "sudo lshw -short"}], "domain": "System Information & Monitoring"}
{"intent": "Find all processes that have been running longer than 24 hours and export the list to a file.", "plan": [{"command": "ps -eo pid,etimes,cmd --no-headers | awk '$2 > 86400 {printf \"%s %s %s\\n\", $1, $2, $3}' > long_running_processes.txt"}, {"command": "echo \"Processes longer than 24h saved to long_running_processes.txt\""}], "domain": "System Information & Monitoring"}
{"intent": "Check current system load averages and alert if the 1\u2011minute load exceeds the number of CPU cores.", "plan": [{"command": "load=$(cut -d' ' -f1 /proc/loadavg); cores=$(nproc); echo \"Load: $load, Cores: $cores\""}, {"command": "if (( $(echo \"$load > $cores\" | bc -l) )); then echo \"ALERT: Load exceeds CPU core count!\"; else echo \"Load is within acceptable range.\"; fi"}], "domain": "System Information & Monitoring"}
{"intent": "Identify the top 5 CPU\u2011intensive processes over the last minute and display their PID, user, and command line.", "plan": [{"command": "ps -eo pid,user,pcpu,comm --sort=-pcpu | head -n 6"}], "domain": "System Information & Monitoring"}
{"intent": "Show a summary of memory usage, including total, used, free, and buffers/cache, and list processes using more than 100\u202fMB of resident memory.", "plan": [{"command": "free -h"}, {"command": "ps -eo pid,user,rss,comm --sort=-rss | awk 'NR==1 || $3>102400 {printf \"%5s %10s %8s %s\\n\", $1, $2, $3/1024 \"M\", $4}'"}], "domain": "System Information & Monitoring"}
{"intent": "Report disk utilization for all mounted filesystems, highlight any that are above 80\u202f% usage, and suggest cleanup commands.", "plan": [{"command": "df -hP | awk 'NR==1 || $5+0>80 {print}'"}, {"command": "echo \"Consider cleaning up with: sudo du -sh /var/log/* | sort -h\""}], "domain": "System Information & Monitoring"}
{"intent": "Determine which services are listening on TCP ports 8000\u20118100, show the corresponding PID and program name, and verify if they belong to the current user.", "plan": [{"command": "ss -tlnp 'sport >= :8000 && sport <= :8100' | awk 'NR>1 {print $5, $7}'"}], "domain": "System Information & Monitoring"}
{"intent": "Collect hardware details (CPU model, number of cores, total RAM, and GPU information) and output them in a readable report.", "plan": [{"command": "lscpu | grep -E 'Model name|CPU(s):'"}, {"command": "awk '/MemTotal/ {printf \"Total RAM: %.2f GB\\n\", $2/1024/1024}' /proc/meminfo"}, {"command": "lspci -nn | grep -i vga || echo \"No GPU detected\""}], "domain": "System Information & Monitoring"}
{"intent": "Identify the top 5 processes that have used the most CPU over the last sampling interval", "plan": [{"command": "ps -eo pid,comm,%cpu --sort=-%cpu | head -n 6"}, {"command": "awk 'NR>1 {printf \"PID: %s  Command: %s  CPU%%: %s\\n\", $1, $2, $3}'"}], "domain": "System Information & Monitoring"}
{"intent": "Monitor real\u2011time network bandwidth usage per network interface for 30 seconds", "plan": [{"command": "echo \"Collecting network stats (1\u2011second interval, 30 samples)...\""}, {"command": "sar -n DEV 1 30"}, {"command": "awk 'NR>3 && $2!=\"IFACE\" {printf \"%s %s %s\\n\", $2, $3, $4}'"}], "domain": "System Information & Monitoring"}
{"intent": "Gather a concise hardware summary: CPU model, total RAM, and GPU details", "plan": [{"command": "lscpu | grep 'Model name'"}, {"command": "free -h | grep Mem"}, {"command": "lspci -nn | grep -i vga"}], "domain": "System Information & Monitoring"}
{"intent": "Find all processes that have more than 1000 open file descriptors", "plan": [{"command": "for pid in $(ps -e -o pid=); do count=$(ls /proc/$pid/fd 2>/dev/null | wc -l); if [ \"$count\" -gt 1000 ]; then cmd=$(ps -p $pid -o comm=); echo \"PID: $pid  FD Count: $count  Command: $cmd\"; fi; done"}], "domain": "System Information & Monitoring"}
{"intent": "Generate a short report of disk I/O statistics for each block device over the last 5 samples (1\u2011second interval)", "plan": [{"command": "iostat -xz 1 5"}, {"command": "awk 'NR>3 && $1!=\"Device:\" {printf \"Device: %s  r/s: %s  w/s: %s  rkB/s: %s  wkB/s: %s\\n\", $1, $2, $3, $4, $5}'"}], "domain": "System Information & Monitoring"}
{"intent": "List the top 5 processes consuming the most memory.", "plan": [{"command": "ps -eo pid,comm,%mem --sort=-%mem | head -n 6"}], "domain": "System Information & Monitoring"}
{"intent": "Check available disk space on the root partition and warn if usage exceeds 80%.", "plan": [{"command": "df -h /"}, {"command": "df -h / | awk 'NR==2{if($5+0>80) print \"WARNING: Root partition usage is \" $5 \" (exceeds 80%)\"}'"}], "domain": "System Information & Monitoring"}
{"intent": "Find the process ID of the service listening on TCP port 8080 and display its full command line.", "plan": [{"command": "lsof -iTCP:8080 -sTCP:LISTEN -Pn | awk 'NR==2{print $2}'"}, {"command": "PID=$(lsof -iTCP:8080 -sTCP:LISTEN -Pn | awk 'NR==2{print $2}'); if [ -n \"$PID\" ]; then ps -p $PID -o pid,cmd=; else echo \"No process found on port 8080\"; fi"}], "domain": "System Information & Monitoring"}
{"intent": "Generate a single formatted report summarizing CPU load, memory usage, and swap usage.", "plan": [{"command": "echo \"--- System Resource Summary ---\"; uptime | awk -F'load average:' '{print \"CPU Load Average:\" $2}'"}, {"command": "free -h | awk 'NR==2{printf \"Memory Used: %s / %s (%.2f%%)\\n\", $3,$2,$3/$2*100}'"}, {"command": "free -h | awk 'NR==3{printf \"Swap Used: %s / %s (%.2f%%)\\n\", $3,$2,$3/$2*100}'"}], "domain": "System Information & Monitoring"}
{"intent": "Monitor network bandwidth usage per interface over a 10\u2011second interval and show the interface with the highest average throughput.", "plan": [{"command": "ifstat -t 1 10 > /tmp/ifstat_output.txt"}, {"command": "awk 'NR>3 {for(i=2;i<=NF;i+=2){iface=$(i-1); recv[$iface]+=$i; sent[$iface]+=$(i+1)} } END {for(iface in recv){avg_recv=recv[iface]/10; avg_sent=sent[iface]/10; total=avg_recv+avg_sent; printf \"%s: %.2f KB/s (recv) + %.2f KB/s (sent) = %.2f KB/s\\n\", iface, avg_recv, avg_sent, total} }' /tmp/ifstat_output.txt | sort -k5 -nr | head -n1"}], "domain": "System Information & Monitoring"}
{"intent": "Identify the top 5 processes consuming the most memory and display their PID, user, memory usage, and command line.", "plan": [{"command": "ps -eo pid,user,%mem,command --sort=-%mem | head -n 6"}], "domain": "System Information & Monitoring"}
{"intent": "Continuously monitor CPU usage for 10 seconds at 1\u2011second intervals and compute the average user and system percentages.", "plan": [{"command": "mpstat 1 10 | awk 'NR>3 {user+=$3; sys+=$5; count++} END {printf \"Average User: %.2f%%, Average System: %.2f%%\\n\", user/count, sys/count}'"}], "domain": "System Information & Monitoring"}
{"intent": "Check disk usage of all mounted filesystems and produce a warning for any that are above 80% utilization.", "plan": [{"command": "df -hP | awk 'NR>1 {print $0}'"}, {"command": "df -hP | awk 'NR>1 {gsub(/%/,\"\",$5); if($5+0>80) print \"WARNING: \" $6 \" is \" $5 \"% full\"}'"}], "domain": "System Information & Monitoring"}
{"intent": "List all network interfaces with their IPv4 addresses and show current inbound/outbound traffic statistics per interface.", "plan": [{"command": "ip -o -4 addr show | awk '{print $2, $4}'"}, {"command": "sar -n DEV 1 1 | awk 'NR>3 && $2!~/(Average|IFACE)/ {print $2, \"RX\", $5, \"KB/s\", \"TX\", $6, \"KB/s\"}'"}], "domain": "System Information & Monitoring"}
{"intent": "Collect a comprehensive hardware summary including CPU model, number of cores, total RAM, and GPU details.", "plan": [{"command": "lscpu | grep -E 'Model name|CPU(s):' | sed 's/^ *//' | paste - -"}, {"command": "free -h | awk '/Mem:/ {print \"Total RAM:\", $2}'"}, {"command": "lspci -nn | grep -i 'vga\\|3d' | sed -e 's/^[0-9a-f]*://'"}], "domain": "System Information & Monitoring"}
{"intent": "Identify the top 5 memory\u2011consuming processes and display their PID, user, and memory usage.", "plan": [{"command": "ps -eo pid,user,%mem,command --sort=-%mem | head -n 6"}], "domain": "System Information & Monitoring"}
{"intent": "Show a disk\u2011usage summary for all mounted filesystems, sorted by usage percentage, and highlight any that exceed 80\u202f% utilization.", "plan": [{"command": "df -hP | awk 'NR>1 {print $5 \" \" $0}' | sort -nr | column -t"}, {"command": "df -hP | awk 'NR>1 && $5+0 > 80 {print \"WARNING: \" $6 \" is \" $5 \" full\"}'"}], "domain": "System Information & Monitoring"}
{"intent": "Monitor overall CPU usage over the last minute in 5\u2011second intervals and write the results to /tmp/cpu_monitor.log.", "plan": [{"command": "mkdir -p /tmp && rm -f /tmp/cpu_monitor.log"}, {"command": "for i in {1..12}; do top -bn1 | grep \"Cpu(s)\" | awk '{print $2 + $4}' >> /tmp/cpu_monitor.log; sleep 5; done"}, {"command": "awk '{sum+=$1} END {print \"Average CPU usage: \" sum/NR \"%\"}' /tmp/cpu_monitor.log"}], "domain": "System Information & Monitoring"}
{"intent": "List all network interfaces with their IPv4 address, MAC address, and current inbound/outbound traffic rates.", "plan": [{"command": "ip -br address show | awk '{print $1, $3}'"}, {"command": "cat /sys/class/net/*/address | paste -d' ' <(ls /sys/class/net) -"}, {"command": "sar -n DEV 1 1 | awk 'NR>3 {print $2, $3, $5, $6}'"}], "domain": "System Information & Monitoring"}
{"intent": "Find the process IDs of services listening on any TCP port between 8000 and 8100 and display the associated program names.", "plan": [{"command": "ss -tlnp \"sport = :8000-8100\" | awk 'NR>1 {print $5, $7}'"}, {"command": "awk -F'[=,]' '{for(i=1;i<=NF;i++) if($i==\"pid\") print $i\"=\"$(i+1)}' <<< \"$(ss -tlnp \"sport = :8000-8100\" 2>/dev/null)\" | while IFS== read -r pid val; do echo \"PID $val -> $(ps -p $val -o comm=)\"; done"}], "domain": "System Information & Monitoring"}
{"intent": "Identify the top 5 processes that are consuming the most CPU, including their PID and command name.", "plan": [{"command": "ps -eo pid,comm,%cpu --sort=-%cpu | head -n 6"}], "domain": "System Information & Monitoring"}
{"intent": "Show a breakdown of memory usage per user on the system.", "plan": [{"command": "ps aux --no-header | awk '{user[$1]+=$4} END{for (u in user) printf \"%s\\t%.2f%%\\n\", u, user[u]}' | sort -k2 -nr"}], "domain": "System Information & Monitoring"}
{"intent": "Monitor network bandwidth usage for each network interface in real\u2011time for the next 10 seconds.", "plan": [{"command": "ifstat 1 10"}], "domain": "System Information & Monitoring"}
{"intent": "Retrieve detailed hardware information, including CPU model, number of cores, and total RAM.", "plan": [{"command": "lscpu | grep -E 'Model name|CPU\\(s\\)'"}, {"command": "free -h | grep Mem"}], "domain": "System Information & Monitoring"}
{"intent": "List all systemd services that have failed in the last 24 hours.", "plan": [{"command": "systemctl --failed --no-legend --plain"}, {"command": "journalctl -u $(systemctl --failed --no-legend --plain | awk '{print $1}') --since '24 hours ago' --priority=3"}], "domain": "System Information & Monitoring"}
{"intent": "Identify the top 5 processes consuming the most memory and show their PID, user, and memory usage", "plan": [{"command": "ps aux --sort=-%mem | head -n 6"}], "domain": "System Information & Monitoring"}
{"intent": "Continuously monitor CPU usage and system load, recording the data every 5 seconds for a total of 1 minute", "plan": [{"command": "sar -u 5 12 > cpu_monitor.log"}], "domain": "System Information & Monitoring"}
{"intent": "Display detailed hardware information, including CPU model, number of cores, and total RAM size", "plan": [{"command": "lscpu"}, {"command": "free -h"}], "domain": "System Information & Monitoring"}
{"intent": "List all network interfaces with their assigned IP addresses and show a snapshot of current traffic statistics per interface", "plan": [{"command": "ip -brief address show"}, {"command": "cat /proc/net/dev"}], "domain": "System Information & Monitoring"}
{"intent": "Find all processes that have opened files under /var/log and output their PIDs and command names", "plan": [{"command": "lsof +D /var/log | awk 'NR>1 {print $2, $1}' | sort -u"}], "domain": "System Information & Monitoring"}
{"intent": "Identify the top 5 processes consuming the most memory and display their PID, user, and memory usage", "plan": [{"command": "ps -eo pid,user,%mem,command --sort=-%mem | head -n 6"}], "domain": "System Information & Monitoring"}
{"intent": "Show detailed CPU usage per core over the last 10 seconds", "plan": [{"command": "mpstat -P ALL 1 10"}], "domain": "System Information & Monitoring"}
{"intent": "List all block devices with size, mount point, and filesystem type", "plan": [{"command": "lsblk -o NAME,SIZE,MOUNTPOINT,FSTYPE -P"}], "domain": "System Information & Monitoring"}
{"intent": "Monitor network bandwidth on interface eth0, logging every 5 seconds for 1 minute", "plan": [{"command": "mkdir -p /tmp/monitor_logs"}, {"command": "for i in {1..12}; do cat /proc/net/dev | awk '/eth0/ {print strftime(\"%Y-%m-%d %H:%M:%S\"), $2, $10}' >> /tmp/monitor_logs/eth0_bandwidth.log; sleep 5; done"}], "domain": "System Information & Monitoring"}
{"intent": "Find process IDs of services listening on TCP ports 8000\u20118100 and show their program names", "plan": [{"command": "ss -tlnp | awk 'NR>1 && $4 ~ /:80[0-9][0-9]?$/ {gsub(/.*:/, \"\", $4); print $4, $NF}' | while read port proc; do echo \"Port $port -> $proc\"; done"}], "domain": "System Information & Monitoring"}
{"intent": "Identify the top 5 CPU\u2011intensive processes currently running", "plan": [{"command": "ps -eo pid,comm,%cpu --sort=-%cpu | head -n 6"}], "domain": "System Information & Monitoring"}
{"intent": "Display a summary of memory usage including total, used, free and cached values", "plan": [{"command": "free -h"}], "domain": "System Information & Monitoring"}
{"intent": "Show the amount of free space on each mounted filesystem, focusing on those with less than 10% free", "plan": [{"command": "df -h --output=source,fstype,size,used,avail,pcent,target | awk 'NR==1 || $5+0 < 10'"}], "domain": "System Information & Monitoring"}
{"intent": "Find the PID(s) of processes that are listening on TCP port 8080", "plan": [{"command": "ss -ltnp \"sport = :8080\" | awk 'NR>1 {print $NF}' | cut -d',' -f2 | cut -d'=' -f2'"}], "domain": "System Information & Monitoring"}
{"intent": "Gather detailed hardware information about CPU and memory modules", "plan": [{"command": "lscpu"}, {"command": "cat /proc/meminfo | grep -i 'memtotal\\|memfree\\|memavailable'"}], "domain": "System Information & Monitoring"}
{"intent": "Identify the top 5 processes consuming the most memory and display their PID, user, and command.", "plan": [{"command": "ps aux --sort=-%mem | head -n 6"}, {"command": "awk 'NR>1 {print $2, $1, $11}'"}], "domain": "System Information & Monitoring"}
{"intent": "Check available and used disk space on all mounted filesystems, and highlight any partitions with less than 10% free space.", "plan": [{"command": "df -h --output=source,fstype,size,used,avail,pcent,target"}, {"command": "awk 'NR==1 || $5 ~ /[0-9]+%/ && $5+0 > 90 {print}'"}], "domain": "System Information & Monitoring"}
{"intent": "Find the process ID(s) and command lines of services listening on TCP ports 80, 443, and 8080.", "plan": [{"command": "sudo lsof -nP -iTCP -sTCP:LISTEN"}, {"command": "awk '$9 ~ /:80$|:443$|:8080$/ {print $2, $1, $9}'"}], "domain": "System Information & Monitoring"}
{"intent": "Collect a comprehensive system hardware report including CPU model, number of cores, total RAM, and GPU information.", "plan": [{"command": "lscpu | grep -E 'Model name|CPU(s):'"}, {"command": "free -h | grep Mem:"}, {"command": "lspci -nn | grep -i vga || true"}], "domain": "System Information & Monitoring"}
{"intent": "Monitor CPU and memory usage of the nginx process every 5 seconds for 1 minute and output a summary.", "plan": [{"command": "pid=$(pidof nginx | awk '{print $1}')"}, {"command": "if [ -z \"$pid\" ]; then echo \"nginx not running\"; exit 1; fi"}, {"command": "for i in {1..12}; do ps -p $pid -o %cpu,%mem --no-headers; sleep 5; done > /tmp/nginx_usage.txt"}, {"command": "awk '{cpu+=$1; mem+=$2} END {print \"Average CPU:\", cpu/NR \"%\", \"Average MEM:\", mem/NR \"%\"}' /tmp/nginx_usage.txt"}], "domain": "System Information & Monitoring"}
{"intent": "Identify the top 5 CPU\u2011intensive processes over the last minute and display their PID, user, and command line.", "plan": [{"command": "ps -eo pid,user,pcpu,comm --sort=-pcpu | head -n 6"}, {"command": "printf '\\nDetailed CPU usage for these PIDs:\\n'"}, {"command": "pid_list=$(ps -eo pid,pcpu --sort=-pcpu | head -n 6 | awk 'NR>1 {print $1}' | tr '\\n' ','); pid_list=${pid_list%,}; top -b -n1 -p $pid_list"}], "domain": "System Information & Monitoring"}
{"intent": "Check free memory, swap usage, and generate a warning if available memory falls below 10% of total RAM.", "plan": [{"command": "free -m | awk 'NR==2 {printf \"Memory: %s/%s MB (%.2f%%)\\n\", $3,$2,$3*100/$2}'"}, {"command": "free -m | awk 'NR==3 {printf \"Swap: %s/%s MB (%.2f%%)\\n\", $3,$2,$3*100/$2}'"}, {"command": "total=$(free -m | awk 'NR==2 {print $2}'); used=$(free -m | awk 'NR==2 {print $3}'); if (( used * 100 / total > 90 )); then echo \"WARNING: Memory usage exceeds 90%\"; fi"}], "domain": "System Information & Monitoring"}
{"intent": "List all network interfaces, their IP addresses, and the amount of traffic transmitted and received in the last 5 seconds.", "plan": [{"command": "ip -brief address show"}, {"command": "cat /proc/net/dev | tail -n +3 | awk '{print $1,$2,$10}'"}, {"command": "echo \"Collecting traffic stats for 5 seconds...\" && sleep 5 && cat /proc/net/dev | tail -n +3 | awk '{print $1,$2,$10}'"}], "domain": "System Information & Monitoring"}
{"intent": "Find the hardware details of the system, including CPU model, number of cores, total RAM, and disk layout.", "plan": [{"command": "lscpu | grep -E 'Model name|CPU(s):'"}, {"command": "grep -i '^memtotal' /proc/meminfo | awk '{printf \"Total RAM: %.2f GB\\n\", $2/1024/1024}'"}, {"command": "lsblk -o NAME,SIZE,TYPE,MOUNTPOINT"}], "domain": "System Information & Monitoring"}
{"intent": "Determine which processes are holding the most open file descriptors and list the top 3 offenders with their descriptor counts.", "plan": [{"command": "ls /proc/*/fd 2>/dev/null | awk -F'/' '{print $3}' | sort | uniq -c | sort -nr | head -n 3"}, {"command": "while read count pid; do cmd=$(ps -p $pid -o comm=); echo \"PID $pid ($cmd) has $count open file descriptors\"; done < <(ls /proc/*/fd 2>/dev/null | awk -F'/' '{print $3}' | sort | uniq -c | sort -nr | head -n 3)"}], "domain": "System Information & Monitoring"}
{"intent": "Identify the top 5 processes consuming the most memory, showing PID, user, and full command line.", "plan": [{"command": "ps aux --sort=-%mem | head -n 6 | awk 'NR==1{print \"PID USER COMMAND\"} NR>1{print $2, $1, $11}'"}], "domain": "System Information & Monitoring"}
{"intent": "Monitor CPU and memory usage in real\u2011time for 30 seconds and save the output to a timestamped log file.", "plan": [{"command": "timestamp=$(date +%Y%m%d_%H%M%S)"}, {"command": "logfile=\"system_monitor_${timestamp}.log\""}, {"command": "echo \"Timestamp,CPU_%usr,CPU_%sys,Mem_used,Mem_free\" > \"$logfile\""}, {"command": "for i in $(seq 30); do vmstat 1 2 | tail -1 | awk -v ts=$(date +%H:%M:%S) '{print ts\",\"$13\",\"$14\",\"$4\",\"$5}' >> \"$logfile\"; done"}], "domain": "System Information & Monitoring"}
{"intent": "List all mounted filesystems with size, usage, and type, and highlight any that are over 80% full.", "plan": [{"command": "df -hT --output=source,fstype,size,used,avail,pcent,target | tail -n +2 | while read line; do usage=$(echo $line | awk '{print $6}' | tr -d '%'); if [ $usage -ge 80 ]; then echo \"$line\" | awk '{print $0, \"<- OVER 80%\"}'; else echo $line; fi; done"}], "domain": "System Information & Monitoring"}
{"intent": "Find the PID of the process listening on TCP port 8080, then display its full command line and current working directory.", "plan": [{"command": "pid=$(lsof -nP -iTCP:8080 -sTCP:LISTEN -t 2>/dev/null | head -n1)"}, {"command": "if [ -n \"$pid\" ]; then ps -p $pid -o pid=,user=,cmd=; echo \"CWD:\"; readlink -f /proc/$pid/cwd; else echo \"No process listening on port 8080\"; fi"}], "domain": "System Information & Monitoring"}
{"intent": "Collect detailed hardware information (CPU model, total RAM, and GPU details) and output it in JSON format.", "plan": [{"command": "cpu=$(lscpu | grep 'Model name' | sed 's/.*: //')"}, {"command": "ram=$(free -b | awk '/Mem:/ {print $2}')"}, {"command": "gpu=$(lspci -mm | grep -i 'vga' | sed -e 's/\"//g' -e 's/\\[.*\\]//g' | awk -F'\\t' '{print $2}' | paste -sd \", \" -)"}, {"command": "printf '{\\\"cpu\\\":\\\"%s\\\", \\\"ram_bytes\\\":%s, \\\"gpu\\\":\\\"%s\\\"}\\n' \"$cpu\" \"$ram\" \"$gpu\""}], "domain": "System Information & Monitoring"}
{"intent": "Identify the top 5 memory\u2011consuming processes and display their PID, user, and command line.", "plan": [{"command": "ps aux --sort=-%mem | head -n 6 | awk 'NR>1 {printf \"%s %s %s\\n\", $2, $1, $11}'"}], "domain": "System Information & Monitoring"}
{"intent": "Show disk usage for all mounted filesystems, sorted by usage percentage, and highlight any that exceed 80% utilization.", "plan": [{"command": "df -hP | awk 'NR==1 || $5+0>80 {print}' | sort -k5 -h"}], "domain": "System Information & Monitoring"}
{"intent": "Collect a short snapshot of CPU load and memory usage every 5 seconds for 30 seconds.", "plan": [{"command": "for i in {1..6}; do echo \"--- $(date '+%H:%M:%S') ---\"; top -b -n1 | grep \"Cpu(s)\"; free -h | grep Mem; sleep 5; done"}], "domain": "System Information & Monitoring"}
{"intent": "List all network interfaces with their assigned IP addresses and current traffic counters.", "plan": [{"command": "ip -brief address show"}, {"command": "cat /proc/net/dev | awk 'NR>2 {gsub(\":\",\"\",$1); printf \"%s: Rx %s bytes, Tx %s bytes\\n\", $1, $2, $10}'"}], "domain": "System Information & Monitoring"}
{"intent": "Find the process IDs and statuses of services listening on ports 22, 80, and 443.", "plan": [{"command": "ss -tulpn | grep -E ':(22|80|443)\\s' | awk '{print $5, $7}'"}], "domain": "System Information & Monitoring"}
{"intent": "Identify the top 5 processes that are consuming the most CPU at the moment.", "plan": [{"command": "ps -eo pid,comm,%cpu,%mem --sort=-%cpu | head -n 6"}, {"command": "printf \"\\nExplanation: The first line is the header, the following five lines are the top CPU consumers.\\n\""}], "domain": "System Information & Monitoring"}
{"intent": "Monitor real\u2011time network bandwidth usage for each network interface over the next 30 seconds.", "plan": [{"command": "sar -n DEV 1 30"}, {"command": "awk 'NR>2 {print $2, $3, $5, $6}'"}], "domain": "System Information & Monitoring"}
{"intent": "Generate a comprehensive hardware inventory report including CPU, memory, disks, and PCI devices.", "plan": [{"command": "lscpu"}, {"command": "free -h"}, {"command": "lsblk -f"}, {"command": "lspci -vv"}, {"command": "lshw -short"}], "domain": "System Information & Monitoring"}
{"intent": "Find all processes that have been running longer than 24 hours and display their resource usage.", "plan": [{"command": "ps -eo pid,etime,comm,%cpu,%mem --sort=-etime | awk '$2 ~ /[0-9]+-[0-9]+/ {print}'"}], "domain": "System Information & Monitoring"}
{"intent": "Check the kernel log for hardware\u2011related error messages generated during the most recent boot.", "plan": [{"command": "dmesg -T | grep -i \"error\" | grep -i \"hardware\""}, {"command": "journalctl -k -b | grep -i \"error\" | grep -i \"hardware\""}], "domain": "System Information & Monitoring"}
{"intent": "Show the top 5 processes that have used the most CPU in the last minute", "plan": [{"command": "ps -eo pid,ppid,cmd,%cpu --sort=-%cpu | head -n 6"}, {"command": "echo \"---\" && ps -p $(ps -eo pid --sort=-%cpu | head -n 6 | tail -n 5 | tr '\\n' ',' | sed 's/,$//') -o pid,ppid,cmd,%cpu"}], "domain": "System Information & Monitoring"}
{"intent": "Monitor network bandwidth usage per interface for 30 seconds and summarize the average throughput", "plan": [{"command": "printf \"Sampling network statistics every second for 30 seconds...\\n\""}, {"command": "sar -n DEV 1 30 | awk 'NR>3 && $2!=\"IFACE\" {bytes_in[$2]+=$5; bytes_out[$2]+=$6; count[$2]++} END {for (i in bytes_in) printf \"%s: In=%.2f KB/s Out=%.2f KB/s\\n\", i, (bytes_in[i]/count[i])/1024, (bytes_out[i]/count[i])/1024}'"}], "domain": "System Information & Monitoring"}
{"intent": "Collect a detailed hardware inventory report, including CPU, memory modules, and disk devices", "plan": [{"command": "lscpu > /tmp/hw_report.txt"}, {"command": "sudo dmidecode -t memory >> /tmp/hw_report.txt"}, {"command": "lsblk -o NAME,SIZE,TYPE,MOUNTPOINT >> /tmp/hw_report.txt"}, {"command": "lshw -short >> /tmp/hw_report.txt"}, {"command": "echo \"Hardware inventory saved to /tmp/hw_report.txt\""}], "domain": "System Information & Monitoring"}
{"intent": "List all currently logged\u2011in users with their idle times and the last command they executed", "plan": [{"command": "w -h | awk '{print $1, $3, $5, $6, $7}'"}, {"command": "who -u"}, {"command": "printf \"\\nRecent commands per user (last 5 entries):\\n\""}, {"command": "for user in $(who | awk '{print $1}' | sort -u); do echo \"User: $user\"; sudo -u $user bash -c \"history | tail -n 5\"; done"}], "domain": "System Information & Monitoring"}
{"intent": "Detect any zombie processes on the system and show their parent process IDs", "plan": [{"command": "ps -eo pid,ppid,stat,cmd | awk '$3==\"Z\" {print $1, $2, $4}'"}, {"command": "printf \"\\nSummary: \" && ps -eo stat | grep -c \"Z\" && echo \" zombie processes found.\""}], "domain": "System Information & Monitoring"}
{"intent": "List the top 5 processes consuming the most memory", "plan": [{"command": "ps aux --sort=-%mem | head -n 6"}], "domain": "System Information & Monitoring"}
{"intent": "Check disk usage of all mounted filesystems and flag any that exceed 80% utilization", "plan": [{"command": "df -h --output=target,pcent | tail -n +2"}, {"command": "awk 'NR>1 && $2+0 > 80 {print \"Warning: \" $1 \" is \" $2 \" full\"}'"}], "domain": "System Information & Monitoring"}
{"intent": "Display detailed CPU information and current load averages", "plan": [{"command": "lscpu"}, {"command": "cat /proc/loadavg"}], "domain": "System Information & Monitoring"}
{"intent": "Find the process ID and command of the service listening on port 8080", "plan": [{"command": "ss -ltnp 'sport = :8080'"}, {"command": "awk -F'[,=]' '/pid=/{print \"PID:\" $2 \", Command:\" $4}'"}], "domain": "System Information & Monitoring"}
{"intent": "Monitor inbound and outbound bandwidth usage on the primary network interface for 10 seconds", "plan": [{"command": "primary_iface=$(ip route get 8.8.8.8 | awk '{print $5; exit}') && echo \"Monitoring $primary_iface...\""}, {"command": "ifstat -i \"$primary_iface\" 1 10"}], "domain": "System Information & Monitoring"}
{"intent": "List the top 5 processes consuming the most memory.", "plan": [{"command": "ps aux --sort=-%mem | head -n 6"}], "domain": "System Information & Monitoring"}
{"intent": "Check available disk space on the root partition and alert if it falls below 10%.", "plan": [{"command": "df -h / | awk 'NR==2 {print $5}'"}, {"command": "usage=$(df / | awk 'NR==2 {print $5}' | tr -d '%'); if [ \"$usage\" -ge 90 ]; then echo \"ALERT: Root partition usage is ${usage}%\"; else echo \"Root partition usage is ${usage}% - OK\"; fi"}], "domain": "System Information & Monitoring"}
{"intent": "Find the process ID of the service listening on TCP port 8080.", "plan": [{"command": "ss -ltnp 'sport = :8080' | awk 'NR>1 {print $NF}' | cut -d',' -f2 | cut -d'=' -f2"}], "domain": "System Information & Monitoring"}
{"intent": "Generate a comprehensive hardware summary (CPU, memory, disks) and save it to a temporary report file.", "plan": [{"command": "printf \"=== CPU INFO ===\\n\" > /tmp/sys_report.txt"}, {"command": "lscpu >> /tmp/sys_report.txt"}, {"command": "printf \"\\n=== MEMORY INFO ===\\n\" >> /tmp/sys_report.txt"}, {"command": "free -h >> /tmp/sys_report.txt"}, {"command": "printf \"\\n=== DISK INFO ===\\n\" >> /tmp/sys_report.txt"}, {"command": "lsblk -o NAME,SIZE,TYPE,MOUNTPOINT >> /tmp/sys_report.txt"}, {"command": "printf \"\\nReport generated at /tmp/sys_report.txt\""}], "domain": "System Information & Monitoring"}
{"intent": "Monitor CPU usage every 2 seconds for 30 seconds and log the readings to a file.", "plan": [{"command": "log_file=\"/tmp/cpu_usage_$(date +%s).log\""}, {"command": "printf \"Timestamp,CPU%%\\n\" > \"$log_file\""}, {"command": "for i in {1..15}; do timestamp=$(date +\"%Y-%m-%d %H:%M:%S\"); cpu=$(top -bn1 | grep \"Cpu(s)\" | awk '{print $2+$4}'); echo \"${timestamp},${cpu}\" >> \"$log_file\"; sleep 2; done"}, {"command": "echo \"CPU monitoring complete. Log saved to $log_file\""}], "domain": "System Information & Monitoring"}
{"intent": "Identify the top 5 CPU-intensive processes currently running.", "plan": [{"command": "ps -eo pid,comm,%cpu --sort=-%cpu | head -n 6"}], "domain": "System Information & Monitoring"}
{"intent": "Monitor real\u2011time memory usage and record it for five minutes.", "plan": [{"command": "vmstat 1 300 > /tmp/memory_usage.log"}, {"command": "echo \"Memory usage logged to /tmp/memory_usage.log\""}], "domain": "System Information & Monitoring"}
{"intent": "Collect a detailed hardware inventory (CPU, RAM, disks) and output it in JSON format.", "plan": [{"command": "lscpu --json > /tmp/hardware_cpu.json"}, {"command": "free -b --json > /tmp/hardware_mem.json"}, {"command": "lsblk -J > /tmp/hardware_disk.json"}, {"command": "jq -s '{cpu: .[0], memory: .[1], disks: .[2]}' /tmp/hardware_cpu.json /tmp/hardware_mem.json /tmp/hardware_disk.json > /tmp/hardware_inventory.json"}, {"command": "cat /tmp/hardware_inventory.json"}], "domain": "System Information & Monitoring"}
{"intent": "List all services listening on TCP ports greater than 1024, showing their PIDs and command names.", "plan": [{"command": "ss -tlnp | awk 'BEGIN {FS=\":\"} $5 > 1024 {print $0}' | while read line; do pid=$(echo \"$line\" | grep -oP \"pid=\\K[0-9]+\"); cmd=$(ps -p $pid -o comm= 2>/dev/null); echo \"Port: $(echo $line | awk '{print $5}')  PID: $pid  Command: $cmd\"; done"}], "domain": "System Information & Monitoring"}
{"intent": "Generate a summary of disk I/O statistics for each block device over the last 10 seconds.", "plan": [{"command": "iostat -dx 10 2 | tail -n +4 > /tmp/disk_io_summary.txt"}, {"command": "echo \"Disk I/O summary (last 10 seconds) saved to /tmp/disk_io_summary.txt\""}, {"command": "cat /tmp/disk_io_summary.txt"}], "domain": "System Information & Monitoring"}
{"intent": "Verify connectivity to multiple critical endpoints and report average latency for each over 10 pings.", "plan": [{"command": "endpoints=(api.company.com db.internal.net 8.8.8.8)"}, {"command": "for host in \"${endpoints[@]}\"; do echo \"--- $host ---\"; ping -c 10 -q \"$host\" | grep 'rtt' | awk -F'/' '{print \"Avg latency: \" $5 \" ms\"}'; done"}], "domain": "Network Operations"}
{"intent": "Resolve all DNS records (A, AAAA, MX, TXT) for example.org and save them to a timestamped file.", "plan": [{"command": "timestamp=$(date +%Y%m%d_%H%M%S)"}, {"command": "output_file=\"dns_report_example_org_${timestamp}.txt\""}, {"command": "echo \"DNS records for example.org\" > \"$output_file\""}, {"command": "for type in A AAAA MX TXT; do echo \"\\n$type records:\" >> \"$output_file\"; dig +short example.org $type >> \"$output_file\"; done"}, {"command": "cat \"$output_file\""}], "domain": "Network Operations"}
{"intent": "Download a large file from a secure URL, verify its SHA256 checksum, and retry download up to 2 times if verification fails.", "plan": [{"command": "url=\"https://speed.hetzner.de/100MB.bin\""}, {"command": "expected_checksum=\"5e5c3b2f9e8b3d0a7c5f3e8d5a6b7c9d1e2f3a4b5c6d7e8f9a0b1c2d3e4f5a6b\""}, {"command": "max_attempts=3; attempt=1; while [ $attempt -le $max_attempts ]; do echo \"Attempt $attempt...\"; curl -fSL \"$url\" -o downloaded.bin && echo \"Downloaded\"; echo \"Verifying checksum...\"; actual_checksum=$(sha256sum downloaded.bin | awk '{print $1}'); if [ \"$actual_checksum\" = \"$expected_checksum\" ]; then echo \"Checksum OK\"; break; else echo \"Checksum mismatch (expected $expected_checksum, got $actual_checksum)\"; rm -f downloaded.bin; ((attempt++)); fi; done"}], "domain": "Network Operations"}
{"intent": "List all listening TCP ports on the local machine along with the associated process names, and export the result as a CSV file.", "plan": [{"command": "output=\"listening_ports_$(date +%Y%m%d_%H%M%S).csv\""}, {"command": "echo \"Port,Process\" > \"$output\""}, {"command": "ss -tlnp | awk 'NR>1 {gsub(/.*:/, \"\", $4); split($7, a, \",\"); proc = (a[2] ? a[2] : \"\"); print $4 \",\" proc}' >> \"$output\""}, {"command": "cat \"$output\""}], "domain": "Network Operations"}
{"intent": "Check the SSL/TLS certificate expiry date for a remote HTTPS service and alert if it expires within 30 days.", "plan": [{"command": "host=\"www.example.com\""}, {"command": "port=443"}, {"command": "cert_info=$(echo | openssl s_client -connect ${host}:${port} -servername ${host} 2>/dev/null | openssl x509 -noout -dates)"}, {"command": "end_date=$(echo \"$cert_info\" | grep notAfter | cut -d= -f2)"}, {"command": "expiry_seconds=$(date -d \"$end_date\" +%s)"}, {"command": "now_seconds=$(date +%s)"}, {"command": "days_left=$(( (expiry_seconds - now_seconds) / 86400 ))"}, {"command": "echo \"Certificate for $host expires on $end_date ($days_left days left)\""}, {"command": "if [ $days_left -le 30 ]; then echo \"WARNING: Certificate expires within 30 days!\"; else echo \"Certificate is valid for more than 30 days.\"; fi"}], "domain": "Network Operations"}
{"intent": "Measure reachability and latency for a set of critical hosts (google.com, github.com, and a corporate DNS) and produce a concise summary of packet loss and average round\u2011trip time.", "plan": [{"command": "ping -c 5 google.com > /tmp/ping_google.txt"}, {"command": "ping -c 5 github.com > /tmp/ping_github.txt"}, {"command": "ping -c 5 8.8.8.8 > /tmp/ping_dns.txt"}, {"command": "awk -F'[=,]' '/packet loss/ {loss=$4} /rtt/ {split($2,a,\"/\"); avg=a[2]} END {print \"Host: \"FILENAME\", Loss: \"loss\"%, Avg RTT: \"avg\" ms\"}' /tmp/ping_*.txt > /tmp/ping_summary.txt"}, {"command": "cat /tmp/ping_summary.txt"}], "domain": "Network Operations"}
{"intent": "Perform a comprehensive DNS interrogation for the domain \"example.org\", retrieving A, AAAA, MX, NS, and TXT records and presenting them in a readable format.", "plan": [{"command": "dig +nocmd example.org any +multiline +noall +answer > /tmp/dig_all.txt"}, {"command": "awk '/\\tA\\t/ {print \"A Record: \" $5}' /tmp/dig_all.txt"}, {"command": "awk '/\\tAAAA\\t/ {print \"AAAA Record: \" $5}' /tmp/dig_all.txt"}, {"command": "awk '/\\tMX\\t/ {print \"MX Record: \" $5 \" (priority \" $4 \")\"}' /tmp/dig_all.txt"}, {"command": "awk '/\\tNS\\t/ {print \"NS Record: \" $5}' /tmp/dig_all.txt"}, {"command": "awk '/\\tTXT\\t/ {print \"TXT Record: \" $5}' /tmp/dig_all.txt"}], "domain": "Network Operations"}
{"intent": "Download a 500\u202fMB data set from a secure HTTP endpoint, show a progress bar, and verify its integrity against a known SHA\u2011256 checksum.", "plan": [{"command": "curl -L -o /tmp/dataset.bin https://example.com/data/dataset.bin"}, {"command": "echo \"3a7bd3e2f9c1d5e6b4f8a9c2d1e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d0e1  /tmp/dataset.bin\" > /tmp/expected.sha256"}, {"command": "sha256sum -c /tmp/expected.sha256"}], "domain": "Network Operations"}
{"intent": "List every listening TCP socket on the host together with the owning process name, PID, and user, then filter to show only those bound to port 22 or 443.", "plan": [{"command": "ss -tlnp | awk 'NR>1 {print $5\" \" $7}' > /tmp/ss_all.txt"}, {"command": "grep -E ':22|:443' /tmp/ss_all.txt"}], "domain": "Network Operations"}
{"intent": "Test TCP connectivity to a remote service (myapp.example.com on port 8080) using a short timeout, report success or failure, and log the result with a timestamp.", "plan": [{"command": "timestamp=$(date '+%Y-%m-%d %H:%M:%S')"}, {"command": "if nc -zvw3 myapp.example.com 8080 >/dev/null 2>&1; then echo \"$timestamp - SUCCESS: myapp.example.com:8080 reachable\"; else echo \"$timestamp - FAILURE: myapp.example.com:8080 unreachable\"; fi > /tmp/tcp_test.log"}, {"command": "cat /tmp/tcp_test.log"}], "domain": "Network Operations"}
{"intent": "Check reachability and measure average latency for google.com and github.com.", "plan": [{"command": "ping -c 5 google.com | tail -1 | awk -F '/' '{print \"Google average latency: \" $5 \" ms\"}'"}, {"command": "ping -c 5 github.com | tail -1 | awk -F '/' '{print \"GitHub average latency: \" $5 \" ms\"}'"}], "domain": "Network Operations"}
{"intent": "Resolve DNS for a set of hostnames and verify they match expected IP addresses.", "plan": [{"command": "declare -A expected=( [\"api.github.com\"]=\"140.82.112.5\" [\"registry.npmjs.org\"]=\"151.101.0.209\" )"}, {"command": "for host in \"${!expected[@]}\"; do ip=$(dig +short $host | head -n1); if [[ \"$ip\" == \"${expected[$host]}\" ]]; then echo \"$host resolves correctly to $ip\"; else echo \"$host resolves to $ip (expected ${expected[$host]})\"; fi; done"}], "domain": "Network Operations"}
{"intent": "Download a file from a URL, compute its SHA256 checksum, and compare it to a known hash.", "plan": [{"command": "url=\"https://example.com/sample.tar.gz\""}, {"command": "expected_hash=\"3a7bd3e2360a7b8f4c9e5b2d6f8a9c1d2e3f4b5c6d7e8f9a0b1c2d3e4f5a6b7c\""}, {"command": "curl -sSL -o /tmp/sample.tar.gz $url"}, {"command": "actual_hash=$(sha256sum /tmp/sample.tar.gz | awk '{print $1}')"}, {"command": "if [[ \"$actual_hash\" == \"$expected_hash\" ]]; then echo \"Checksum matches.\"; else echo \"Checksum mismatch! Expected $expected_hash but got $actual_hash.\"; fi"}], "domain": "Network Operations"}
{"intent": "List all active TCP connections in ESTABLISHED state and show the owning process name.", "plan": [{"command": "ss -t -a state ESTABLISHED -p | awk 'NR>1{print $5, $6}'"}], "domain": "Network Operations"}
{"intent": "Test upload speed to a public test server using curl and report the transfer rate.", "plan": [{"command": "test_file=$(mktemp)"}, {"command": "dd if=/dev/urandom of=$test_file bs=1M count=10 status=none"}, {"command": "curl -w \"\\nUpload speed: %{speed_upload}\\n\" -o /dev/null -s -T $test_file https://transfer.sh/$(basename $test_file)"}, {"command": "rm -f $test_file"}], "domain": "Network Operations"}
{"intent": "Verify that the host example.org is reachable, measure round\u2011trip latency, and trace the network path to it.", "plan": [{"command": "ping -c 5 -i 0.2 example.org"}, {"command": "traceroute -n -w 2 example.org"}], "domain": "Network Operations"}
{"intent": "Resolve the IPv4 and IPv6 addresses for the domain api.service.internal and check that the returned records match expected patterns.", "plan": [{"command": "dig +short A api.service.internal"}, {"command": "dig +short AAAA api.service.internal"}, {"command": "grep -E '^([0-9]{1,3}\\.){3}[0-9]{1,3}$' <<< \"$(dig +short A api.service.internal)\""}, {"command": "grep -E '^([0-9a-fA-F]{0,4}:){1,7}[0-9a-fA-F]{0,4}$' <<< \"$(dig +short AAAA api.service.internal)\""}], "domain": "Network Operations"}
{"intent": "Download a tarball from a secure URL, verify its SHA256 checksum, and extract it to a temporary directory.", "plan": [{"command": "TMPDIR=$(mktemp -d)"}, {"command": "curl -fSL https://example.com/files/archive.tar.gz -o $TMPDIR/archive.tar.gz"}, {"command": "echo \"d2d2f0e5b8c4a1e9f7c3a9b6e4d5f8a9c2b1e0d3f4a6b7c8d9e0f1a2b3c4d5e6  $TMPDIR/archive.tar.gz\" | sha256sum -c -"}, {"command": "tar -xzvf $TMPDIR/archive.tar.gz -C $TMPDIR"}], "domain": "Network Operations"}
{"intent": "List all current TCP listening sockets, display the associated process names, and filter for ports above 1024.", "plan": [{"command": "ss -tlnp | awk 'NR>1 && $5 > 1024 {print $0}'"}], "domain": "Network Operations"}
{"intent": "Test connectivity to a remote service on port 443 using TCP, measure the handshake time, and report success or failure.", "plan": [{"command": "START=$(date +%s%3N)"}, {"command": "timeout 5 bash -c \"echo > /dev/tcp/remote.example.com/443\" && RESULT=success || RESULT=failure"}, {"command": "END=$(date +%s%3N)"}, {"command": "ELAPSED=$((END-START)) && echo \"Connection $RESULT, handshake time: ${ELAPSED}ms\""}], "domain": "Network Operations"}
{"intent": "Verify reachability and average latency for multiple critical hosts", "plan": [{"command": "ping -c 5 google.com | tail -1 | awk -F '/' '{print \"Avg latency to google.com: \"$5\" ms\"}'"}, {"command": "ping -c 5 github.com | tail -1 | awk -F '/' '{print \"Avg latency to github.com: \"$5\" ms\"}'"}], "domain": "Network Operations"}
{"intent": "Gather comprehensive DNS records (A, AAAA, MX, TXT) for a domain and store them in a report file", "plan": [{"command": "dig +nocmd example.com any +multiline +noall +answer > dns_report.txt"}, {"command": "echo \"DNS report saved to dns_report.txt\""}], "domain": "Network Operations"}
{"intent": "Download a binary file from a URL, verify its SHA\u2011256 checksum, and keep it only if the checksum matches", "plan": [{"command": "curl -L -o /tmp/sample.bin https://example.com/files/sample.bin"}, {"command": "echo 'd2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2  /tmp/sample.bin' | sha256sum -c -"}], "domain": "Network Operations"}
{"intent": "List all currently established TCP connections, extract remote IPs, and produce a count of connections per unique remote IP", "plan": [{"command": "ss -t state established -o pid,process > tcp_conns.txt"}, {"command": "awk '{print $5}' tcp_conns.txt | cut -d':' -f1 | sort | uniq -c | sort -nr > remote_ip_counts.txt"}], "domain": "Network Operations"}
{"intent": "Run a traceroute to a target host, log each hop, and flag any hop with round\u2011trip time greater than 100\u202fms", "plan": [{"command": "traceroute -n -w 2 google.com > traceroute.log"}, {"command": "awk '/^[0-9]+/ { for(i=2;i<=NF;i++) if($i ~ /^[0-9.]+$/) { rtt=$i; if(rtt>100) print \"High latency at hop \" $1 \": \" rtt \" ms\" } }' traceroute.log > high_latency_hops.txt"}], "domain": "Network Operations"}
{"intent": "Verify connectivity to google.com, measure latency, and trace the route to identify any network hops causing delay.", "plan": [{"command": "ping -c 5 google.com"}, {"command": "traceroute google.com"}, {"command": "mtr --report --count 10 google.com"}], "domain": "Network Operations"}
{"intent": "Download a file from a given URL, compute its SHA\u2011256 checksum, and compare it with an expected hash to ensure integrity.", "plan": [{"command": "curl -L -o /tmp/downloaded_file \"https://example.com/path/to/file.bin\""}, {"command": "sha256sum /tmp/downloaded_file > /tmp/downloaded_file.sha256"}, {"command": "echo \"expected_hash_value  /tmp/downloaded_file\" | sha256sum -c -"}], "domain": "Network Operations"}
{"intent": "List all currently established TCP connections, show the associated processes, and filter for connections to remote port 443 (HTTPS).", "plan": [{"command": "ss -t -a -p"}, {"command": "ss -t -a -p | grep ':443'"}, {"command": "netstat -tnp | grep ':443'"}], "domain": "Network Operations"}
{"intent": "Resolve A and AAAA DNS records for a set of domains, measure the time each query takes, and present the results in a table.", "plan": [{"command": "domains=(example.com github.com stackoverflow.com)"}, {"command": "for d in \"${domains[@]}\"; do echo \"Domain: $d\"; time dig +short A $d; time dig +short AAAA $d; echo; done"}], "domain": "Network Operations"}
{"intent": "Monitor bandwidth usage on the eth0 interface for 30 seconds, capturing average inbound and outbound rates.", "plan": [{"command": "ifstat -i eth0 1 30"}, {"command": "sar -n DEV 1 30 | grep eth0"}, {"command": "cat /proc/net/dev | grep eth0"}], "domain": "Network Operations"}
{"intent": "Check reachability and latency for multiple critical hosts (google.com, github.com, and a private server) and produce a summary report.", "plan": [{"command": "hosts=(google.com github.com 10.0.0.5)"}, {"command": "for h in \"${hosts[@]}\"; do echo \"--- $h ---\"; ping -c 5 -q \"$h\" | tail -1; done > latency_report.txt"}, {"command": "echo \"Latency summary saved to latency_report.txt\""}], "domain": "Network Operations"}
{"intent": "Resolve the DNS A record for api.example.com, compare it to an expected IP address, and alert if they differ.", "plan": [{"command": "expected_ip=\"192.0.2.45\""}, {"command": "resolved_ip=$(dig +short A api.example.com | head -n1)"}, {"command": "if [ \"$resolved_ip\" = \"$expected_ip\" ]; then echo \"DNS match: $resolved_ip\"; else echo \"WARNING: DNS mismatch (expected $expected_ip, got $resolved_ip)\"; fi"}], "domain": "Network Operations"}
{"intent": "Securely copy a backup file from a remote server, verify its SHA256 checksum, and place it in the local backups directory.", "plan": [{"command": "remote_user=\"backupuser\""}, {"command": "remote_host=\"backup.example.org\""}, {"command": "remote_path=\"/var/backups/db_backup.tar.gz\""}, {"command": "local_dir=\"~/backups\""}, {"command": "mkdir -p \"$local_dir\""}, {"command": "scp \"$remote_user@$remote_host:$remote_path\" \"$local_dir/\""}, {"command": "expected_checksum=\"$(ssh $remote_user@$remote_host \"sha256sum $remote_path\" | awk '{print $1}')\""}, {"command": "actual_checksum=$(sha256sum \"$local_dir/$(basename $remote_path)\" | awk '{print $1}')"}, {"command": "if [ \"$expected_checksum\" = \"$actual_checksum\" ]; then echo \"Checksum verified successfully.\"; else echo \"ERROR: Checksum mismatch!\"; fi"}], "domain": "Network Operations"}
{"intent": "List all currently listening TCP ports along with the owning process name and PID, and export the list to a CSV file.", "plan": [{"command": "ss -tlnp | awk 'NR>1 {gsub(/,/,\";\",$5); print $4\",\"$5}' | sed 's/.*pid=\\([0-9]*\\),.*name=\\([^,]*\\).*/\\1,\\2/' > listening_ports.csv"}, {"command": "echo \"Listening ports exported to listening_ports.csv\""}], "domain": "Network Operations"}
{"intent": "Perform a traceroute to the corporate gateway, capture both the raw output and a summary of hop counts, and store them in a dated log file.", "plan": [{"command": "gateway=\"gateway.corp.example.com\""}, {"command": "logfile=\"traceroute_$(date +%Y%m%d_%H%M%S).log\""}, {"command": "traceroute -n \"$gateway\" | tee \"$logfile\""}, {"command": "hop_count=$(traceroute -n \"$gateway\" | tail -1 | awk '{print $1}')"}, {"command": "echo \"Total hops: $hop_count\" >> \"$logfile\""}, {"command": "echo \"Traceroute saved to $logfile\""}], "domain": "Network Operations"}
{"intent": "Verify that google.com is reachable, record round\u2011trip latency statistics, and display the average latency.", "plan": [{"command": "ping -c 10 -i 0.5 google.com | tail -2"}, {"command": "awk -F '/' '/rtt/ {print \"Average latency: \" $5 \" ms\"}'"}], "domain": "Network Operations"}
{"intent": "Perform a DNS lookup for multiple records of example.org and display the resolved IPv4, IPv6, MX, and NS entries.", "plan": [{"command": "dig +noall +answer A example.org"}, {"command": "dig +noall +answer AAAA example.org"}, {"command": "dig +noall +answer MX example.org"}, {"command": "dig +noall +answer NS example.org"}], "domain": "Network Operations"}
{"intent": "Download a tarball from a given URL, verify its SHA\u2011256 checksum, and extract it only if the checksum matches.", "plan": [{"command": "URL=\"https://example.com/archive.tar.gz\"; CHECKSUM=\"d2d2f1e8b7c9a5e6f1c2b3a4d5e6f7a8b9c0d1e2f3a4b5c6d7e8f9a0b1c2d3e4\"; FILENAME=\"$(basename $URL)\""}, {"command": "curl -L -o $FILENAME $URL"}, {"command": "echo \"$CHECKSUM  $FILENAME\" | sha256sum -c -"}, {"command": "if [ $? -eq 0 ]; then tar -xzvf $FILENAME; else echo \"Checksum verification failed; extraction aborted.\"; fi"}], "domain": "Network Operations"}
{"intent": "List all currently established TCP connections, show the remote IPs and ports, and count how many connections exist per remote host.", "plan": [{"command": "ss -tn state ESTABLISHED | awk '{print $5}' | cut -d':' -f1 | sort | uniq -c | sort -nr"}], "domain": "Network Operations"}
{"intent": "Check if port 22 (SSH) on a list of remote hosts is open, report which hosts accept connections, and measure the connection time.", "plan": [{"command": "HOSTS=(\"server1.example.com\" \"server2.example.com\" \"192.0.2.10\"); for h in \"${HOSTS[@]}\"; do echo -n \"Testing $h: \"; timeout 3 bash -c \"echo > /dev/tcp/$h/22\" && echo \"open\" || echo \"closed\"; done"}], "domain": "Network Operations"}
{"intent": "Check reachability and average latency for a list of critical hosts (e.g., google.com, github.com, and a corporate VPN endpoint).", "plan": [{"command": "hosts=(google.com github.com vpn.corp.example.com); for h in \"${hosts[@]}\"; do echo \"--- $h ---\"; ping -c 5 -q $h | awk -F'/' 'END{print \"Avg latency: \" $5 \" ms\"}'; done"}], "domain": "Network Operations"}
{"intent": "Perform detailed DNS queries for multiple domains and compare the results with the local resolver cache.", "plan": [{"command": "domains=(example.com openai.com); for d in \"${domains[@]}\"; do echo \"--- $d ---\"; echo \"Resolver query:\"; dig +short $d; echo \"Cache query:\"; dig +nocmd +noall +answer $d @127.0.0.1; done"}], "domain": "Network Operations"}
{"intent": "Securely copy a backup archive to a remote server via SCP and verify its integrity with SHA\u2011256 checksum.", "plan": [{"command": "src_file=/tmp/backup_$(date +%F).tar.gz; remote_user=backup; remote_host=backup.example.com; remote_path=/var/backups/; sha256sum $src_file > /tmp/backup.sha256;"}, {"command": "scp $src_file ${remote_user}@${remote_host}:${remote_path}"}, {"command": "ssh ${remote_user}@${remote_host} \"cd ${remote_path} && sha256sum -c /tmp/backup.sha256\""}], "domain": "Network Operations"}
{"intent": "List all active TCP connections, show their state, and produce a summary sorted by connection state.", "plan": [{"command": "ss -t state established '( sport :>1024 )' | awk '{print $1}' | sort | uniq -c | sort -nr"}], "domain": "Network Operations"}
{"intent": "Test an HTTP API endpoint, follow redirects, capture response time, HTTP status code, and log the results to a file.", "plan": [{"command": "url=\"https://api.example.com/v1/status\"; logfile=/tmp/api_test.log; curl -L -w \"\\nTime_Total:%{time_total}\\nHTTP_Code:%{http_code}\\n\" -o /dev/null $url > $logfile; echo \"Test result saved to $logfile\""}], "domain": "Network Operations"}
{"intent": "Verify latency, jitter, and packet loss to a set of critical servers and produce a concise summary report.", "plan": [{"command": "servers=(google.com github.com example.com)"}, {"command": "printf \"Server,Sent,Received,Loss%%,AvgLatency(ms)\\n\" > latency_report.csv"}, {"command": "for s in \"${servers[@]}\"; do ping -c 10 \"$s\" | awk -F',|=' '/packets transmitted/ {sent=$1; recv=$4; loss=$6} /rtt/ {split($4, a, \"/\"); avg=a[2]} END {printf \"%s,%s,%s,%s,%s\\n\", s, sent, recv, loss, avg}' >> latency_report.csv; done"}], "domain": "Network Operations"}
{"intent": "Resolve a list of domain names using two different DNS resolvers and highlight any mismatched IP addresses.", "plan": [{"command": "domains=(example.com wikipedia.org openai.com)"}, {"command": "resolver1=8.8.8.8"}, {"command": "resolver2=1.1.1.1"}, {"command": "printf \"Domain,Resolver,IPs\\n\" > dns_comparison.csv"}, {"command": "for d in \"${domains[@]}\"; do ips1=$(dig +short @$resolver1 \"$d\"); ips2=$(dig +short @$resolver2 \"$d\"); printf \"%s,%s,%s\\n\" \"$d\" \"${resolver1}\" \"$ips1\" >> dns_comparison.csv; printf \"%s,%s,%s\\n\" \"$d\" \"${resolver2}\" \"$ips2\" >> dns_comparison.csv; done"}, {"command": "awk -F',' 'NR>1{key=$1; val[$2]=val[$2]\" \"$3} END{for(k in val){split(val[k],a,\" \"); asort(a); printf \"%s -> %s\\n\", k, a[1]}}' dns_comparison.csv > dns_mismatches.txt"}], "domain": "Network Operations"}
{"intent": "Download a large tarball from a remote HTTP server, verify its SHA256 checksum, and extract it into a specific directory.", "plan": [{"command": "url=\"https://example.org/files/bigdata.tar.gz\""}, {"command": "checksum_file=\"bigdata.sha256\""}, {"command": "target_dir=\"/tmp/bigdata_extracted\""}, {"command": "curl -fSL \"$url\" -o bigdata.tar.gz"}, {"command": "curl -fSL \"${url}.sha256\" -o \"$checksum_file\""}, {"command": "sha256sum -c \"$checksum_file\" --status && echo \"Checksum OK\" || { echo \"Checksum FAILED\"; exit 1; }"}, {"command": "mkdir -p \"$target_dir\""}, {"command": "tar -xz -f bigdata.tar.gz -C \"$target_dir\""}], "domain": "Network Operations"}
{"intent": "Continuously monitor TCP connections to port 22 on a remote host and log any new connections that appear within a 5\u2011minute interval.", "plan": [{"command": "remote_host=\"203.0.113.5\""}, {"command": "port=22"}, {"command": "tmp_before=$(mktemp)"}, {"command": "tmp_after=$(mktemp)"}, {"command": "ss -tn state ESTABLISHED \"src $remote_host\" \"dst :$port\" > \"$tmp_before\""}, {"command": "sleep 300"}, {"command": "ss -tn state ESTABLISHED \"src $remote_host\" \"dst :$port\" > \"$tmp_after\""}, {"command": "comm -13 <(sort \"$tmp_before\") <(sort \"$tmp_after\") > new_connections.log"}, {"command": "rm -f \"$tmp_before\" \"$tmp_after\""}], "domain": "Network Operations"}
{"intent": "Trace the route to a destination, identify hops with latency exceeding 100\u202fms, and generate a brief report of problematic hops.", "plan": [{"command": "dest=\"8.8.8.8\""}, {"command": "trace_output=$(mktemp)"}, {"command": "traceroute -n -w 2 \"$dest\" > \"$trace_output\""}, {"command": "awk '/^[ 0-9]+/ {for(i=2;i<=NF;i++) if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} if($i~/[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/){next} {print $1, $NF}' \"$trace_output\" | awk '$2>100 {print \"Hop \" $1 \": \" $2 \" ms\"}' > high_latency_hops.txt"}, {"command": "rm -f \"$trace_output\""}], "domain": "Network Operations"}
{"intent": "Verify connectivity and latency to multiple critical endpoints (google.com, github.com, internal.example.com) and log results for later analysis.", "plan": [{"command": "echo \"--- Connectivity Check ---\" > connectivity_report.txt"}, {"command": "for host in google.com github.com internal.example.com; do echo \"Testing $host\" >> connectivity_report.txt; ping -c 5 -i 0.2 $host | tail -2 >> connectivity_report.txt; echo \"\" >> connectivity_report.txt; done"}], "domain": "Network Operations"}
{"intent": "Resolve DNS records (A, AAAA, MX, TXT) for a domain and compare them against a cached snapshot to detect changes.", "plan": [{"command": "DOMAIN=example.org"}, {"command": "echo \"--- DNS Records for $DOMAIN ---\" > dns_change_report.txt"}, {"command": "dig +noall +answer $DOMAIN A AAAA MX TXT >> dns_change_report.txt"}, {"command": "if [ -f dns_snapshot.txt ]; then echo \"--- Differences from snapshot ---\" >> dns_change_report.txt; diff dns_snapshot.txt dns_change_report.txt >> dns_change_report.txt || true; fi"}, {"command": "cp dns_change_report.txt dns_snapshot.txt"}], "domain": "Network Operations"}
{"intent": "Download a large file from a secure HTTPS source, verify its SHA256 checksum, and extract it if it is a tar.gz archive.", "plan": [{"command": "URL=\"https://downloads.example.com/archive.tar.gz\""}, {"command": "EXPECTED_SHA256=\"a3f5c2e9d8b7c1e6f4a9d3b2c1e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3\""}, {"command": "curl -L -o archive.tar.gz \"$URL\""}, {"command": "ACTUAL_SHA256=$(sha256sum archive.tar.gz | awk '{print $1}')"}, {"command": "if [ \"$ACTUAL_SHA256\" = \"$EXPECTED_SHA256\" ]; then echo \"Checksum OK\"; else echo \"Checksum mismatch!\"; exit 1; fi"}, {"command": "tar -xzvf archive.tar.gz"}], "domain": "Network Operations"}
{"intent": "Perform a traceroute to a remote host, capture each hop's IP and RTT, and generate a concise summary of the path.", "plan": [{"command": "TARGET=8.8.8.8"}, {"command": "traceroute -n $TARGET > traceroute_raw.txt"}, {"command": "awk '/^[ 0-9]+/ {print $1 \":\" $2 \" RTTs:\" $3 \",\" $4 \",\" $5}' traceroute_raw.txt > traceroute_summary.txt"}, {"command": "echo \"Traceroute summary saved to traceroute_summary.txt\""}], "domain": "Network Operations"}
{"intent": "Check all listening TCP ports on the local machine, identify processes using them, and alert if any unexpected ports (e.g., >1024) are open.", "plan": [{"command": "ss -tlnp > listening_ports.txt"}, {"command": "awk 'NR>1 && $5 > 1024 {print $0}' listening_ports.txt > unexpected_ports.txt"}, {"command": "if [ -s unexpected_ports.txt ]; then echo \"Alert: Unexpected listening ports detected:\"; cat unexpected_ports.txt; else echo \"No unexpected listening ports found.\"; fi"}], "domain": "Network Operations"}
{"intent": "Verify connectivity and latency for a set of critical hosts", "plan": [{"command": "ping -c 5 192.168.1.10 > /tmp/ping_host1.txt"}, {"command": "ping -c 5 10.0.0.5 > /tmp/ping_host2.txt"}, {"command": "ping -c 5 google.com > /tmp/ping_google.txt"}, {"command": "awk '/time=/{sum+=$NF;cnt++} END{if(cnt) printf \"Average latency: %.2f ms\\n\", sum/cnt; else print \"No replies\"}' /tmp/ping_host1.txt /tmp/ping_host2.txt /tmp/ping_google.txt > /tmp/latency_report.txt"}], "domain": "Network Operations"}
{"intent": "Resolve IP addresses for multiple domains and flag any unexpected DNS results", "plan": [{"command": "dig +short example.com > /tmp/dns_example.txt"}, {"command": "dig +short api.service.org > /tmp/dns_api.txt"}, {"command": "dig +short www.internal.net > /tmp/dns_internal.txt"}, {"command": "cat /tmp/dns_example.txt /tmp/dns_api.txt /tmp/dns_internal.txt > /tmp/all_resolved_ips.txt"}, {"command": "grep -v -f /opt/expected_ips.txt /tmp/all_resolved_ips.txt > /tmp/unexpected_ips.txt"}], "domain": "Network Operations"}
{"intent": "Download a secure archive, verify its SHA\u2011256 checksum, and extract it to a target directory", "plan": [{"command": "curl -L -o /tmp/archive.tar.gz https://downloads.example.com/releases/app-v2.3.tar.gz"}, {"command": "echo \"d2d2f8e5a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8  /tmp/archive.tar.gz\" > /tmp/checksum.sha256"}, {"command": "sha256sum -c /tmp/checksum.sha256"}, {"command": "mkdir -p /opt/app && tar -xz -f /tmp/archive.tar.gz -C /opt/app"}], "domain": "Network Operations"}
{"intent": "Continuously monitor established TCP connections to the PostgreSQL service and log the count every minute for five minutes", "plan": [{"command": "for i in {1..5}; do ss -tn state ESTABLISHED '( dport = :5432 )' | wc -l >> /tmp/pg_conn_counts.log; sleep 60; done"}], "domain": "Network Operations"}
{"intent": "Run a traceroute to a remote API endpoint and store detailed hop information for later analysis", "plan": [{"command": "traceroute -n api.example.com > /tmp/traceroute_api.txt"}], "domain": "Network Operations"}
{"intent": "Verify connectivity to google.com, measure round\u2011trip latency, and trace the network path.", "plan": [{"command": "ping -c 5 -i 0.2 -W 2 google.com"}, {"command": "traceroute -n -w 2 google.com"}], "domain": "Network Operations"}
{"intent": "Download a tarball from a given URL, compute its SHA\u2011256 checksum, and compare it to the expected value.", "plan": [{"command": "curl -fSL -o /tmp/archive.tar.gz https://example.com/files/archive.tar.gz"}, {"command": "echo \"d2d2f0e5b8c9a6e5f3c7a9b4e2d1c8a9b0c1d2e3f4a5b6c7d8e9f0a1b2c3d4e5  /tmp/archive.tar.gz\" > /tmp/expected.sha256"}, {"command": "sha256sum -c /tmp/expected.sha256"}], "domain": "Network Operations"}
{"intent": "List all currently established TCP connections on the host with process details.", "plan": [{"command": "ss -t -a -p state established"}, {"command": "netstat -tnp | grep ESTABLISHED"}], "domain": "Network Operations"}
{"intent": "Resolve a set of domain names, capture their IPv4 addresses, and compare the results against a known list of expected IPs.", "plan": [{"command": "cat <<'EOF' > /tmp/domains.txt\nexample.com\napi.service.io\ninternal.net\nEOF"}, {"command": "while read domain; do dig +short A $domain; done < /tmp/domains.txt | sort > /tmp/resolved_ips.txt"}, {"command": "cat <<'EOF' > /tmp/expected_ips.txt\n93.184.216.34\n203.0.113.42\n10.0.0.5\nEOF"}, {"command": "sort /tmp/expected_ips.txt > /tmp/expected_ips_sorted.txt"}, {"command": "comm -3 /tmp/expected_ips_sorted.txt /tmp/resolved_ips.txt || echo \"No differences found\""}], "domain": "Network Operations"}
{"intent": "Measure upload bandwidth to a public iperf3 test server and store a concise summary of the result.", "plan": [{"command": "iperf3 -c iperf.scottlinux.com -p 5201 -u -b 0 -t 10 --json > /tmp/iperf_result.json"}, {"command": "jq '.end.sum_sent.bits_per_second | tostring + \" bps\"' /tmp/iperf_result.json > /tmp/upload_bandwidth.txt"}, {"command": "cat /tmp/upload_bandwidth.txt"}], "domain": "Network Operations"}
{"intent": "Verify reachability and latency for multiple critical hosts and store a trace route for each.", "plan": [{"command": "hosts=(google.com github.com example.com); for h in \"${hosts[@]}\"; do echo \"--- Ping $h ---\"; ping -c 5 -i 0.2 \"$h\"; echo \"--- Traceroute $h ---\"; traceroute -n \"$h\"; done > /tmp/network_check_$(date +%Y%m%d%H%M%S).log"}], "domain": "Network Operations"}
{"intent": "Resolve all DNS record types for a domain, compare the A records against a known whitelist, and alert if any unexpected IP appears.", "plan": [{"command": "domain=example.com; whitelist=('93.184.216.34' '2606:2800:220:1:248:1893:25c8:1946'); dig +nocmd $domain any +multiline +noall +answer > /tmp/dns_$domain.txt"}, {"command": "awk '/\\tA\\t/ {print $5}' /tmp/dns_$domain.txt | while read ip; do if [[ ! \" ${whitelist[@]} \" =~ \" $ip \" ]]; then echo \"Unexpected A record: $ip\"; fi; done"}], "domain": "Network Operations"}
{"intent": "Securely copy a large archive from a remote host, verify its SHA\u2011256 checksum, and extract it only if the checksum matches.", "plan": [{"command": "remote_user=ops; remote_host=10.0.0.5; remote_path=/data/backups/archive.tar.gz; local_dir=/tmp; scp \"$remote_user@$remote_host:$remote_path\" \"$local_dir/\" "}, {"command": "expected_sum='d2d2f9c1e5a6b7c8d9e0f1a2b3c4d5e6f7a8b9c0d1e2f3a4b5c6d7e8f9a0b1c2'; actual_sum=$(sha256sum \"$local_dir/archive.tar.gz\" | awk '{print $1}'); if [ \"$expected_sum\" = \"$actual_sum\" ]; then echo \"Checksum OK\"; tar -xzvf \"$local_dir/archive.tar.gz\" -C \"$local_dir\"; else echo \"Checksum mismatch! Aborting extraction.\"; fi"}], "domain": "Network Operations"}
{"intent": "List all established TCP connections, group them by remote IP, count occurrences, and save a concise report.", "plan": [{"command": "ss -tn state established | awk '{print $5}' | cut -d':' -f1 | sort | uniq -c | sort -nr > /tmp/established_tcp_report_$(date +%Y%m%d).txt"}], "domain": "Network Operations"}
{"intent": "Run a short bandwidth test against a public iperf3 server, capture jitter, loss, and throughput, and generate a JSON summary.", "plan": [{"command": "server=iperf.scottlinux.com; duration=10; iperf3 -c $server -t $duration -J > /tmp/iperf_raw.json"}, {"command": "jq '{timestamp: now, server: \"$server\", duration: $duration, bitrate_Mbps: .end.sum_received.bits_per_second/1000000, jitter_ms: .end.sum.jitter_ms, loss_percent: .end.sum.lost_percent}' /tmp/iperf_raw.json > /tmp/iperf_summary_$(date +%Y%m%d%H%M%S).json"}], "domain": "Network Operations"}
{"intent": "Verify connectivity to a remote host and measure round\u2011trip latency", "plan": [{"command": "ping -c 5 example.com"}, {"command": "ping -c 5 example.com | tail -1 | awk -F '/' '{print \"Average latency: \" $5 \" ms\"}'"}], "domain": "Network Operations"}
{"intent": "Perform a DNS lookup for A, AAAA, and MX records of a domain and display their TTL values", "plan": [{"command": "dig +nocmd +noall +answer example.org A AAAA MX"}, {"command": "dig +nocmd +noall +answer example.org A AAAA MX | awk '{print $1, $4, $5}'"}], "domain": "Network Operations"}
{"intent": "Download a file from a URL, compute its SHA\u2011256 checksum, and compare it to an expected value", "plan": [{"command": "curl -L -o /tmp/downloaded_file.tar.gz https://example.com/files/sample.tar.gz"}, {"command": "sha256sum /tmp/downloaded_file.tar.gz | awk '{print $1}' > /tmp/computed_checksum.txt"}, {"command": "echo \"expectedchecksumvalue1234567890abcdef\" > /tmp/expected_checksum.txt && diff -u /tmp/expected_checksum.txt /tmp/computed_checksum.txt && echo \"Checksum verification $(if [ $? -eq 0 ]; then echo PASSED; else echo FAILED; fi)\""}], "domain": "Network Operations"}
{"intent": "List all established TCP connections, show remote IP/port, and filter for connections to port 443", "plan": [{"command": "ss -t state established -o pid,proc,peer"}, {"command": "ss -t state established '( dport = :443 )' -o pid,proc,peer"}], "domain": "Network Operations"}
{"intent": "Test whether port 22 on a remote server is open and capture its SSH banner", "plan": [{"command": "timeout 5 bash -c \"echo | nc -v -w 3 remote.example.com 22\" 2>&1 | grep -i 'ssh'"}, {"command": "nc -zv -w5 remote.example.com 22 && echo \"Port 22 is open\" || echo \"Port 22 is closed\""}], "domain": "Network Operations"}
{"intent": "Check reachability and measure average latency for multiple hosts (e.g., google.com and github.com).", "plan": [{"command": "hosts=(google.com github.com); for h in \"${hosts[@]}\"; do echo \"Pinging $h...\"; ping -c 5 -q $h | grep 'rtt' || echo \"$h is unreachable\"; done"}], "domain": "Network Operations"}
{"intent": "Resolve DNS records (A, AAAA, MX) for a domain and compare them with the local resolver cache.", "plan": [{"command": "domain=example.com; echo \"Fetching live DNS records...\"; dig +noall +answer $domain A AAAA MX > /tmp/live_dns.txt; echo \"Fetching cached records...\"; getent hosts $domain > /tmp/cached_dns.txt; echo \"Live DNS records:\"; cat /tmp/live_dns.txt; echo \"Cached resolver entries:\"; cat /tmp/cached_dns.txt"}], "domain": "Network Operations"}
{"intent": "Download a file from a given HTTPS URL, compute its SHA256 checksum, and verify it against an expected hash.", "plan": [{"command": "url='https://example.com/file.tar.gz'; expected_hash='d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2d2'; filename=$(basename $url); echo \"Downloading $url...\"; curl -fSL -o $filename $url; echo \"Computing SHA256...\"; actual_hash=$(sha256sum $filename | awk '{print $1}'); echo \"Actual: $actual_hash\"; echo \"Expected: $expected_hash\"; if [ \"$actual_hash\" = \"$expected_hash\" ]; then echo \"Checksum verification passed.\"; else echo \"Checksum verification FAILED.\"; fi"}], "domain": "Network Operations"}
{"intent": "List all active TCP listening sockets with their owning processes and filter for services running on port 22 (SSH).", "plan": [{"command": "echo \"All TCP listening sockets:\"; ss -tlnp; echo \"\\nFiltering for port 22 (SSH):\"; ss -tlnp | grep ':22'"}], "domain": "Network Operations"}
{"intent": "Run a traceroute to a destination host, capture each hop's IP and round\u2011trip time, and save the output to a log file.", "plan": [{"command": "dest=8.8.8.8; logfile=traceroute_$(date +%Y%m%d_%H%M%S).log; echo \"Traceroute to $dest...\"; traceroute -n $dest | tee $logfile; echo \"Traceroute saved to $logfile\""}], "domain": "Network Operations"}
{"intent": "Measure latency and packet loss to google.com using both ping and a detailed route analysis.", "plan": [{"command": "ping -c 5 google.com"}, {"command": "mtr --report --no-dns google.com"}], "domain": "Network Operations"}
{"intent": "Resolve all IPv4 and IPv6 addresses for example.com and verify basic reachability to each resolved IP.", "plan": [{"command": "dig +short example.com A > /tmp/example_ipv4.txt"}, {"command": "dig +short example.com AAAA > /tmp/example_ipv6.txt"}, {"command": "cat /tmp/example_ipv4.txt /tmp/example_ipv6.txt | while read ip; do echo \"Pinging $ip\"; ping -c 2 $ip; done"}], "domain": "Network Operations"}
{"intent": "Download a large file from a given URL with resume capability, verify its SHA\u2011256 checksum, and log the download speed.", "plan": [{"command": "curl -L -O --retry 3 https://example.org/largefile.tar.gz"}, {"command": "sha256sum largefile.tar.gz"}, {"command": "curl -L -w \"\\nAverage speed: %{speed_download} bytes/sec\\n\" -o /dev/null https://example.org/largefile.tar.gz"}], "domain": "Network Operations"}
{"intent": "List all active TCP connections on the host, then filter to show only ESTABLISHED connections to the remote host 192.0.2.45.", "plan": [{"command": "ss -tn state established"}, {"command": "ss -tn state established | grep 192.0.2.45"}], "domain": "Network Operations"}
{"intent": "Perform a quick TCP port scan on ports 20\u201130 of host scan.example.net and save the results to a file.", "plan": [{"command": "nc -zv -w2 scan.example.net 20-30 2>&1 | tee port_scan_results.txt"}], "domain": "Network Operations"}
{"intent": "Verify connectivity and latency for a set of remote hosts and produce a summary report.", "plan": [{"command": "hosts=(google.com github.com stackoverflow.com)"}, {"command": "report_file=\"connectivity_report_$(date +%F).txt\""}, {"command": "echo \"Connectivity Report - $(date)\" > \"$report_file\""}, {"command": "for h in \"${hosts[@]}\"; do echo \"\\n--- $h ---\" >> \"$report_file\"; ping -c 5 \"$h\" | tail -1 | awk -F '/' '{print \"Avg latency: \" $5 \" ms\"}' >> \"$report_file\"; done"}, {"command": "cat \"$report_file\""}], "domain": "Network Operations"}
{"intent": "Resolve A and AAAA DNS records for a list of domains and compare the results with an expected IP address list.", "plan": [{"command": "domains=(example.com api.example.com internal.service.local)"}, {"command": "expected_ips=(93.184.216.34 93.184.216.34 10.0.0.42)"}, {"command": "for i in \"${!domains[@]}\"; do d=\"${domains[$i]}\"; exp=\"${expected_ips[$i]}\"; echo \"Domain: $d\"; dig +short A \"$d\"; dig +short AAAA \"$d\"; echo \"Expected IP: $exp\"; echo; done"}], "domain": "Network Operations"}
{"intent": "Download a compressed archive from a secure URL, verify its SHA256 checksum, and extract it to a target directory.", "plan": [{"command": "url=\"https://example.org/files/data_bundle.tar.gz\""}, {"command": "checksum_file=\"data_bundle.sha256\""}, {"command": "target_dir=\"/tmp/data_bundle\""}, {"command": "mkdir -p \"$target_dir\""}, {"command": "curl -L -o \"$(basename $url)\" \"$url\""}, {"command": "curl -L -o \"$checksum_file\" \"${url}.sha256\""}, {"command": "sha256sum -c \"$checksum_file\" && echo \"Checksum OK\" || echo \"Checksum FAILED\""}, {"command": "tar -xzvf \"$(basename $url)\" -C \"$target_dir\""}, {"command": "ls -l \"$target_dir\""}], "domain": "Network Operations"}
{"intent": "Continuously monitor active TCP connections on port 443 and log any new connections with timestamps.", "plan": [{"command": "log_file=\"tls_connections_$(date +%F).log\""}, {"command": "echo \"Monitoring TCP port 443 - $(date)\" > \"$log_file\""}, {"command": "while true; do ss -tn state ESTAB '( dport = :443 )' | tail -n +2 | while read line; do ts=$(date '+%Y-%m-%d %H:%M:%S'); echo \"$ts - $line\" >> \"$log_file\"; done; sleep 5; done &"}], "domain": "Network Operations"}
{"intent": "Run a traceroute to a destination, capture each hop's IP address, and perform reverse DNS lookups for every hop.", "plan": [{"command": "dest=\"8.8.8.8\""}, {"command": "trace_file=\"traceroute_$(date +%F).txt\""}, {"command": "traceroute -n \"$dest\" > \"$trace_file\""}, {"command": "echo \"Traceroute to $dest:\"; cat \"$trace_file\""}, {"command": "awk '{if($1 ~ /^[0-9]+$/) print $2}' \"$trace_file\" | while read ip; do echo -n \"$ip -> \"; dig +short -x \"$ip\"; done"}], "domain": "Network Operations"}
{"intent": "Check reachability and latency of multiple hosts (google.com and github.com) and log the average round\u2011trip times.", "plan": [{"command": "ping -c 5 google.com > /tmp/ping_google.txt"}, {"command": "ping -c 5 github.com > /tmp/ping_github.txt"}, {"command": "awk -F'/' '/^rtt/ {print \"google.com avg latency: \" $5 \" ms\"}' /tmp/ping_google.txt >> /tmp/network_report.txt"}, {"command": "awk -F'/' '/^rtt/ {print \"github.com avg latency: \" $5 \" ms\"}' /tmp/ping_github.txt >> /tmp/network_report.txt"}], "domain": "Network Operations"}
{"intent": "Resolve DNS A records for a list of domains and verify they match expected IP addresses.", "plan": [{"command": "domains=(example.com openai.com wikipedia.org)"}, {"command": "expected_ips=(\"93.184.216.34\" \"104.18.30.162\" \"208.80.154.224\")"}, {"command": "for i in \"${!domains[@]}\"; do ip=$(dig +short \"${domains[i]}\"); echo \"${domains[i]} resolved to $ip (expected ${expected_ips[i]})\"; done > /tmp/dns_check.txt"}], "domain": "Network Operations"}
{"intent": "List all active TCP connections on port 22 (SSH) and count how many unique remote IPs are connected.", "plan": [{"command": "ss -tn state ESTABLISHED '( dport = :22 )' | awk 'NR>1 {print $5}' | cut -d: -f1 | sort | uniq -c > /tmp/ssh_connections.txt"}, {"command": "total=$(awk '{sum+=$1} END {print sum}' /tmp/ssh_connections.txt); echo \"Total active SSH connections: $total\" >> /tmp/ssh_connections.txt"}], "domain": "Network Operations"}
{"intent": "Download a large file from a given URL using curl, show progress, and verify its SHA\u2011256 checksum.", "plan": [{"command": "url=\"https://speed.hetzner.de/100MB.bin\""}, {"command": "expected_sha256=\"5a8c0f5c6c9f4c4c0b0f5d2e8e5c8f5c9d4e6b7a8c9d0e1f2a3b4c5d6e7f8a9b\""}, {"command": "curl -L --progress-bar -o /tmp/100MB.bin \"$url\""}, {"command": "echo \"$expected_sha256  /tmp/100MB.bin\" | sha256sum -c -"}], "domain": "Network Operations"}
{"intent": "Run a traceroute to a destination and flag any hops whose round\u2011trip time exceeds 100\u202fms.", "plan": [{"command": "target=amazon.com"}, {"command": "traceroute -n \"$target\" > /tmp/traceroute.txt"}, {"command": "awk 'NR>1 {for(i=2;i<=NF;i++) if($i>100) {print \"High latency at hop \" NR-1 \": \" $i \" ms\"; break}}' /tmp/traceroute.txt > /tmp/high_latency_hops.txt"}], "domain": "Network Operations"}
{"intent": "Check reachability and measure latency for multiple hosts, then compute the average round\u2011trip time for each host.", "plan": [{"command": "hosts=(google.com github.com stackoverflow.com); for h in \"${hosts[@]}\"; do ping -c 5 -q \"$h\" | grep 'rtt' | awk -F'/' '{print \"$h,\"$5}' ; done > /tmp/latency_report.csv"}, {"command": "column -t -s',' /tmp/latency_report.csv"}], "domain": "Network Operations"}
{"intent": "Perform a comprehensive DNS lookup for a domain, retrieve A, AAAA, CNAME, MX, and TXT records, and compare the current A record with a previously stored IP address.", "plan": [{"command": "domain=example.com; dig +noall +answer $domain A AAAA CNAME MX TXT > /tmp/dns_full.txt"}, {"command": "current_ip=$(dig +short $domain A | head -n1); previous_ip=$(cat /tmp/previous_ip.txt 2>/dev/null || echo \"none\"); echo \"Current A record: $current_ip\"; echo \"Previous A record: $previous_ip\"; echo \"$current_ip\" > /tmp/current_ip.txt"}, {"command": "diff -u /tmp/previous_ip.txt /tmp/current_ip.txt || true; cp /tmp/current_ip.txt /tmp/previous_ip.txt"}], "domain": "Network Operations"}
{"intent": "Download a large file from a remote server via SCP, then verify its integrity using SHA\u2011256 checksum.", "plan": [{"command": "remote_user=admin; remote_host=192.0.2.10; remote_path=/var/backups/backup.tar.gz; local_path=/tmp/backup.tar.gz; scp \"$remote_user@$remote_host:$remote_path\" \"$local_path\""}, {"command": "expected_checksum=$(ssh \"$remote_user@$remote_host\" \"sha256sum $remote_path\" | awk '{print $1}') ; echo \"Expected checksum: $expected_checksum\""}, {"command": "actual_checksum=$(sha256sum \"$local_path\" | awk '{print $1}'); echo \"Actual checksum: $actual_checksum\""}, {"command": "if [ \"$expected_checksum\" = \"$actual_checksum\" ]; then echo \"Checksum verification passed.\"; else echo \"Checksum mismatch!\"; fi"}], "domain": "Network Operations"}
{"intent": "List all listening TCP ports on the host, show the associated process name and PID, and filter the output to only include ports higher than 1024.", "plan": [{"command": "ss -tlnp | awk 'NR>1 {split($5, a, \":\"); port=a[2]; if (port+0 > 1024) print $5, $6}'"}, {"command": "while read -r addr proc; do pid=$(echo \"$proc\" | grep -oP '\\d+/'); pname=$(echo \"$proc\" | grep -oP '(?<=/).+'); echo \"Port $(echo $addr | cut -d':' -f2) -> PID $pid, Process $pname\"; done < <(ss -tlnp | awk 'NR>1 {split($5, a, \":\"); port=a[2]; if (port+0 > 1024) print $5, $6}')"}], "domain": "Network Operations"}
{"intent": "Perform a quick scan of the 192.168.1.0/24 subnet to identify hosts with HTTP (80) or HTTPS (443) ports open, and output the results in CSV format.", "plan": [{"command": "subnet=192.168.1.0/24; nmap -p 80,443 -oG - $subnet | awk '/Ports:/{print $2\",\"$4}' | sed 's//open//' > /tmp/http_scan.csv"}, {"command": "echo \"IP,OpenPort\" > /tmp/http_scan_header.csv; cat /tmp/http_scan_header.csv /tmp/http_scan.csv > /tmp/http_scan_final.csv; column -t -s',' /tmp/http_scan_final.csv"}], "domain": "Network Operations"}
{"intent": "Extract all unique email addresses from all .log files in the current directory and save them sorted to emails.txt", "plan": [{"command": "grep -Eho \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\" *.log | sort -u > emails.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the word 'foo' with 'bar' in all .txt files under the current directory tree, keeping a backup of each original file", "plan": [{"command": "find . -type f -name \"*.txt\" -exec sed -i.bak \"s/foo/bar/g\" {} +"}], "domain": "Text Processing"}
{"intent": "Count how many distinct IP addresses appear in access.log and list them ordered by frequency (most frequent first)", "plan": [{"command": "awk '{print $1}' access.log | sort | uniq -c | sort -nr > ip_frequency.txt"}, {"command": "awk 'END{print \"Unique IP count:\", NR}' ip_frequency.txt"}], "domain": "Text Processing"}
{"intent": "Create a CSV report (user,login_attempts) summarizing how many times each username appears in auth.log", "plan": [{"command": "awk '/sshd/ && /session opened/ {for(i=1;i<=NF;i++) if($i==\"user=\") {gsub(\"user=\",\"\",$(i+1)); print $(i+1)}}' auth.log | sort | uniq -c | awk '{print $2\",\"$1}' > login_report.csv"}], "domain": "Text Processing"}
{"intent": "Find all lines containing dates in YYYY-MM-DD format in data.txt, convert the dates to DD/MM/YYYY, and write the transformed lines to data_converted.txt", "plan": [{"command": "sed -E \"s/([0-9]{4})-([0-9]{2})-([0-9]{2})/\\3\\/\\2\\/\\1/g\" data.txt > data_converted.txt"}], "domain": "Text Processing"}
{"intent": "Extract all email addresses from every text file in the current directory tree, deduplicate them, and save the sorted list to a file called emails.txt.", "plan": [{"command": "find . -type f -name \"*.txt\" -exec grep -Eo \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]\\\\.[A-Za-z]{2,}\" {} + | sort -u > emails.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the word \"foo\" with \"bar\" in all .txt files, preserving the original files with a .bak backup extension.", "plan": [{"command": "find . -type f -name \"*.txt\" -exec sed -i.bak \"s/foo/bar/g\" {} +"}], "domain": "Text Processing"}
{"intent": "Count the number of unique IP addresses in an Apache access log and display the top 10 most frequent IPs with their request counts.", "plan": [{"command": "awk '{print $1}' access.log | sort | uniq -c | sort -nr | head -10"}], "domain": "Text Processing"}
{"intent": "Create a CSV report of word frequencies from a large text file, ignoring common English stopwords and case differences.", "plan": [{"command": "tr -cs \"[:alnum:]\" \"\\n\" < large_text.txt | tr '[:upper:]' '[:lower:]' | grep -v -E \"^(the|and|a|an|of|in|to|is|it|for|on|with|as|by|that|this|at)$\" | sort | uniq -c | sort -nr | awk '{print $2\",\"$1}' > word_frequencies.csv"}], "domain": "Text Processing"}
{"intent": "Convert a tab\u2011delimited file (input.tsv) into a JSON array where each line becomes an object with keys col1, col2, col3, handling any quoted fields correctly.", "plan": [{"command": "awk -F'\t' 'BEGIN{print \"[\"} {gsub(/\"/, \"\\\\\\\"\"); printf \"%s{\\\"col1\\\":\\\"%s\\\",\\\"col2\\\":\\\"%s\\\",\\\"col3\\\":\\\"%s\\\"}\", (NR==1?\"\":\" ,\\n\"), $1, $2, $3} END{print \"\\n]\"}' input.tsv > output.json"}], "domain": "Text Processing"}
{"intent": "Extract all email addresses from all text files under the current directory, deduplicate them, and save the sorted list to a file.", "plan": [{"command": "find . -type f -name \"*.txt\" -exec grep -Eho \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\" {} + | sort -u > emails.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the word 'foo' with 'bar' in all .txt files, keeping a backup of each original file with a .bak extension.", "plan": [{"command": "find . -type f -name \"*.txt\" -exec sed -i.bak 's/foo/bar/g' {} +"}], "domain": "Text Processing"}
{"intent": "Count how many unique IP addresses appear in the access.log file and list the top 10 IPs by request frequency.", "plan": [{"command": "awk '{print $1}' access.log | sort | uniq -c | sort -nr | head -10"}], "domain": "Text Processing"}
{"intent": "Create a CSV report of word frequencies from a document, ignoring case and punctuation, with columns \"word,count\" sorted by descending count.", "plan": [{"command": "tr -c '[:alnum:]' '\\n' < doc.txt | tr '[:upper:]' '[:lower:]' | sort | uniq -c | sort -nr | awk '{print $2\",\"$1}' > word_freq.csv"}], "domain": "Text Processing"}
{"intent": "Extract all lines between the markers START and END (inclusive) from every .log file, prefix each extracted line with its source filename, and write the combined output to a single file.", "plan": [{"command": "find . -type f -name \"*.log\" -exec awk '/START/,/END/ {print FILENAME \":\" $0}' {} + > extracted.txt"}], "domain": "Text Processing"}
{"intent": "Extract all unique email addresses from every text file in the current directory tree and store them sorted in a file.", "plan": [{"command": "find . -type f -name \"*.txt\" -exec grep -Eho \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}\" {} + | sort -u > emails.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the string \"foo\" with \"bar\" in all .txt files, keeping a backup of each original file.", "plan": [{"command": "find . -type f -name \"*.txt\" -exec sed -i.bak \"s/foo/bar/g\" {} +"}], "domain": "Text Processing"}
{"intent": "Count the number of distinct IP addresses that appear in an Apache access log and display the top 10 most frequent IPs.", "plan": [{"command": "awk '{print $1}' access.log | sort | uniq -c | sort -nr | head -10"}], "domain": "Text Processing"}
{"intent": "Create a CSV file summarizing word frequencies from a large text document, ignoring a list of common stop\u2011words.", "plan": [{"command": "tr -c \"[:alnum:]\" \"\\n\" < big_document.txt | tr '[:upper:]' '[:lower:]' > words.tmp"}, {"command": "grep -vi -f stopwords.txt words.tmp > filtered_words.tmp"}, {"command": "sort filtered_words.tmp | uniq -c | sort -nr > word_counts.tmp"}, {"command": "awk '{printf \"%s,%d\\n\", $2, $1}' word_counts.tmp > word_frequencies.csv"}, {"command": "rm -f words.tmp filtered_words.tmp word_counts.tmp"}], "domain": "Text Processing"}
{"intent": "Convert a tab\u2011separated values (TSV) file to a pipe\u2011delimited format, trim leading/trailing whitespace from each field, and discard empty lines.", "plan": [{"command": "awk 'NF' OFS=\"|\" '{$1=$1; for(i=1;i<=NF;i++) gsub(/^ +| +$/,\"\",$i)} 1' input.tsv > output.pipe"}], "domain": "Text Processing"}
{"intent": "Extract all email addresses from all *.log files in the current directory and its subdirectories, deduplicate them, sort alphabetically, and save to emails.txt.", "plan": [{"command": "grep -Eroh \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\" ./**/*.log | sort -u > emails.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the word 'foo' with 'bar' in all .txt files under the project directory, while keeping a backup of each original file with a .bak extension.", "plan": [{"command": "find ./project -type f -name \"*.txt\" -exec sed -i.bak 's/foo/bar/g' {} +"}], "domain": "Text Processing"}
{"intent": "Count the number of distinct IP addresses that accessed the web server, ignoring commented lines, and display the total.", "plan": [{"command": "awk '!/^#/ && $1 ~ /^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$/ {print $1}' access.log | sort -u | wc -l"}], "domain": "Text Processing"}
{"intent": "Create a CSV summary that shows, for each day, how many INFO, WARN, and ERROR entries appear in syslog.", "plan": [{"command": "awk '($0 ~ /^(\\w{3} [ 0-9]{2})/) {date=$1\" \"$2; level=$3} (level==\"INFO\"||level==\"WARN\"||level==\"ERROR\") {count[date\",\"level]++} END {print \"Date,INFO,WARN,ERROR\"; for (d in count) {split(d,parts,\",\"); dates[parts[1]\" \"parts[2]] = 1} for (d in dates) {printf \"%s,%d,%d,%d\\n\", d, count[d\",INFO\"]+0, count[d\",WARN\"]+0, count[d\",ERROR\"]+0}' syslog > syslog_summary.csv"}], "domain": "Text Processing"}
{"intent": "Extract every fenced code block (delimited by triple backticks) from all markdown files in docs/, and write each block to a separate .code file named after the source markdown file and the block index.", "plan": [{"command": "mkdir -p extracted_codes"}, {"command": "find ./docs -type f -name \"*.md\" -print0 | while IFS= read -r -d '' mdfile; do awk 'BEGIN{block=0} /^```/ {if (block==0) {block++; next} else {print > outfile; block=0; next}} block{print > outfile} {if (block && !outfile) {outfile = sprintf(\"extracted_codes/%s_block_%d.code\", gensub(/\\.\\/|\\//, \"_\", \"g\", mdfile), block)}}' \"$mdfile\"; done"}], "domain": "Text Processing"}
{"intent": "Extract all unique email addresses from a mixed\u2011content log file, sort them alphabetically, and save to emails.txt", "plan": [{"command": "grep -Eo \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\" mixed_log.txt | sort -u > emails.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the word 'foo' with 'bar' in all .txt files under the current directory tree, keeping a backup of each original file", "plan": [{"command": "find . -type f -name \"*.txt\" -print0 | while IFS= read -r -d '' file; do sed -i.bak \"s/foo/bar/g\" \"$file\"; done"}], "domain": "Text Processing"}
{"intent": "Count the number of unique IP addresses in an Apache access log and list the top 10 most frequent IPs with their request counts", "plan": [{"command": "awk '{print $1}' access.log | sort | uniq -c | sort -nr | head -n 10"}], "domain": "Text Processing"}
{"intent": "Create a CSV summary (filename, lines, words, characters) for every Markdown (.md) file in the docs/ folder", "plan": [{"command": "printf \"filename,lines,words,chars\\n\" > summary.csv"}, {"command": "for f in docs/*.md; do wc -l -w -c \"$f\" | awk '{print \"$f,\" $1 \",\" $2 \",\" $3}' >> summary.csv; done"}], "domain": "Text Processing"}
{"intent": "Transform a tab\u2011separated values file (data.tsv) into a pipe\u2011separated file, drop rows where the third column is empty, and sort the result by the numeric value in the second column in descending order", "plan": [{"command": "awk -F '\\t' 'NF>=3 && $3 != \"\" {print $1 \"|\" $2 \"|\" $3}' data.tsv | sort -t '|' -k2,2nr > data_processed.psv"}], "domain": "Text Processing"}
{"intent": "Extract all unique email addresses from every *.log file in /var/logs and save them sorted to emails.txt", "plan": [{"command": "grep -Eho '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}' /var/logs/*.log | sort -u > /tmp/emails.txt"}, {"command": "mv /tmp/emails.txt ./emails.txt"}], "domain": "Text Processing"}
{"intent": "Recursively replace every occurrence of the word 'foo' with 'bar' in all .txt files under the project directory, keeping a backup of each original file", "plan": [{"command": "find ./project -type f -name \"*.txt\" -exec sed -i.bak 's/foo/bar/g' {} +"}, {"command": "echo \"Replacement complete. Backup files have extension .bak\""}], "domain": "Text Processing"}
{"intent": "Count how many distinct external IP addresses appear in the Apache access log, ignoring private and loopback ranges", "plan": [{"command": "awk '{print $1}' /var/log/apache2/access.log | grep -Ev '^127\\.0\\.0\\.1$|^10\\.|^172\\.(1[6-9]|2[0-9]|3[0-1])\\.|^192\\.168\\.' | sort -u | wc -l > /tmp/unique_ip_count.txt"}, {"command": "cat /tmp/unique_ip_count.txt"}], "domain": "Text Processing"}
{"intent": "Create a CSV report summarizing user activity: for each username found in auth.log, count logins and output username,count sorted by count descending", "plan": [{"command": "grep 'session opened for user' /var/log/auth.log | awk -F'user ' '{print $2}' | awk '{print $1}' | sort | uniq -c | awk '{print $2\",\"$1}' | sort -t',' -k2 -nr > ./user_activity_report.csv"}, {"command": "head -n 10 ./user_activity_report.csv"}], "domain": "Text Processing"}
{"intent": "Convert a tab\u2011delimited data file to pipe\u2011delimited, trim leading/trailing whitespace from each field, and discard empty lines", "plan": [{"command": "awk -F'\\t' 'NF{ for(i=1;i<=NF;i++){gsub(/^\\s+|\\s+$/,\"\",$i)}; print $0 }' input.tsv | tr '\\t' '|' > cleaned_output.txt"}, {"command": "sed -i.bak '/^$/d' cleaned_output.txt"}], "domain": "Text Processing"}
{"intent": "Extract all unique email addresses from every *.log file in the current directory and store them sorted in emails.txt", "plan": [{"command": "find . -maxdepth 1 -type f -name \"*.log\" -print0 | xargs -0 grep -Eho \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}\" | sort -u > emails.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the word \"foo\" with \"bar\" in all .txt files, keeping a backup of each original file with a .bak extension", "plan": [{"command": "for f in *.txt; do sed -i.bak 's/foo/bar/g' \"$f\"; done"}], "domain": "Text Processing"}
{"intent": "Count distinct IP addresses per day from an Apache access.log and output a CSV file with columns: date,IP,count", "plan": [{"command": "awk '{print $4,$1}' access.log | cut -d: -f1 | awk '{date=$1; ip=$2; key=date\",\"ip; cnt[key]++} END {for (k in cnt) {split(k,a,\",\"); printf \"%s,%s,%d\\n\", a[1], a[2], cnt[k]}}' | sort -t, -k1,1 > ip_counts_per_day.csv"}], "domain": "Text Processing"}
{"intent": "Create a report of the top 10 most frequent error messages from the system log, stripping timestamps and any leading bracketed codes", "plan": [{"command": "grep -i \"error\" /var/log/syslog | sed -E 's/^[A-Za-z]{3} [ 0-9]{2} [0-9:]{8} [^ ]+ //;s/^\\\\[[0-9]+\\\\] //' | sort | uniq -c | sort -nr | head -10 > top_errors.txt"}], "domain": "Text Processing"}
{"intent": "Clean a CSV file by removing rows where the third column is empty and convert the delimiter from comma to tab, saving the result as a TSV file", "plan": [{"command": "awk -F',' 'NF>=3 && $3 != \"\" {OFS=\"\\t\"; $1=$1; print}' input.csv > output.tsv"}], "domain": "Text Processing"}
{"intent": "Extract all email addresses from every .log and .txt file in the current directory and its subdirectories, deduplicate them, sort alphabetically, and save to emails.txt.", "plan": [{"command": "find . -type f \\( -name \"*.log\" -o -name \"*.txt\" \\) -print0 | xargs -0 grep -Po \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}\" | tr 'A-Z' 'a-z' | sort -u > emails.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the word 'foo' with 'bar' in all .txt files under the project directory, creating a backup of each original file with a .bak extension.", "plan": [{"command": "find . -type f -name \"*.txt\" -exec sed -i.bak \"s/foo/bar/g\" {} +"}], "domain": "Text Processing"}
{"intent": "Count the number of unique IP addresses that accessed the web server, then list the top 10 most frequent IPs from access.log.", "plan": [{"command": "awk '{print $1}' access.log | sort | uniq -c | sort -nr | head -n 10 > top_ips.txt"}], "domain": "Text Processing"}
{"intent": "Mask any credit card numbers (format: 4 groups of 4 digits separated by spaces or dashes) in customers.csv, replacing them with 'XXXX-XXXX-XXXX-XXXX', and keep a backup of the original file.", "plan": [{"command": "sed -i.bak -E \"s/([0-9]{4}[- ][0-9]{4}[- ][0-9]{4}[- ][0-9]{4})/XXXX-XXXX-XXXX-XXXX/g\" customers.csv"}], "domain": "Text Processing"}
{"intent": "Generate a word frequency report for all Markdown (.md) files in the repository, ignoring case and punctuation, and output the 20 most common words to word_freq.txt.", "plan": [{"command": "find . -type f -name \"*.md\" -print0 | xargs -0 cat | tr -d '\".,;:()[]{}<>?' | tr '[:upper:]' '[:lower:]' | tr -s '[:space:]' '\\n' | grep -v '^$' | sort | uniq -c | sort -nr | head -n 20 > word_freq.txt"}], "domain": "Text Processing"}
{"intent": "Extract all unique email addresses from a large mixed-format log file, ignoring case and saving them sorted to a file.", "plan": [{"command": "grep -Eio \"[A-Z0-9._%+-]+@[A-Z0-9.-]+\\\\.[A-Z]{2,}\" /var/log/mixed.log | tr 'A-Z' 'a-z' | sort -u > extracted_emails.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the word 'foo' (as a whole word) with 'bar' in all .txt files under ~/documents, creating backup copies with a .bak extension.", "plan": [{"command": "find ~/documents -type f -name \"*.txt\" -exec sed -i.bak -E \"s/\\\\bfoo\\\\b/bar/g\" {} +"}], "domain": "Text Processing"}
{"intent": "Count how many distinct IPv4 addresses appear in the Apache access log, ignoring any malformed entries.", "plan": [{"command": "awk '{print $1}' /var/log/apache2/access.log | grep -E \"^([0-9]{1,3}\\\\.){3}[0-9]{1,3}$\" | sort -u | wc -l"}], "domain": "Text Processing"}
{"intent": "Generate a CSV report of the top 10 most frequent error messages from syslog, including the count for each message.", "plan": [{"command": "grep -i \"error\" /var/log/syslog | sed -E \"s/^[^:]*: //\" | sort | uniq -c | sort -nr | head -n 10 | awk '{print $2\",\"$1}' > top_errors.csv"}], "domain": "Text Processing"}
{"intent": "Convert a tab\u2011delimited file to a pipe\u2011delimited one, trim leading/trailing whitespace from each field, and remove any empty lines.", "plan": [{"command": "awk -F '\\t' 'NF {for(i=1;i<=NF;i++){$i=gensub(/^\\\\s+|\\\\s+$/,\"\",\"g\",$i)}; print $0}' OFS='|' input.tsv | sed '/^$/d' > output.psv"}], "domain": "Text Processing"}
{"intent": "Extract all email addresses from all .log files in the current directory, deduplicate them, sort alphabetically, and write the result to emails.txt", "plan": [{"command": "grep -Eho '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}' *.log | sort -u > emails.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the string 'TODO:' with 'DONE:' in all Markdown files under ./docs, preserving original files as .bak backups", "plan": [{"command": "find ./docs -type f -name '*.md' -exec sed -i.bak 's/TODO:/DONE:/g' {} +"}], "domain": "Text Processing"}
{"intent": "Count the number of unique IP addresses that accessed the site each day from an Apache access log (access.log) and output a CSV with columns date,unique_ip_count", "plan": [{"command": "awk '{print $4}' access.log | cut -d'[' -f2 | cut -d':' -f1 | sort | uniq -c | awk '{print $2\",\"$1}' > ip_counts_per_day.csv"}], "domain": "Text Processing"}
{"intent": "Reformat data.csv by swapping the first and third columns, removing rows where the second column is empty, and writing the cleaned output to data_cleaned.csv", "plan": [{"command": "awk -F',' 'NF==3 && $2 != \"\" {print $3\",\"$2\",\"$1}' data.csv > data_cleaned.csv"}], "domain": "Text Processing"}
{"intent": "Generate a word\u2011frequency list (excluding common English stopwords) from large.txt and store the top 20 most frequent words with their counts in top_words.txt", "plan": [{"command": "tr -cs \"[:alnum:]\" '\\n' < large.txt | tr 'A-Z' 'a-z' | grep -v -w -F -f <(printf \"%s\\n\" and the of to in is it you that for on with as at be this have are) | sort | uniq -c | sort -nr | head -n 20 > top_words.txt"}], "domain": "Text Processing"}
{"intent": "Extract all unique email addresses from a log file and save them sorted to emails.txt.", "plan": [{"command": "grep -E -o \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\" server.log | sort -u > emails.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the word 'foo' with 'bar' in all .txt files under the current directory, keeping a backup of each original file.", "plan": [{"command": "find . -type f -name \"*.txt\" -exec sed -i.bak 's/foo/bar/g' {} +"}], "domain": "Text Processing"}
{"intent": "Count the number of unique IPv4 addresses in an Apache access log and display the top 10 most frequent ones.", "plan": [{"command": "awk '{print $1}' access.log | grep -E '^([0-9]{1,3}\\.){3}[0-9]{1,3}$' | sort | uniq -c | sort -nr | head -10"}], "domain": "Text Processing"}
{"intent": "Create a CSV file summarizing word frequencies (case\u2011insensitive) from a large text document, ordered from most to least frequent.", "plan": [{"command": "tr -cs '[:alpha:]' '\\n' < document.txt | tr '[:upper:]' '[:lower:]' | sort | uniq -c | sort -nr | awk 'BEGIN{FS=OFS=\",\"} {print $2,$1}' > word_frequencies.csv"}], "domain": "Text Processing"}
{"intent": "Extract all sections delimited by lines containing 'START' and 'END' from every .conf file in /etc/configs, prepend the filename to each extracted block, and concatenate the results into combined_sections.txt.", "plan": [{"command": "for file in /etc/configs/*.conf; do awk 'BEGIN{in_section=0} /START/{in_section=1; print FILENAME \":\"} /END/{print; in_section=0} in_section' \"$file\"; done > combined_sections.txt"}], "domain": "Text Processing"}
{"intent": "Extract all email addresses from a text file and save a sorted list of unique addresses to emails.txt", "plan": [{"command": "grep -E -o \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\" input.txt > emails_raw.txt"}, {"command": "sort -u emails_raw.txt > emails.txt"}, {"command": "rm -f emails_raw.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the string 'foo' with 'bar' in all .txt files under the current directory, preserving original files as .bak backups", "plan": [{"command": "find . -type f -name \"*.txt\" -print0 | while IFS= read -r -d '' file; do cp \"${file}\" \"${file}.bak\" && sed -i 's/foo/bar/g' \"${file}\"; done"}], "domain": "Text Processing"}
{"intent": "Count the number of unique IP addresses in an Apache access log and display the top 10 most frequent IPs", "plan": [{"command": "awk '{print $1}' access.log | sort | uniq -c | sort -nr | head -n 10"}], "domain": "Text Processing"}
{"intent": "Create a CSV report of word frequencies from a Markdown file, sorted from most to least frequent", "plan": [{"command": "pandoc -f markdown -t plain source.md -o plain.txt"}, {"command": "tr -c \"[:alnum:]\" \"\\n\" < plain.txt | tr 'A-Z' 'a-z' | grep -v '^$' | sort | uniq -c | sort -nr | awk '{print $2\",\"$1}' > word_frequencies.csv"}, {"command": "rm -f plain.txt"}], "domain": "Text Processing"}
{"intent": "Search all *.log files for HTTP error codes 500 or 503, extract the timestamp and error message, and consolidate them into a single summary file", "plan": [{"command": "grep -E \"(500|503)\" -h *.log | awk -F'[][]' '{print $2\",\"$3}' > error_summary.csv"}], "domain": "Text Processing"}
{"intent": "Extract all email addresses from every *.log file under /var/logs, remove duplicates, sort them alphabetically, and write the result to ~/email_list.txt.", "plan": [{"command": "find /var/logs -type f -name \"*.log\" -print0 | xargs -0 grep -Eho \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\" | sort -u > ~/email_list.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the word 'foo' with 'bar' in all .txt files within the current directory tree, keeping a backup of each original file with a .bak extension.", "plan": [{"command": "find . -type f -name \"*.txt\" -exec sed -i.bak \"s/foo/bar/g\" {} +"}], "domain": "Text Processing"}
{"intent": "Count the number of unique IP addresses appearing in an Apache access log (access.log), ignoring any lines that are comments, and display the top 10 most frequent IPs with their request counts.", "plan": [{"command": "awk '!/^#/ {print $1}' access.log | sort | uniq -c | sort -nr | head -n 10"}], "domain": "Text Processing"}
{"intent": "Create a CSV report (word_counts.csv) that lists each non\u2011stopword word and its total frequency across all Markdown (*.md) files in the docs/ directory, sorted by descending count.", "plan": [{"command": "stopwords=\"a|an|the|and|or|but|if|in|on|with|for|to|of\"; find docs -type f -name \"*.md\" -exec cat {} + | tr -cs \"[:alpha:]\" \"\\n\" | tr 'A-Z' 'a-z' | grep -Ev \"^(${stopwords})$\" | sort | uniq -c | sort -nr | awk '{print $2\",\"$1}' > word_counts.csv"}], "domain": "Text Processing"}
{"intent": "Search the configuration file /etc/myapp/conf.cfg for any lines containing the deprecated setting 'EnableLegacyMode', comment out those lines by prefixing them with '#', and log each change (original line and timestamp) to /var/log/myapp/legacy_mode_changes.log.", "plan": [{"command": "grep -n \"EnableLegacyMode\" /etc/myapp/conf.cfg | cut -d: -f1 | while read lineno; do sed -i \"${lineno}s/^/#/\" /etc/myapp/conf.cfg; done"}, {"command": "grep \"#.*EnableLegacyMode\" /etc/myapp/conf.cfg | while read line; do echo \"$(date '+%Y-%m-%d %H:%M:%S') - $line\" >> /var/log/myapp/legacy_mode_changes.log; done"}], "domain": "Text Processing"}
{"intent": "Extract all email addresses from a log file, deduplicate them, and store the sorted list in a separate file.", "plan": [{"command": "grep -E -o '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}' logfile.txt | sort -u > emails.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the string 'foo' with 'bar' in all .txt files under the current directory, keeping a backup of each original file.", "plan": [{"command": "find . -type f -name \"*.txt\" -exec sed -i.bak 's/foo/bar/g' {} +"}], "domain": "Text Processing"}
{"intent": "Count how many distinct IP addresses appear in an Apache access log.", "plan": [{"command": "awk '{print $1}' access.log | sort -u | wc -l > unique_ip_count.txt"}], "domain": "Text Processing"}
{"intent": "Extract the block of lines from a configuration file that starts with a line containing 'START' and ends with a line containing 'END', preserving the original line order.", "plan": [{"command": "awk '/START/{flag=1} flag; /END/{flag=0}' config.cfg > extracted_block.cfg"}], "domain": "Text Processing"}
{"intent": "Create a CSV report of word frequencies from a large text document, ignoring case and sorting the results from most to least frequent.", "plan": [{"command": "tr -cs '[:alnum:]' '\\n' < document.txt | tr '[:upper:]' '[:lower:]' | sort | uniq -c | sort -nr | awk '{print $2\",\"$1}' > word_frequencies.csv"}], "domain": "Text Processing"}
{"intent": "Extract all email addresses from every *.log file under /var/logs and write a deduplicated, alphabetically sorted list to emails.txt", "plan": [{"command": "mkdir -p /tmp/output"}, {"command": "grep -Eho \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\" /var/logs/*.log | sort -u > /tmp/output/emails.txt"}], "domain": "Text Processing"}
{"intent": "Recursively replace every occurrence of the word 'foo' with 'bar' in all .txt files under ./documents, keeping a .bak backup of each original file", "plan": [{"command": "find ./documents -type f -name \"*.txt\" -exec sed -i.bak \"s/foo/bar/g\" {} +"}], "domain": "Text Processing"}
{"intent": "Count the number of unique IP addresses in the Apache access log and produce a frequency\u2011sorted list of the top 10 IPs", "plan": [{"command": "awk '{print $1}' /var/log/apache2/access.log | sort | uniq -c | sort -nr > /tmp/output/ip_counts.txt"}, {"command": "head -n 10 /tmp/output/ip_counts.txt > /tmp/output/top_10_ips.txt"}], "domain": "Text Processing"}
{"intent": "Create a CSV report from /etc/passwd containing username, UID, home directory, and shell, sorted by UID descending", "plan": [{"command": "awk -F: 'BEGIN{OFS=\",\"; print \"username,uid,home,shell\"} {print $1,$3,$6,$7}' /etc/passwd | sort -t, -k2 -nr > /tmp/output/passwd_report.csv"}], "domain": "Text Processing"}
{"intent": "Mask all credit\u2011card numbers (16 consecutive digits) in .log files under ./logs, replacing all but the last four digits with X, and write the sanitized files to ./masked_logs preserving the directory hierarchy", "plan": [{"command": "mkdir -p ./masked_logs"}, {"command": "find ./logs -type f -name \"*.log\" | while read file; do dest=\"./masked_logs/${file#./logs/}\"; mkdir -p \"$(dirname \"$dest\")\"; sed -E \"s/([0-9]{12})([0-9]{4})/XXXXXXXXXXXX\\2/g\" \"$file\" > \"$dest\"; done"}], "domain": "Text Processing"}
{"intent": "Extract all email addresses from a large text file and save a sorted list of unique addresses.", "plan": [{"command": "grep -E -o \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\" large_text.txt > emails_unsorted.txt"}, {"command": "sort -u emails_unsorted.txt > emails_unique_sorted.txt"}, {"command": "rm emails_unsorted.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the word 'foo' with 'bar' in all .txt files under the current directory, keeping a backup of each original file.", "plan": [{"command": "find . -type f -name \"*.txt\" -exec sed -i.bak 's/foo/bar/g' {} +"}], "domain": "Text Processing"}
{"intent": "Count the number of unique IPv4 addresses that accessed the web server in the last 7 days from the access.log.", "plan": [{"command": "date -d \"7 days ago\" \"+%d/%b/%Y\" > start_date.txt"}, {"command": "START=$(cat start_date.txt)"}, {"command": "awk -v start=\"$START\" '$4 >= \"[\"start\"\"' access.log | awk '{print $1}' | sort | uniq | wc -l > unique_ip_count.txt"}], "domain": "Text Processing"}
{"intent": "Extract log entries for the date 2023-09-15 from a combined log file and format them as CSV with columns: timestamp, IP, request, status.", "plan": [{"command": "grep \"\\[15/Sep/2023\" combined.log | awk -F'[][]' '{split($2,dt,\" \"); split($6,req,\" \\\"\"); printf \"%s,%s,%s,%s\\n\", dt[1]\" \"dt[2], $1, req[2], $9}' > logs_2023-09-15.csv"}], "domain": "Text Processing"}
{"intent": "Generate a report of the top 10 most frequent words (excluding common stopwords) across all markdown files in the docs/ directory.", "plan": [{"command": "cat docs/*.md | tr -c \"[:alnum:]\" \"\\n\" | tr '[:upper:]' '[:lower:]' > all_words.txt"}, {"command": "grep -v -E \"^(the|and|for|with|that|this|from|are|was|but|not|you|your|have|has|can|will|would|should|could|it|its|on|in|at|by|of|to|a|an)$\" all_words.txt > filtered_words.txt"}, {"command": "sort filtered_words.txt | uniq -c | sort -nr | head -n 10 > top10_words.txt"}, {"command": "rm all_words.txt filtered_words.txt"}], "domain": "Text Processing"}
{"intent": "Extract all unique email addresses from all *.md files in the current directory and its subdirectories, sort them alphabetically, and write the result to a file named emails.txt.", "plan": [{"command": "grep -Eho '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}' -r --include='*.md' . | sort -u > emails.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the word 'foo' with 'bar' in all .txt files under ./documents, while keeping a backup of each original file with a .bak extension.", "plan": [{"command": "find ./documents -type f -name '*.txt' -exec sed -i.bak 's/foo/bar/g' {} +"}], "domain": "Text Processing"}
{"intent": "Count the number of unique IP addresses that appear in the Apache access log (access.log) and display the top 10 most frequent IPs along with their request counts.", "plan": [{"command": "awk '{print $1}' access.log | sort | uniq -c | sort -nr | head -n 10"}], "domain": "Text Processing"}
{"intent": "Convert a CSV file (data.csv) to a tab\u2011separated values (TSV) file, removing any leading or trailing whitespace from each field, and save the output as data.tsv.", "plan": [{"command": "awk -F',' 'BEGIN{OFS=\"\\t\"} {for(i=1;i<=NF;i++) gsub(/^ +| +$/,\"\",$i)}1' data.csv > data.tsv"}], "domain": "Text Processing"}
{"intent": "Generate a daily error summary: for each .log file in ./logs, extract lines containing the word 'ERROR', pull the date (assumed to be the first field in YYYY-MM-DD format), and produce a report (error_summary.txt) that lists each date with the total number of error lines across all logs.", "plan": [{"command": "grep -h 'ERROR' ./logs/*.log | awk '{print $1}' | sort | uniq -c | sort -nr > error_summary.txt"}], "domain": "Text Processing"}
{"intent": "Extract all email addresses from all *.log files in the current directory, deduplicate them, sort alphabetically, and save to emails.txt.", "plan": [{"command": "grep -Eho '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}' *.log | sort -u > emails.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the word 'foo' with 'bar' in all .txt files under the current directory tree, keeping a backup of each original file.", "plan": [{"command": "find . -type f -name \"*.txt\" -exec sed -i.bak 's/foo/bar/g' {} +"}], "domain": "Text Processing"}
{"intent": "Count how many times each unique IP address appears in the Apache access log and display the top 10 most frequent IPs.", "plan": [{"command": "awk '{print $1}' access.log | sort | uniq -c | sort -nr | head -10"}], "domain": "Text Processing"}
{"intent": "Extract all lines from large.txt that are between lines containing 'START' and 'END' (inclusive), preserve original line numbers, and write them to extracted.txt.", "plan": [{"command": "awk 'NR==FNR{if($0~/START/){f=1} if(f){print NR\":\"$0} if($0~/END/){f=0}}' large.txt large.txt > extracted.txt"}], "domain": "Text Processing"}
{"intent": "Create a CSV file listing each system user and their UID from /etc/passwd, sorted by UID in descending order.", "plan": [{"command": "awk -F: '{print $1\",\"$3}' /etc/passwd | sort -t, -k2 -nr > users.csv"}], "domain": "Text Processing"}
{"intent": "Extract all email addresses from a text file and save a sorted list of unique addresses.", "plan": [{"command": "grep -E -o '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}' input.txt | sort -u > emails.txt"}], "domain": "Text Processing"}
{"intent": "Replace every occurrence of the word 'foo' with 'bar' in all .txt files under the current directory, keeping a backup of each original file.", "plan": [{"command": "find . -type f -name \"*.txt\" -exec sed -i.bak 's/foo/bar/g' {} +"}], "domain": "Text Processing"}
{"intent": "Count the number of distinct IP addresses that appear in an Apache access log.", "plan": [{"command": "awk '{print $1}' /var/log/apache2/access.log | sort | uniq | wc -l"}], "domain": "Text Processing"}
{"intent": "Create a CSV summary that shows how many requests returned each HTTP status code in a log file.", "plan": [{"command": "awk '{print $9}' /var/log/apache2/access.log | sort | uniq -c | awk '{print $2\",\"$1}' > status_summary.csv"}], "domain": "Text Processing"}
{"intent": "Mask all credit\u2011card numbers in .log files, keeping only the last four digits visible.", "plan": [{"command": "find . -type f -name \"*.log\" -exec sed -i.bak -E 's/([0-9]{4}[- ]?){3}([0-9]{4})/XXXX-XXXX-XXXX-\\4/g' {} +"}], "domain": "Text Processing"}
